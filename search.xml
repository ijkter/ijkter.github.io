<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test_2340]]></title>
    <url>%2F2020%2F09%2F30%2Ftest-2340%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2020%2F09%2F30%2Ftest-1%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[active_data]]></title>
    <url>%2F2020%2F09%2F30%2Factive-data%2F</url>
    <content type="text"><![CDATA[[Updated] 本文基于《活用数据》整理了数据分析思维的相关内容。 数据思维先行概念市场营销企业在现有营销环境下，根据目标消费者的需求，利用现有的资源和能力，比竞争对手更快捷、更有效地向目标消费者提供产品和服务，实现企业盈利以及可持续发展的生产和经营活动。 — 现代市场营销学之父 菲利普·科特勒 为更好地驱动企业营销业务，围绕市场营销的定义展开作数据分析。 案例 角度 企业战略分析 现有营销环境 用户偏好分析 目标消费者的需求 STP 目标消费者的需求 品牌建设分析 让消费者选择自己 营销组合分析 比竞争对手更快捷、更有效 数据分析定义：数据分析是实现研究目的与研究内容的闭环。 做法：首先将研究目的分解为内容，然后再通过研究内容实现研究目的。 具体流程： 明确分析思路 获取数据 处理数据 分析数据 解读数据 定性分析与定量分析 特点 定性分析 关注意义，能做梳理，不能作为选择 定量分析 关注频率，量化成具体数据作为指导 预测预测是推断的过程： 由已知推断未知 条件：已知和未知具有相似性（类推原则）或相关性（相关原则）。 由过去推断未来 条件：市场具有惯性（惯性原则）。 预测的类型： 定性预测：基于经验和判断推断，主观性强。 定量预测：基于数据和模型推断，客观性强。基于同样的数据和模型，结论一定相同。 推断思路： 类推原则： 相关原则 惯性原则 解决什么问题 - 明确分析思维 识别机会 规避风险 问题诊断 营销效果评价 量化管理 分析哪些内容 - 开启分析思路学会提问 发散思维，头脑风暴，罗列出关键问题 MECE 原则（mutually exclusive collectively exhausted） 分解出的各项内容相互独立，交集为空集；汇总在一起完全穷尽，并集是全集。 核心：不重不漏 方法：二分法…等 使用模型或结构化思维对罗列的关键问题进行归纳划分 熟悉模型 模型是经过长期检验的成熟的分析思路，灵活运用可以事半功倍。 熟悉常用模型及其适用场景： 适用场景 模型 顾客满意度 RATER 产品属性优先度 KANO 战略选择 SWOT 战略选择 IEF 内外因素评价矩阵 用户行为 用户行为五阶段、5W2H 品牌形象 品牌知觉图 品牌知名度 Graveyard 定价决策 PSM 营销组合 4P营销理论 用户转化 AARRR 环境分析 PEST 市场分析 波特五力 结构化思维时间与结构思维 “四方上下曰宇，往古来今曰宙。” 时间：事务的过程，即发展阶段 结构：事务的方面，即构成要素 演绎思维 演绎而不是归纳。 原因 演绎：由共性原理（或假设）推出个性结论 归纳：由个性推出共性 标准式演绎 — 三段式 大前提：已知共性原理（或假设），该原理（或假设）具有一般性和普遍性。 小前提：关于对所研究对象的个性情况的描述。小前提应与大前提有关。 结论：从共性原理（或假设）推出对所研究对象的具体判断。 例如：（大前提）18 岁以上为成年人，（小前提）叶某今年 20 岁，（结论）那么叶某是成年人。 常见式演绎 — 4W What’s going on? 描述现象 Why did thits happen? 分析原因 What lies ahead? 判断趋势 Which course of action should i take? 提出对策 重要性思维 将资源花在关键改进点上。 分析到什么程度 - 打开分析视角对比视角对比类型 参照物 与自身纵向对比：过去和现在对比 与其他横向对比：与同类或相似的其他对象对比 对比指标性质 频数统计：分类型数据 均值分析：数值型数据 对比可信度 时间可比性：刨除特殊时段/时间粒度和长度一致。例如：夏天卖棉袄和冬天卖棉袄。 空间可比性：刨除特殊环境。例如：北极卖棉袄和赤道卖棉袄。 数量可比性：同量纲 统一单位，使数据可比。例如：身高（cm）和体重(kg)。 方法：变异系数法 — $v = \frac{\sigma}{\bar x}$，刻画单位平均水平下的差异。 剔除数量量级差异，使变量统一可比。例如：手掌长度与身高（接近 10 倍）。 方法：变量标准化 — $Z = \frac{x - \mu}{\sigma}$，各变量均值和标准差都化为 0 和 1。 分类视角意义同类共性，异类区别，针对性营销。 方法以客户分类为例： 分类维度处理：标准化、因子分析 客户细分检验：聚类分析、方差分析 目标市场分析：矩阵分析 目标客户定位：方差分析、交叉分析、比较分析、对应分析 相关视角相关定义A 与 B 相关有以下情况： 因果：A -&gt; B 或 B -&gt; A； 共存：C -&gt; A 且 C -&gt; B。 注：要判断因果，先证相关性。 相关判定 数值型：相关系数 — $r(X, Y) = \frac{Cov(X ,Y)}{\sqrt{Var|X| Var|Y|}} = \frac{E|XY| - E|X|E|Y|}{\sqrt{Var|X| Var|Y|}}$ 分类型：方差分析 — $SST（总体信息） = SSR（组间） + SSE（组内）$ SSR 大 =&gt; 分组属性引起差异，相关 SSE 大 =&gt; 分组属性以外的属性引起差异，无关 综上，SSR 小，SSE 大 =&gt; 无关，不存在显著差异 若 P(SSR = 0) &lt; 0.05，拒绝假设“SSR = 0”，即拒绝“无关”，则表明“相关”，即“存在差异” 应用 规模预测 例如：道格拉斯生产函数 $y = A K^\alpha L^\beta \mu，A-技术，K-资本，L-劳动力，\alpha 、\beta-弹性系数，\mu-干扰因子$ 精准预测 例如：方差分析（验证相关） + 交叉分析（偏好差异），对不同用户群体进行精准营销。 描述视角整体趋势 集中趋势：加权平均分 $\bar x = \frac{\sum x_i f_i}{i}$ 离中趋势：变异系数 $v = \frac{\sigma}{x}$ 个体波动 研究单独个体 离群、异常发现 研究个体间差异 具体业务中的视角选择不同层次组合应用。 模型RATER客户满意度模型： 硬实力 外在：Tangible 有形度 有形的服务设施、环境、服务人员的仪表以及对客户的帮助和关怀的有形表现。 内在：Assurances 专业度 企业的服务人员所具备的专业知识、技能和职业素质。 软实力 外在：Responsiveness 反应度 服务人员对于客户的需求给予及时反应并能迅速提供服务的愿望。 内在： Reliability 信赖度 企业是否能够始终如一地履行自己对客户所做出的承诺，获得良好的口碑与客户的信赖。 Empathy 同理度 服务人员能够随时设身处地地为客户着想，真正地同情理解客户的处境，了解客户的需求。 KANO属性分类： 属性 含义 用户情绪 重要性 示例 必备属性 M（Must be） 产品或服务的最核心的属性 有则无感，无则厌恶 1 + 手机能打电话 一维属性 O（One-dimensional） 与用户态度线性正相关的属性 有则好感，无则厌恶 2 \ 手机电池快充 魅力属性 A（Attractive） 用户期望的属性 有则喜，无无感 3 \ 手机高刷新率 可有可无属性 I（Inessential） 无论是否具备该属性，用户都无所谓 有无皆无感，无关紧要 4 - 手机编程 厌恶属性 R（Repugnant） 具备了反而让用户不满 有则厌恶，无则无感 × 不该有 手机上传隐私数据 两大原则确定新产品的属性开发顺序： 优先原则（M &gt; O &gt; A &gt; I） 产品研发人员的优先研发顺序：必备属性 &gt; 一维属性 &gt; 魅力属性 &gt; 可有可无属性。 组合原则(M + O + A) 有竞争力的产品应尤三部分组成：必须满足用户的必备属性 + 领先竞争对手的一维属性 + 差异化的魅力属性。 注意事项： 用户的差异性 对于同一个属性，不同用户的态度是不同的。 在已知产品细分市场的前提下，针对不同的细分市场进行 KANO 分析 在不清楚细分市场的前提下，根据 KANO 分析结果对用户需求进行市场细分，以便对不同细分市场提供不同功能的产品或服务 用户需求的发展性 对于同一个属性，其属性不可能恒定不变。即使是创新属性也可能随业界发展变成通用标准，变成一维属性或必备属性。因此，产品或服务的设计者需要进行连续性的 KANO 调查，以把握用户需求的发展和变化。 SWOT 内部\外部 Strength（优势） Weakness（弱势） Opportunity（机会） SO WO Threaten（威胁） ST WT IEF内外因素评价矩阵IEF 内外因素评价矩阵用来对于 SWOT 分析中的数据进行量化： 分别计算割割因素的评分和权重 分别计算机会、威胁、优势、劣势的加权平均数 用加权平均数的大小判断市场吸引力和企业竞争力，并给出相应的战略建议 评分数据来源: 专家访谈：专家适合对具体研究的企业外部因素（PEST）进行评分。 市场调研：消费者对产品或服务有切身体验，适合对企业内部因素评分。 权重确定法 主观赋权法 客观赋权法 思路与优缺点 专家经验主观判断；定性；简单 历史数据研究评价；定量；复杂 常用方法 层次分析法 主成分分析法（或因子分析法） 其他方法 环比评分法、最小平方法 变异系数法、回归分析 权重计算： 计算平均分：$\bar{x} = \frac{\sum x_i}{n}$ 计算频率：$p_i = \frac{n_i}{n}$ 计算方差：$\sigma_i = \frac{\sum(x_i - \bar{x})^2 p_i}{n}$ 计算变异系数：$v_i = \frac{v_i}{\sigma_i}$ 计算权重：$\omega_i = \frac{v_i}{\sum v_i}$ 内部\外部 Strength（优势） Weakness（弱势） Opportunity（机会） SO WO Threaten（威胁） ST WT 用户行为五阶段、5W2H用户行为五阶段： 产生需求 信息收集 方案比选 购买决策 购后行为 5W2H： When 时间 Where 地点 Who 对象 What 事件 Why 原因 How 方式 How much 程度 品牌知觉图品牌知觉图用于品牌形象分析，是指基于品牌形象数据用距离远近反映品牌与精神价值相关程度的图形。距离越近，表示相关程度越大。 解读方法： 圆心定理 - 最符合品牌的形象 以各品牌为圆心，最先圈进去的指标就是最符合该品牌定位的精神价值。 向量分析 - 品牌具有某形象的程度 从原点向任一指标画一条射线，构成一个向量； 然后将所有品牌对这个向量做垂线； 垂点越靠近向量箭头指向的指标，表明该产品越具有该指标描述的形象。 余弦定理 - 品牌定位相似性/竞争性 从原点向任意两品牌分别画一条射线，构成两个向量。向量夹角越小，则夹角余弦越大，表明两个品牌相关性越强，定位相似性高，具有竞争关系。 原点定理 - 品牌差异性 越远离原点的品牌，消费者越容易识别，说明品牌的特征越明显； 越靠近原点的品牌，消费者越不容易识别，说明品牌没有显著特征，越缺乏差异化认知。 GraveyardGraveyard 模型能够反映提示前知名度和提示后知名度之间的内在关系。 提示后知名度为横坐标，提示前知名度为纵坐标。 回归拟合曲线体现了品牌变化发展的总体趋势和平均发展水平，分布在该回归拟合线周围的品牌则体现了各品牌相对于平均发展水平的波动和差异。 正常品牌：位于回归曲线周围，品牌知名度与市场上的平均水平比较一致。 衰退品牌：位于回归曲线右下方，其提示前知名度明显低于提示后知名度，显现出该品牌被消费者淡忘的趋势。 利基品牌：位于回归曲线左上方，其提示前知名度高于提示后知名度，虽然品牌认知率相对不高，但是品牌回忆率较高，消费者对其忠诚度较高。 强势品牌：位于回归曲线右上方，其提示前知名度和提示后知名度都很高，消费者对其忠诚度很高，这些大都是市场上的强势品牌。 制作方法: 先计算各品牌提示前知名度和提示后知名度； 以提示后知名度为 X 轴、提示前知名度为 Y 轴绘制散点图，每个点代表一个品牌； 对散点做回归拟合线。 PSMPSM（Price Sensitivity Measurement）模型即价格敏感度测试模型。利用 PSM 模型测试价格，不需要预先给出价格，而是让受访者自己表示他们可接受的价格范围。 模型搭建 出示新品 demo 或概念后，询问 4 个问题： 哪个价格让你开始觉得便宜？ 哪个价格让你开始觉得贵？ 哪个价格让你开始觉得太贵而不买？ 哪个价格让你觉得太便宜，不相信它的质量而不买? 根据受访者回答，统计出每个价格在上述 4 个问题上的累计人数百分比 根据每个价格（X 轴）的累计人数百分比（Y 轴）绘图，得到四条相交曲线 由图可确定 两条相交曲线与其交点所构成的上方区域表示接受该交点对应的价格水平的市场规模 最优价格点 两条曲线与交点构成的上方区域面积最大的交点价格即最优价格点。 可接受价格点 “开始觉得便宜”曲线和“开始觉得贵”曲线的交点。 可接受价格区间 曲线所谓区域的左右端点价格。 证明： 最优价格点和可接受价格点都在该区域内； 对于端点，“觉得太贵而不买”和“觉得太便宜而不买”的人数增幅大于“开始觉得便宜”和“开始觉得贵”，导致整体市场规模减小； 因此端点价格即可接受价格区间。 不同市场的规模 设： A = 开始觉得便宜的累计人数百分比 B = 觉得太便宜而不买的累计人数百分比 C = 开始觉得贵的累计人数百分比 D = 觉得太贵而不买的累计人数百分比 有： $可接受者 = 1 - A - C$：对于该价格不觉得贵也不觉得便宜的人数比例 $保留接收者 = A - B + C - D$：对于该价格觉得贵但是不太贵或者觉得便宜但是不太便宜的人数比例。 $不接受者 = B + D$：觉得该价格太贵或太便宜而不买的人数比例。 4P 营销理论产品、价格、渠道、促销合成营销组合。通过营销组合，企业引导商品或服务从生产者到达消费者的决策活动成为营销决策。做出科学的营销决策就需要平衡好做好产品、定好价格、铺好渠道、打好促销这四个方面。 Product 产品 规模预测分析：季节分解法 如何决定生产规模？ 该生产多少？ 在市场经济环境下，该生产多少由市场需求决定，需要预测市场规模。 能生产多少？ 在资源约束环境下，能生产多少是由企业所拥有的生产要素决定的，需要预测产出规模。 市场规模预测和产出规模预测统称“规模预测”。 产品属性分析：KANO 模型 根据市场所需要的产品属性及其需求程度，挖掘消费者对产品的核心需求与偏好，明确产品属性开发优先级。 Price 定价 定价既是产品优劣的反映，也是顾客眼中的产品价值，是产品的竞争性定位和销售力的体现。 定佳决策分析：PSM Place 渠道 渠道价值分析 - “渠道为王”，是影响企业能否赢得市场的一个重要竞争力。在多渠道共同发生作用时，只有准确地对每个渠道的价值进行评价，才能做好渠道资源分配，实现效益最大化。 Promotion 促销 营销者向消费者传递有关本企业及产品的各种消息，说服或吸引消费者购买其产品，以达到扩大销售量的目的。为了达到促销目的，通常会在约束条件下搭配媒体组合使传播效果最优，常用线性规划解决。 资源配置三要素： 目标函数 约束条件 决策变量 资源配置方法：线性规划 AARRR用户生存周期可归纳为 AARRR 漏斗模型。 Acquisition 获取关键：降低用户的使用门槛，结合不同阶段用户群体的特征，制定最适合的拉新策略，同时时刻关注各个核心数据指标；而不是考虑各种渠道推广引流。 产品角度 简化注册登录流程 账号强相关产品可采取手机+短信验证码快捷注册登陆；非账号强相关产品可在用户体验核心功能时才要求注册登录。 滞后权限授权 体验相关功能时才进行授权；安装后第一时间授权影响体验。 用户初次进行产品介绍/操作指引 适量（&lt;=4）的页面/流程介绍核心/优质内容，复杂功能进行操作指引降低使用门槛；忌繁琐冗长。 运营角度 产品不同阶段中用户群体的特征不同，可从冷启动期/增长期/稳定期/衰退期四个阶段看： 冷启动期 现状：只有第一批少量用户。 措施：关注用户质量而非数量，打造用户交流社区，评估核心用户的反馈，对产品进行打磨。 关注指标: DAU、留存、活跃时长。 增长期 现状：产品经过冷启动期的打磨得到核心用户认证，开始进入飞速增长阶段。 措施：结合目标群体的普遍特征，寻找合适的渠道进行推广引流。 关注指标：ROI — 以以尽可能少的成本，获取高质量的用户。 稳定期 现状：由于产品用户群体的不同，产品的用户规模达到增长瓶颈。 措施：寻找产品是否有延展方向，通过需求的延展获取更多用户。 关注指标: 用户留存。 衰退期 现状：产品早晚会步入衰退期，用户慢慢转移至其他替代产品，用户数量逐渐降低。流失用户的召回存在于每个时期，在此阶段则尤为重要，往往需要面临较大的困难与成本守住用户流量。 措施：召回流失用户，减缓产品的衰退。可从四个角度召回： 利益驱动召回：优惠券、礼品、抽奖等活动； 社交属性召回：社交联动（关注、评论、邀请）； 产品核心需求召回：周期性收益产品查看/获取收益； 新功能刺激召回：版本重大更新、新功能。 关注指标: 召回率，流失指标。 Activation 激活注：激活 != 用户注册。对于账号强相关的产品，可以将注册作为用户激活的一个参考依据。从整体上来讲，用户激活更应该考虑的是用户对于产品核心功能的使用情况。比如：微博用户活跃情况、抖音用户短视频观看情况。 关键： 找到自己产品用户激活的标准。 根据产品的类型特点，做对应的产品模块设计，同时辅以运营活动，创造用户可触达的需求场景，进而达成目标，激活用户。 关注核心指标，而非一味提高用户注册量。 不同类型的产品用户群体不同，用户激活的方法也不尽不同，整体上可分为三种类型进行讨论： 单用户产品 单用户产品：服务于个人用户，没有过多的不同用户之间的接触以及不同角色的产品。大致可分为两类： 工具类 核心：便捷使用，舒适体验。 工具产品需要拿来即用，所以必须简化流程，让用户速上手。 工具产品设计应该符合生活中的逻辑，降低使用门槛，增强用户体验。 游戏 核心：提升代入感。 合适的玩家指引：无论单人还是网络游戏，玩家在游戏初期都存在认知的过程，应该尽快让玩家了解规则，体验游戏。 由浅入深的体验：游戏初期应该给予玩家适应的过程，初期可以降低游戏的难度和复杂度，先保证游戏体验，待其适应再逐步放开。 多用户产品 多用户产品：多用户接触的产品。根据用户关系，分为两个不同的类型： 熟人产品 核心：获取熟人的联系。 作为通讯类产品若不能及时联系到熟人，就没有沟通对象，则失去使用意义。例如：微信能够通过手机通讯录和 QQ 好友快速建立起与熟人之间的联系，开启新的通讯体验，所以很快获取了大量的用户。若新的通讯 APP 需要通过一套新的 ID 标识（与原本的联系方式无关联）去相互添加才能使用，这无疑增加了使用成本。 陌生人产品 核心：搭建起陌生人之间的联系，即用户品配。 水军：投放一定的虚拟用户，用于激活前期为用户创造需求场景。但随着用户体验的深入，水军会被用户察觉并反感，因此只适用于激活前期。 用户匹配：通过为用户匹配或者推荐用户，促成用户之间的联系，进一步沉浸于产品中，达成用户激活。常见的匹配策略：LBS 匹配/用户属性匹配/个性化推荐匹配。 多角色产品 多角色产品：多用户产品且用户存在不同角色。如：美团 — 既有商家也有买家；Uber — 既有司机也有乘客。 核心：激活各方用户，并达成多角色之间的平衡连接。以 Uber 为例：通过首次免单吸引乘客；通过算法实现多订单下司机与乘客的合理匹配，保证供需的稳定；通过司机补助、乘客优惠促成订单的产生。 可通过核心指标进行用户的激活与连接情况： 用户激活率：即用户使用核心功能的占比。各产品的激活定义规则不尽相同，每个产品都必须找到适合自己的激活定义； 用户激活花费时长：用用户从进入产品，到被激活所花费的时间。时间越短，证明激活效果越明显； DAU/MAU：用户日活跃/月活跃的比值。不同类型的产品的 DAU/MAU 存在一定的基准线，如移动游戏的基准线为20%，工具类APP为40%。比值越大，说明用户对于产品的粘性越强，激活效果越明显； DAOT：用户日均使用时长。使用时长越长，说明用户的粘性越强，但同时需结合其他指标，评估是否产品流程过长导致的时长增加。 Retention 留存用户留存在任何时期都是评估产品是否真正具有价值的重要因素。只有用户感到了价值，才会选择留下了。 关键：使用户持续使用产品，形成稳定的依赖。 有效地评估产品的留存水平的指标： 第 N 天计算法 次日留存：统计日新增用户次日仍然使用产品的用户数量占总新增用户数量的比例； 7 天留存：统计日新增用户第七天仍然使用产品的用户数量占总新增用户数量的比例； 30 天留存：统计日新增用户第七天仍然使用产品的用户数量占总新增用户数量的比例。 强调第N天，其反应的结果也就是随着时间的推移，留存用户逐渐减少，而行业上也存在着对应较为权威的基准 4-2-1 基准，也即40%/20%/10% 为此计算方法下，较为合理的一个水平。 N 天内计算法 次日留存：统计日新增用户次日仍然使用产品的用户数量占总新增用户数量的比例； 7 天留存：统计日新增用户7天内，再次使用产品的用户数量占总新增用户数量的比例； 30 天留存：统计日新增用户30天内，再次使用产品的用户数量占总新增用户数量的比例。 强调N天内，其反应的结果就是30天留存 &gt; 7天留存 &gt; 次日留存，更多的是表现一个产品的活跃水平。 改良后第N天计算法： 次日留存：统计日新增用户，再次使用产品的时间间隔小于24小时的用户所占比例（T+2出数据指标）； 7天留存：统计日新增用户，再次使用产品的时间间隔小于7个自然天然填的用户所占比例（T+8出数据指标）； 30天留存：统计日新增用户，再次使用产品的时间间隔小于30个自然天然填的用户所占比例（T+8出数据指标）。 改良后的第N天计算法，可以准确地反映新增用户的留存水平，避免特殊场景造成的数据指标影响。 改良后N天内计算法： 次日留存：统计日活跃用户中，次日再次使用产品的用户占比（T+2出数据指标）； 7天留存：统计日活跃用户中，往后7天内再次使用产品的用户占比（T+8出数据指标）； 30天留存：统计日活跃用户中，往后30天内再次使用产品的用户占比（T+8出数据指标）。 针对N天内计算法，由于计算方式反映的更多是产品的活跃水平，并不单纯针对新增用户，产品的迭代过程中，往往会有很多核心功能/用户体验等的改变，想要真实观察产品迭代过程中的留存活跃情况，应当将用户群体进行扩展。 改良后的N天内计算法，更能清晰/完整地表现所有用户的活跃留存情况。 留存标准 不应该盲目套用，而是应该结合自身产品所处的行业、产品形态以及自身产品定位进行考虑。 提升留存的方向: 产品方向 提升自己产品的竞争力，不断满足用户的需求，并优化用户体验。 提升用户对于产品的粘性。 日常活跃功能：培养用户习惯。如：日常签到及其奖励体系。 用户激励体系：鼓励用户行为，并给予认可的反馈，激励用户，成正向循环。如：用户头衔、用户等级。 强化用户投入：沉淀用户在产品上的行为，使用户对产品产生依赖。即使后来出现竞品，用户也不会轻易放弃。如： 时间投入：通过阶段性的任务，量化并强调用户的时间投入，使用户不甘放弃； 金钱投入：通过年度会员/月度会员等，强化用户的金钱投入，增加用户离平台的损失； 内容投入：通过 UGC 内容的沉淀，强化用户的内容投入，比如微博/朋友圈/笔记类产品等，用户在使用过程中沉淀了大量的内容，不会轻易放弃； 情感投入：通过引导用户投入情感，产生精神寄托； 社交投入：通过形成稳定的圈子关系，强化用户的社交投入，比如微信等社交平台。社交关系越牢靠，用户对于产品的依赖性也越强。 运营方向 用户挽留：减少用户的流失 明确产品定位及核心竞争力 从流失用户和活跃用户的特征入手，分析用户流失原因 结合二者制定优化方案 用户召回：召回已流失用户 召回的两个契机： 让用户看到 数据关联：通过输入法/浏览记录等，获取并及时为用户推送感兴趣的产品或内容 关注内容发生变动：当用户关注/收藏的内容发生更新变化时，及时推送用户 特定时间段/时间点：固定时间点（节日/生日等），向用户推送相关内容 用户场景变化：当用户所处场景（城市/天气等）发生切换时推送相关内容 让用户想起 要达到让用户主动回想，需要结合产品的调性，长期的诱导或宣传，促使用户的生活记忆与产品产生联系并不断加深认知。 Revenue 变现 用户群体区分 基本特征 合适的变现方式 免费用户 固有思维“互联网免费”，基本不进行消费行为 流量变现；观念引导成为付费用户 普通用户 有一定的消费行为及消费意识，挖掘潜力大 诱导持续消费行为，提高消费额度和频次，养成消费习惯 优质用户 消费金额远大于实际价值，注重精神感受 维护消费后的用户体验，给予特殊待遇 变现方式： 产品及服务变现 通过与用户直接建立传统买卖关系或为用户提供付费服务获得盈利。 关键： 提高产品的核心竞争力（前提） 找到免费与付费的平衡 平衡在于满足免费用户基本需求的同时，而使付费用户有所收获。在明确用户需求的基础上，对预期需求进行免费，在预期之外的需求（特色服务、体验升级）进行收费。 付费行为引导培养 上瘾模型及激励策略可以培养用户付费行为，诱导用户提升消费额度和频次，提高产品及服务变现的效率。 上瘾模型：使用户的某些行为发展为习惯。 触发 - 为用户创造一个场景，使其产生我们想要培养的行为。如：产品和服务的试用。 行动 - 用户产生培养行为。如：通过一元体验活动开启会员服务。 激励 - 在用户产生培养行为给予正向反馈。如：给予荣誉成就（精神）或红包（物质）等奖励。 投入 - 引导用户进行付费后的产品体验，沉浸用户的行为成本，增加用户产生持续付费的可能。如：购物礼品卡（单次抵消额度有限），定时提醒余额和限期。 流量变现 广告变现：基于用户，通过广告定向投放进行变现。 现代化广告三要点: 用户精准 - 精准找到广告的目标用户群体，提升触达有效率； 时间精准 - 相同的广告，在用户休闲的情况下，往往能够促成转化；在用户忙碌的情况下，更有可能的则是造成骚扰，引起反感； 场景精准 - 适当的场景下，往往更能促成广告的转化。如：购买手机时推荐耳机。 数据变现：基于用户行为活动，通过用户行为脱敏，抽取过滤加工，形成有效、可利用的数据，通过数据的运用或商业转让，获取盈利。并非卖数据。 构建用户画像，掌握用户的付费意愿及付费倾向，精准把控用户需求，提供推荐或服务，从而获取盈利。还可以根据相似性，进行关联推荐。 相关指标： LTV：客户终生价值， 公司从用户生命周期中所得到的全部经济收益的总和，即从用户上手到离开产品所获取的总收益。 用户付费率：付费用户群体在活跃用户群体中所占规模比例。 一旦用户产生付费行为，便成为付费用户。该指标往往用来衡量产品的付费模块是否能够真正触达用户需求。 二次付费率：付费用户群体中，产生过二次及以上付费行为的用户所占规模比例。 通过观察二次付费率指标，可以评估产品付费模块是否对用户产生正向价值？付费体验是否良好？ ARPU 及 ARRPU：（通常以月份为维度进行统计）ARPU是指平均每用户收入，ARRPU是指平均每活跃用户收入。 用以以评估不用渠道的用户质量，不同时期的用户付费情况。 Referral 传播自传播：无需借助过多外力，产品自身激发用户间的自发传播。 自传播优势： 指数级增长（一传十，十传百） 用户获取成本低（用户自行推荐节省了渠道成本） 用户获取质量高（自传播在相似的用户群体中进行） 口碑效应（用户间讨论形成话题） 自传播步骤： 传播基础 条件： 产品能满足用户需求 传播手段便捷 纵向传播：内容形式转换 横向传播：跨平台分享 自主传播 关键：激发用户传播的欲望。 出发点： 制造话题：话题可能会引起讨论并在人群中迅速传播。如：热搜讨论。 从众心理：个人受到外界人群行为的影响，而在自己的知觉、判断、认识上表现出符合于公众舆论或多数人的行为方式。如：转发锦鲤。 情绪引导：通过情绪上的引导（产生波动、感同身受），能够促使用户进行分享传播。如：水滴筹。 引导参与：通过沉淀用户的行为，促使用户分享自身的成果。如：短视频平台发布视频。 超预期场景：在超预期的场景下，用户分享传播将会变得更加简单。如：买东西被告知中奖。 传播转换 发生自主传播后需要进行有效传播转换，才能达到用户增长的目的。 出发点: 可读性：降低被传播用户的接受门槛，轻松获取信息。如：图表优于文字。 互动：使被传播者与传播者产生互动行为，有参与感或收获。如：帮别人砍价自己也能得到优惠。 注意力：足够吸引用户的注意力，不至于被忽视。如：标题党。 指标： K 因子 = 传播数量（每个用户向他的朋友们发出的邀请的数量）* 转化率（接收到邀请的人转化为新用户的转化率）K 因子直接体现自传播结果水平： K 值大于1时，将激发自传播巨大的力量，K值越大，力量越强 K值小于1，那么传播水平会逐步减弱，直至消失。 PEST宏观环境是指影响市场的宏观因素，可归纳为 PEST。 环境 关键指标 Politics 政治环境 政治/经济体制、财政/税收/产业/投资/补助政策、国际/地区关系 Economics 经济环境 GDP 及增长率、利率汇率、居民可支配收入、产业结构 Social 社会文化环境 人口规模、性别比例、年龄构成、价值观、生活方式、教育状况、消费观念、宗教信仰、风俗习惯 Technology 技术环境 国家重点支持、技术更新与传播速度、商品化速度、技术保护情况 波特五力企业竞争环境可归纳为影响企业生存状态的波特五力。 对象 与企业的关系 供应商 原材料 讨价还价 购买者 产品 讨价还价 直接竞争对手 同类竞品 抢占市场份额，直接竞争 间接竞争对手 替代类竞品 替代品削弱需求，间接竞争 潜在进入者 有可能进入该领域的大企业 抢占市场的潜在威胁 归因分析常见归因分析模型 模型 定义 最后交互模型（Last Model） 认定最后一个渠道的贡献为 100%，因此把转化归功于最后一个渠道 第一次交互模型（First Model） 认定第一个渠道的贡献为 100%，因此把转化归功于第一个渠道 平均模型（Average Model） 认为所有的渠道的贡献相等，因此将权重均摊到参与转化的所有渠道中 时间衰减模型（Time Decay Model） 认为贡献程度随时间而衰减 自定义模型（Customized Model） 以上四种权重分配都比较武断，无法直接指导投放的优化，因此有针对性的产生了生存分析、通径分析、马尔科夫链、夏普利值等模型 夏普利值原则：在合作博弈中，所得与贡献相等。 适用夏普利值的三个特点： 夏普利值的有效性：联盟 S 具有完整性，不存在具有贡献却未纳入联盟里的参与者 夏普利值的对称性：参与者价值 V{a, b} = V{b, a} 夏普利值的可加性：联盟具有独立性，任意两个联盟合并的值等于两个联盟的值的合计，即 V{a, b} = V{a} + V{b} = V(a) + V(b) 设 |S| 表示与该参与者相关的某联盟 S 中成员的数量，n 表示在合作博弈中所有参与者的数量，则 与该参与者相关的每个联盟 S 的加权因子为：$\gamma_n (S) = \frac{(|S| - 1)! × (n - |S|)!}{n!}$ 该参与者价值的夏普利值为：$\varphi(v) = \sum \gamma_n(S) × (V(S) - V(S - {I}))$ RFM 用户画像是了解用户的重要手段，其包含多个方面：用户属性、用户消费特征、用户关联、用户非消费行为…等。其中，用户消费特征是用户画像中最核心、与业绩最直接相关的指标。RFM 模型就是根据消费特征对用户进行分层。 DefRFM 模型是根据最近消费时间（Recency）、消费频率（Frequency）、消费金额（Monetary）三个指标构建的用户分层模型。 R（Recency）：用户最近一次消费时间间隔 即，用户最后一次下单时间距今天多长时间。R 的值越小，用户价值越高。 F（Frequency）：用户消费频率 即，用户在固定的时间段内消费了几次。 该指标反映了用户的消费活跃度。F 的值越大，用户价值越高。 M（Monetary）：用户消费金额 即，用户在固定的周期内在平台上花了多少钱。 该指标直接反映了用户对公司贡献的价值。M 的值越大，用户价值越高。 用户分层以下三种方法可以根据不同需求选用，一般第一种方法最常用，因为可以直观反映用户价值和重要性，然后根据用户价值和重要性采取不同策略，可操作性强。 等级变量划分法 根据R、F、M三个指标数据，将其转化为等级变量，如高、低，具体划分标准可依据指标数据分布确定（可取中位数或平均数作为分界线），再根据三个指标等级划分用户等级。 如果每个指标划分为高、低两种等级，则用户可出现2^3=8种，但可根据实际需要将用户划分为3种等级，如下表所示： |R-Recency|F-Frequency|M-Monetary|用户等级|划分群体类型||-|-|-|-|-||高|高|高|A|重要价值客户||高|低|高|A|重要发展客户||低|高|高|B|重要保持客户||低|低|高|B|重要挽留客户||高|高|低|B|一般价值客户||高|低|低|B|一般发展客户||低|高|低|C|一般保持客户||低|低|低|C|一般挽留客户| 加权得分法 将每一个指标归一化，$x_1 = \frac{x - min}{max - min}$，将每个指标都转化到0~1的区间内。需要注意的是R的取值需要进行转化，取 $x_2 = 1 - x_1$； 赋予指标权重； 计算用户的加权得分，根据得分对用户进行分层。 将指标取值转化为顺序变量，再计算加权得分 根据分位数（如四分位数）将各项指标转化为1,2,3,4顺序变量 赋予指标权重； 计算用户的加权得分，根据得分对用户进行分层。 方法多维度拆解 辛普森悖论：考察数据的整体，和考察数据的部分会得出相反的结论。 Def： 通过不同维度观察同一组数据，发掘数据波动真正的原因。 拆分角度： 指标构成：根据单一指标的构成进行拆解分析。如：买家城市分为一线、二线、三线。 业务流程：根据业务流程进行拆解分析。如：推荐页购物分为浏览、收藏、加购、购买。 示例： 为何推荐页购买率低？ 从指标构成拆解：对于不同城市，买家对本身没有强烈购买意愿的推荐物品的购买决策取决于当地经济水平和收入剩余，所以一线城市购买率可能高于二三线，但二三线用户基数大，所以拉低整体购买率。可以根据城市经济水平推荐价格更合适的物品。 从业务流程拆解：通过各流程发现，浏览/收藏/加购多，而最终购买少，表明用户是感兴趣的但无法下定决心，猜测可能是不符合心里的预期价格导致推迟或放弃购买。可以通过组合优惠或购买赠送优惠券的方式增大购买的吸引力。 逻辑树分析Def： 逻辑树分析法通过将问题的各个要素以逻辑树的形式体现出来，同时从广度和深度两方面找出问题所在。逻辑树结构能够帮助理清思路及层次，避免混乱、重复、无关的思考，从而有针对性地制定策略。 步骤： 找出核心问题，将其放在逻辑树最上层； 思考并罗列出所有能够影响核心问题的因素或思路，将其罗列在第二层； 思考并罗列出所有能够实现各影响因素或思路的方法，将其罗列在第三层； 针对第三次层方法，思考并罗列相应的解决对策； 查漏补缺。 示例： 增加利润 增加销售额 增加销售量 -&gt; 促销活动 提高单价 -&gt; 组合套装 降低成本 降低原料成本 -&gt; 改变供应商 降低人力成本 -&gt; 智能化设备 假设检验Def： 假设结果：对总体参数提出一个假设值 验证假设：利用样本信息判断这一假设是否成立 显著性水平： 群组分析Def： 同期群分析（Cohort Analysis），又称群组分析，根据初始行为的发生时间将用户划分为不同的群组，观察相似群组用户的行为特征表现。 示例： （注：百分比为留存率 = 基期新增用户某月使用数 / 基期新增用户数） 月份 新增用户 +1 月 +2 月 +3 月 +4 月 1 月 106 62% 51% 43% 34% 2 月 122 60% 48% 36% 3 月 279 49% 27% 4 月 302 35% 横向对比：每个月份的新增用户的在几个月内的留存变化情况 纵向对比：比较不同月份的新增用户的新增数量和留存变化 可以观察到： 1、2 月份新增用户数少于 3、4 月份 — 因为 3、4 月份有拉新活动，新用户明显增多； 3、4 月份的留存率低于同期 1、2 月份的留存率 — 3、4 月份拉新得来的新用户并未有效转化，可能新用户羊毛党居多或者不是目标用户。 如果将观察指标从“留存率”换为“日均使用时长”，则可以从另一个角度了解这段时间的变化情况。 因子分析Def： 同类型因素间的相关性会造成重叠信息的扩大化，增加分类偏差。因子分析是数据消减的常用方法，通过数据聚合，用少数不相关的因子反映多个具有相关性的原始信息，起到剔除相关性和数据降维的作用。 步骤： 适用性检验：原始维度具有相关性才能进行因子分析。 因子提取：提取主要因子。常见方法有主成分分析。 因子旋转：（类似旋转坐标轴）有效区隔各因子的维度特征，使之差异化。 因子载荷：表示因子对维度信息的解释程度。 因子命名：根据维度特征的特点对因子进行命名。 计算因子得分：某因子得分越高表明越具有该因子的特征。 聚类分析Def： 用于非监督分类 — 事先不知该分成几类，探索样本数据的内在规律进行归类，使各类别之间具有显著性差异并描述各类别特征。 分类： 层次聚类（系统聚类） 特点：事先不需要知道分几类，树状图会显示出所有的聚类方案。 步骤： 根据样本距离，将距离最近的样本合为一类； 然后计算所形成的类别与其他样本的距离，对距离最近样本再做合并； 依此类推，直到所有样本聚成一类，形成树状图。 迭代聚类 特点：根据指定类别数进行分类。 步骤： 选择初始类中心点； 将每个点按最近距离进行归类，并重新计算形成的新类的中心点； 不断迭代，直至归类正确（每个点到所归类的中心点最近）。 层次聚类与迭代聚类比较： 层次聚类 迭代聚类 思路 逐层合并 不断迭代，以确定类别中心点和类别构成 类别数 事先未知。树状图会显示所有聚类方案，可以从中选择最优方案 事先已知并需要指定。若聚类效果不好，则需要重新设定类别数，重新聚类 计算速度 由于反复计算距离，当样本量太大或者变量比较多时计算速度比较慢 计算量小，内存占用低，运行速度快。常用于处理多变量、大样本的数据 聚类对象 记录与变量均可 只能对记录聚类 数据类型 连续变量和分类变量均可 只可用连续变量 回归分析Def： 探索两种或两种以上变量间相互依赖的定量关系（方程）。 基本概念（以道格拉斯生产函数为例：$y = A K^\alpha L^\beta \mu$）： 自变量与因变量 自变量是因，常用 x 表示；因变量是果，常用 y 表示。 在道格拉斯生产函数中： y（生产规模）是因变量 K（资本） 和 L（劳动力） 为自变量，受企业影响 A 是外生变量，不受企业影响，是一个常数。 一元与多元 元，指变量的个数。 在道格拉斯生产函数中： 存在 K 和 L 两个变量，因此为二元分析。 线性与非线性 如果回归模型中的所有自变量都是一次幂，则是线性回归；否则，为非线性回归。 在道格拉斯生产函数中： 自变量 K 和 L 分别为 $\alpha ∈ (0, 1)$ 次幂和 $\beta ∈ (0, 1)$ 次幂，因此为非线性回归 为了使用线性回归，可以将非线性函数线性化（通过取 ln 对数化），将 K 和 L 转化为一次幂： $lny = lnA + \alpha lnK + \beta lnB + ln\mu$，设 $y_1 = lny$，$K_1 = lnK$，$B_1 = lnB$，则 $K_1$ 和 $B_1$ 为一次幂，与 $y_1$ 存在线性关系。 回归分析预测步骤： 整理数据源与线性化 调用回归分析 确定常量 A 和系数$\alpha$，$\beta$，求出回归方程 假设检验 原因：经验模型不一定适用于所有场景；根据散点图判断回归模型具有主观性。 检验方法: $T$ 检验 对回归系数的检验，思路：若 X 与 Y 相关，则回归系数≠0。检验标准是 $T_{统计量}$ 的伴随概率 $P &lt; \alpha（显著性水平，默认为 0.05）$。 若检验不通过，以 L（劳动力）为例，表明 L 的回归系数等于 0 不是小概率事件，即 L 与 y 的相关性不强，因此提出 L 这个自变量，重新回归；对重新回归后的系数再次检验，知道所有剩余的自变量都通过检验为止。若用剔除法仍有自变量没有通过检验，则表明回归模型不恰当，需要重新建立回归模型。 $F$ 检验 对回归系数的检验，思路：若回归方程有效，则回归方程对样本数据的信息解释量要高于误差项 $\mu$ 对样本数据信息的解释量。检验标准是 $F_{统计值}$ 的伴随概率 $Significance F &lt; \alpha（显著性水平，默认为 0.05）$。 若检验不通过，则表明从样本数据来看，L（劳动力）和 K（资本）并不能充分解释生产规模的变动，即可能还有其他重要的影响因素没有纳入模型中，需要重新建立模型。 回归预测]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[new_post]]></title>
    <url>%2F2020%2F08%2F29%2Fnew-post%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2020%2F08%2F12%2Ftest%2F</url>
    <content type="text"><![CDATA[Test for hexo.]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 实战问题]]></title>
    <url>%2F2020%2F06%2F20%2Fmysql-practise%2F</url>
    <content type="text"><![CDATA[[Updated] 本文收集了一些常见 MySQL 实践知识点。（基于《MySQL 实战 45 讲》） MySQL 实战问题普通索引和唯一索引的选择普通索引唯一索引查询过程的索引区别过程 对于普通索引来说，查找到满足条件的第一个记录后，需要继续查找下一个记录，直至碰到第一个不满足条件的记录； 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 性能差距大部分情况下，微乎其微。 InnoDB 以“数据页”为单位来读写数据。当需要读一条记录的时候，会将记录所在的页整体（InnoDB 中数据页的默认大小是 16KB）从磁盘读出来放至内存。连续记录大概率会在同一个数据页内，而在内存中寻找下一条记录的操作只是：一次指针寻址 + 一次计算，这在 CPU 的操作成本微乎其微。当连续记录分别在两个数据页的页尾和页头时，操作复杂度较高。 更新过程的索引区别change buffer当更新一个数据页时，若该数据页还没有在内存中，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入该数据页。直到需要查询该数据页的时，才将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 change buffer 使用的是 buffer pool 里的内存，因此不能无限增大。change buffer 可以通过参数 innodb_change_buffer_max_size 来动态地设置大小，如：当参数设置为 50，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。 change buffer 优点： 减少读磁盘（随机 IO 的访问），提升 SQL 语句的执行速度 避免占用内存，提高内存利用率（数据读入内存会占用内存为 RDBMS 开辟的 buffer pool） 使用场景 一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 反之，在写少读多的场景下，每次更新数据都先记录在 change buffer，之后很快就要读数据页并触发 merge，既不能减少随机 IO 访问，又增加了 change buffer 维护成本 merge将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。 触发 merge 的条件： 访问数据页时，change buffer 中有与这个页有关的操作 系统的后台线程定期 merge 在数据库正常关闭（shutdown）的过程中执行 merge 操作。 过程 对于唯一索引： 无法使用 change buffer。 对于唯一索引，所有的更新操作都要先判断这个操作是否违反唯一性约束（即更新后的结果是否唯一），而该过程需要将数据页读入内存，故直接在内存中修改更快，不需要 change buffer。 对于普通索引： 事实上，只有普通索引能用 change buffer。 记录要更新的目标页在内存中，找到目标记录进行更新 记录要更新的目标页不在内存中，将更新记录在 change buffer，语句执行即结束 持久化change buffer 是可以持久化的数据，在内存中有拷贝，也会被写入到磁盘上。 change buffer 有一部分在内存有一部分在 ibdata。 merge 操作会把 change buffer 里相应的数据持久化到 ibdata； redo log 里记录了数据页的修改以及 change buffer 新写入的信息。 如果掉电，持久化的 change buffer 数据已经 merge，不用恢复。主要分析没有持久化的数据情况又分为以下几种： change buffer 写入，redo log 虽然做了 fsync 但未 commit，binlog 未 fsync 到磁盘，这部分数据丢失； change buffer 写入，redo log 写入但没有 commit，binlog 已经 fsync 到磁盘，先从 binlog 恢复 redo log，再从 redo log 恢复 change buffer； change buffer 写入，redo log 和 binlog 都已经 fsync，那么直接从 redo log 里恢复。 注： fsync，同步内存中所有已修改的文件数据到磁盘/储存设备。 索引选择以上可见，类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。建议尽量选择普通索引 + change buffer。 实用场景 在使用机械硬盘时，change buffer 的收效非常显著。如果有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘，那么应该尽量使用普通索引，并把 change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。 特殊场景 如果所有更新后面，往往都伴随着对该数据的查询，那么建议关闭 change buffer。 change buffer 和 redo log两者关系假设更新过程有两个待更新数据页 A、B，A 在内存中，B 在磁盘中。该过程涉及四个部分：内存（buffer pool）、redo log、 数据表空间、系统表空间： 对于在内存中的 A，直接更新内存中的数据表；（写一次内存） 对于不在内存中的 B，在内存的 change buffer 区域，记录下更新信息；（写一次内存） 上述两个动作都记入 redo log 中。（写一次磁盘，顺序写） 若更新后，随即读取这些数据： 对于在内存中的 A，直接从内存读取数据； 对于不在内存中的 B，将 B 从磁盘读入内存中，然后应用 change buffer 操作日志，生成正确的版本再返回。 提升性能上的区别 redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写）； change buffer 主要节省的则是随机读磁盘的 IO 消耗。 MySQL 索引选择异常和处理排查方法 explain 命令查看语句的执行情况； 查看优化器选择的索引是否符合预期； 若 key 选择不符合预期，通过慢查询日志（slow log）查看具体的执行情况； 12set long_query_time=0;&lt;查询语句&gt;; 查看扫描行数，是否进行全表扫描/索引扫描。 优化器逻辑优化器通过选择索引，找到最优的执行方案，并用最小的代价去执行语句。优化器主要会结合以下因素进行综合判断： 扫描行数 是否使用临时表 是否排序 扫描行数扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。 索引统计 统计信息 索引的“区分度”，一个索引上不同的值越多，这个索引的区分度就越好。 索引基数 一个索引上不同的值的个数称为“基数”（cardinality），显然，基数越大，索引的区分度越好。查看一个索引的基数： 1show index from &lt;表名&gt;; 但这个统计结果并不一定准确。 MySQL 获取索引基数 MySQL 通过采样统计得到索引的基数。因为取整张表逐行统计虽然结果精确，但代价太高。 采样统计的方法： InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值； 对 N 个数据页的不同值计算平均值； 用所得的 N 页平均值，乘以索引的页面数，得到该索引基数； 此后，当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。 存储索引统计的两种方式 通过设置参数 innodb_stats_persistent 的值来选择： 设置为 on 表示：统计信息会持久化存储。此时，默认的 N 是 20，M 是 10； 设置为 off 表示：统计信息只存储在内存中。此时，默认的 N 是 8，M 是 16。 显然，不管 N 取 20 还是 8，采样统计的基数都很容易不准。 优化器对扫描行数的判断 执行 SQL 前，根据统计信息来估算扫描记录数； 针对“直接主键扫描”和“索引扫描+主键回表”两种方案，分别估算扫描行数，选择代价更小的方案。 解决方法扫描行数的估计值不准确的情况 统计信息的修正 explain 的结果预估的 rows 值跟实际情况差距比较大的情况下，可使用命令修正统计信息： 1analyze table &lt;表名&gt;; 再执行则可以正确使用索引。 选错索引的情况 采用 force index 强行选择一个索引 MySQL 会根据词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少行。如果 force index 指定的索引在候选索引列表中，就直接选择这个索引，不再评估其他索引的执行代价。 缺点： 写法不佳； 修改了索引名，语句要进行相应改动； 迁库后可能存在兼容性问题。 修改语句，引导 MySQL 使用我们期望的索引 在语义逻辑不变的情况下，修改 SQL 语句，引导其用上我们期待的索引。 如：order by b limit 1 和 order by b,a limit 1，都返回按 b 排序第一行，但后者会使用上 a 的索引。 新建更合适的索引供优化器做选择，或删掉误用的索引 新建更合适的索引的情况比较少见。尤其经过 DBA 索引优化过的库，找到更合适的索引一般比较难。 根据实际情况，若检查发现优化器错误选择的索引其实根本没有必要存在，应予以删除。 字符串字段加索引前缀索引原则使用合适长度的前缀索引，就可以做到既节省空间（减少每个索引长度），又不用额外增加太多的查询成本（回主键索引取数据的次数）。 寻找合适的前缀索引 核心：索引区分度越高越好，那么重复的键值越少。 首先，统计索引上有多少个不同的值 N； 然后，依次截取不同长度的前缀串，查看各前缀串不同的值 M； 最后：选取区分效率最高（M 趋于 N）的截取长度，取该截取长度作前缀索引。 前缀索引对覆盖索引的影响以如下方 SQL 为例 1select id, email from User where email='******@xxx.com'; 若使用对 email 的完整索引，可以了利用覆盖索引，查到结果直接返回，不需要回表再查一遍； 若使用对 email 的前缀索引，即使是 email(14)，在查到结果后，为获取完整信息，都必须回表查找。因为 RDBMS 并不确定前缀索引的定义是否截断了完整信息，此时便用不上覆盖索引对查询性能的优化。 其他索引方式倒序存储 方法 先对字符串数据进行倒序存储，再创建前缀索引。 优点 绕过字符串本身前缀的区分度不够的问题。如：身份证号码。 缺点 不支持范围扫描； 从数据库读写数据时，需要额外进行翻转操作。 hash 字段存储 方法 在表上再创建一个整数字段，保存字符串的校验码，同时在这个字段上创建索引。MySQL 常用校验码 hash 函数：crc32()、crc64()。 优点 控制索引的长度为 4 个字节。 缺点 不支持范围扫描； 校验码可能存在冲突， 查询语句 where 部分要额外判断 id_card 的值是否精确相同。 倒序存储与 hash 字段存储异同 同 都不支持范围查询。 异 额外占用空间：≈ 倒序存储方式在主键索引上，不会消耗额外的存储空间；hash 字段方法需要增加一个字段。实际上，倒序存储使用的前缀长度为了区分度往往不止 4 个字节，二者总体的空间消耗可能相差无几。 CPU 消耗：倒序小 倒序方式每次读写数据时，都需要额外调用一次 reverse 函数；hash 字段的方式需要额外调用一次 hash 函数。从函数的计算复杂度来看，二者 CPU 消耗：reverse 函数 &lt; hash 函数。 查询效率：hash 高 倒序存储 + 前缀索引的方式，可能会增加扫描行数；hash 字段方式的查询性能相对更稳定一些，虽然 hash 有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。 索引方式选择结合业务需求和设备条件，进行合理选择。 MySQL 突然变慢（数据库 flush）脏页当内存数据页跟磁盘数据页内容不一致时，称该内存页为“脏页”。将内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。 引发 flush 的场景InnoDB 的 redo log 写满 场景描述 此时系统会停止所有更新操作，把 checkpoint 往前推进，推进区间的日志所对应的脏页全都 flush 到磁盘上，为 redo log 留出继续写的空间。 性能影响 此时整个系统不能再接受更新，所有更新操作将被阻塞，应该尽力避免。 系统内存不足 场景描述 当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给新的数据页使用。如果淘汰的是“脏页”，就要先将脏页 flush 到磁盘。 “刷脏页一定会写盘”的机制保证了每个数据页只会有两种状态： 内存里存在，内存里就肯定是正确的结果，直接返回； 内存里没有，就可以肯定数据文件上是正确的结果，读入内存后返回。 相比“从内存直接淘汰，等到下次请求从磁盘读入数据页，再拿 redo log 应用”的做法，效率更高。 性能影响 InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态： 未使用的； 使用了的“干净页”； 使用了的“脏页”。 而 InnoDB 的策略是尽量使用内存，因此对于长时间运行的库来说，未被使用的页面很少。当待读入的数据页在内存中不存在，而内存又不足时，缓冲池中将淘汰最久未使用数据页： 若淘汰页是“干净页”，则直接释放用来复用； 若淘汰页是“脏页”，则要先 flush 到磁盘，变成干净页后才能释放复用。 MySQL 认为系统“空闲”的时候进行 flush 场景描述 可以灵活设置 MySQL 定期 flush 的时间，以提高空闲或繁忙时 MySQL 刷“脏页”的效率。 性能影响 MySQL 空闲时的操作，对系统压力不大。 MySQL 正常关闭 场景描述 MySQL 需要正常关闭时，会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。 性能影响 MySQL 关闭前的操作，对性能无影响。 性能影响总结根据上述的第一、二个 flush 场景，对应易引发两种明显影响性能情况： 日志写满，更新全部堵住，写性能跌为 0。这种情况对敏感业务来说，是不能接受的； 一次查询要淘汰的“脏页”个数太多，导致查询响应时间明显变长。 因此，InnoDB 需要控制脏页比例的机制，以避免上述这两种情况。 InnoDB 控制脏页比例的机制 正确地设置 innodb_io_capacity 参数 正确地设置 innodb_io_capacity 参数能够告知 InnoDB 所在主机的 IO 能力，在需要全力刷脏页的时候尽最快的速度。 测试磁盘随机读写的命令： 1fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest innodb_io_capacity 的设置不当可能导致较好的硬件设备也产生脏页累计（刷脏页的速度比脏页生成还慢），影响查询和更新性能。 控制 redo log 写盘速度 参数 innodb_max_dirty_pages_pct 是脏页比例上限（默认值是 75%），InnoDB 会根据当前的脏页比例 M，算出一个范围在 0 到 100 之间的数字，计算公式记为 $F_1(N)$。 InnoDB 每次写入的日志都有一个序号，根据当前写入的序号跟 checkpoint 对应的序号之间的差值 N，算出一个范围在 0 到 100 之间的数字，计算公式记为 $F_2(N)$。$F_2(N)$ 算法比较复杂，其特点为：N 越大，$F_2(N)$ 的值越大。 根据上述算得的 $F_1(N)$ 和 $F_2(N)$ 两个值，取 R = max{$F_1(N)$, $F_2(N)$}，之后引擎就按照 v = innodb_io_capacity × R% 来控制刷脏页的速度。 控制脏页比例 平时应多关注脏页比例，不要让它经常接近 75%。 查看脏页比例： 123456-- 查看 Innodb_buffer_pool_pages_dirtyselect VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty';-- 查看 Innodb_buffer_pool_pages_totalselect VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total';-- 脏页比例 = Innodb_buffer_pool_pages_dirty / Innodb_buffer_pool_pages_totalselect @a/@b; MySQL 刷脏页的“连坐”机制描述对于每个数据页，如果跟它相邻的数据页和它一样也是脏页，就会被放到一起 flush，因此，一个脏页的 flush 可能肯能导致相邻脏页的连锁 flush，使查询更慢。 设置在 InnoDB 中，通过设置 innodb_flush_neighbors 参数来控制这个行为： 值为 1 时，开启“连坐”机制 适合用于使用机械硬盘的情况。 机械硬盘的随机 IOPS（Input/Output Operations Per Second，每秒的读写次数） 一般只有几百，“连坐”机制可以减少随机 IO，大幅提高系统性能。 值为 0 时，仅刷选中脏页自身 适合用于使用 IOPS 比较高的设备的情况，比如 SSD。 该场景下 IOPS 往往不是瓶颈，仅刷选中脏页能够更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。 数据表删掉一半，但表文件大小不变skip。 COUNT(*) 慢count(*) 的实现方式在没有使用 where 时： MyISAM 引擎：将表的总行数存在磁盘上，执行 count(*) 时会直接返回 InnoDB 引擎：把数据逐行读出，然后累积计数。 此时，InnoDB 使用 COUNT(*) 慢于 MyISAM。而使用 where 时，二者都需要逐行读出数据并进行条件过滤。 InnoDB 不存储表行数的原因即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。 TABLE_ROWSshow table status 命令中的 TABLE_ROWS 是根据采样估计得来的，误差可能达到 40% 到 50%，并不准确，所以不能直接使用。 快速获取记录总数的方案用缓存系统保存计数用一个 Redis 服务来保存这个表的总行数。这个表每被插入一行 Redis 计数就加 1，每被删除一行 Redis 计数就减 1。读写效率快，但要注意以下两个问题： 丢失更新 可能情况：数据库更新了一条数据，Redis 在内存中（未永久化）计数 +1 后，发生了异常重启，+1 操作丢失。 解决方案：异常重启后，到数据库中单独执行依次 COUNT(*) 再写入 Redis。鉴于异常重启是小概率事件，该操作成本可以接受。 逻辑不精确 可能情况： 假设有会话 A 和会话 B。A 中的操作有：① Redis 计数 +1;② 插入一行数据。B中的操作有：③ 读 Redis 计数，取最近 100 条记录。当执行顺序为 ①③② 或 ①③② 时，操作是数据不一致的。 在并发系统里面，我们无法精确控制不同线程的执行时刻，因为此可能存在以上数据不一致的操作序列。所以，即使 Redis 正常工作，这个计数值还是逻辑上不精确的。 解决方案：无。 在数据库保存计数将计数直接放到数据库里单独的一张计数表 C 中。 丢失更新 由于 InnoDB 支持崩溃恢复，所以可以解决丢失更新的问题。 逻辑不精确 利用事务特性，实现一致性读。 不同 count 用法count() 的语义count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。故 count(*)、count(主键 id) 和 count(1)，表示返回满足条件的结果集的总行数； count(字段)，表示返回字段不为 NULL 的结果集的总行数 性能差异原则： server 层要什么就给什么； InnoDB 只给必要的值； 优化器只优化了 count(*) 的语义为“取行数”，其他“显而易见”的优化并没有做。 各用法差别如下： count(主键 id)：取 id。 InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断不可能为空（主键不能为空），按行累加。 count(1)：不取值，只置“1”。 InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断不可能为空，按行累加。 与 count(主键 id) 相比，count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。 count(字段)：取值，若类型定义为 not null 直接累加，否则逐行判段不为 null 才累加。 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不可能为 null，按行累加； 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。即第一条原则，server 层要什么字段，InnoDB 就返回什么字段。 count(*)：不取值，直接累加。 count(*) 不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。 按效率排序，count(字段) &lt; count(主键 id) &lt; count(1) ≈ count(*)，因此建议尽量使用 count(*)。 order by 工作原理全字段排序全字段排序执行流程以下方 SQL 为例，其中，id 是主键，a 是普通索引 1select a, b, c from T where a=1 order by b limit N; 初始化 sort_buffer，确定放入 a、b、c 这三个字段； 从索引 a 找到第一个满足 a=1 条件的主键 id； 到主键 id 索引取出整行，取 a、b、c 三个字段的值，存入 sort_buffer 中； 从索引 a 取下一个记录的主键 id； 重复步骤 3、4 直到 a 的值不满足查询条件为止，对应的主键 id； 对 sort_buffer 中的数据按照字段 b 做快速排序； 按照排序结果取前 N 行返回给客户端。 sort_bufferMySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。sort_buffer 中做快速排序，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。 sort_buffer_size sort_buffer_size 是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。 查看一个排序语句是否使用了临时文件的方法如下： 12345678910111213141516171819/* 打开optimizer_trace，只对本线程有效 */SET optimizer_trace='enabled=on'; /* @a保存Innodb_rows_read的初始值 */select VARIABLE_VALUE into @a from performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 执行排序语句 */select a, b, c from T where a=1 order by b limit N;/* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G/* @b保存Innodb_rows_read的当前值 */select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 计算Innodb_rows_read差值 */select @b-@a;-- 通过查看 OPTIMIZER_TRACE 的结果中的 number_of_tmp_files，可以判断是否使用了临时文件 number_of_tmp_files number_of_tmp_files = N（N &gt; 0），表示排序过程中使用的临时文件数为 N。外部排序一般使用归并排序算法，将待排数据分成 N 份做 N 路归并排序。sort_buffer_size 越小，需要分成的份数越多，number_of_tmp_files 的值就越大。 number_of_tmp_files = 0，表示 sort_buffer_size 大于待排数据的大小，排序可以直接在内存中完成。 缺点全字段排序只需读取一遍原表数据，剩下的操作都是在 sort_buffer 和临时文件中执行的。在查询需要返回很多个字段时，sort_buffer 中要放的字段数太多，会导致内存里能够同时放下的行数会很少，需要分成很多个临时文件，排序的性能会很差。 rowid 排序max_length_for_sort_datamax_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。若单行的长度超过这个值，MySQL 就认为单行太大，会从全字段排序算法切换成rowid 排序算法。 采用 rowid 排序时，放入 sort_buffer 的字段只有需要排序的列和主键 id。 rowid 排序执行流程以下方 SQL 为例，其中，id 是主键，a 是普通索引 1select a, b, c from T where a=1 order by b limit N; 初始化 sort_buffer，确定放入两个字段，即 b 和 id； 从索引 a 找到第一个满足 a=1 条件的主键 id； 到主键 id 索引取出整行，取 b、id 这两个字段，存入 sort_buffer 中； 从索引 a 取下一个记录的主键 id； 重复步骤 3、4 直到不满足 a=1 条件为止； 对 sort_buffer 中的数据按照字段 b 进行排序； 遍历排序结果，取前 N 行，并按照 id 的值回到原表中取出 a、b 和 c 三个字段返回给客户端。 与全字段排序区别rowid 排序相比全自担排序，在取最后结果集时多出一步：访问主键索引，以取出其他字段数据。 MySQL 排序算法的选择思路MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。故对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。 对排序算法的优化并非所有的 order by 语句都需要排序操作的，若从表中取出的数据行天然有序，则无需排序。 联合索引以下方 SQL 为例，其中，id 是主键 1select a, b, c from T where a=1 order by b limit N; 创建 (a, b) 的联合索引，由于 (a, b) 这个0联合索引本身有序，那么查询过程中，只要 a=1，b 的值一定有序。 此时，排序执行过程如下： 从索引 (a, b) 找到第一个满足 a=1 条件的主键 id； 到主键 id 索引取出整行，取 a、b、c 三个字段的值，作为结果集的一部分直接返回； 从索引 (a, b) 取下一个记录主键 id； 重复步骤 2、3，直到查到第 N 条记录，或者是不满足 a=1 条件时循环结束。 该查询过程不需要临时表，也不需要排序。 联合索引 + 覆盖索引以下方 SQL 为例，其中，id 是主键 1select a, b, c from T where a=1 order by b limit N; 创建 (a, b， c) 的联合索引，由于 (a, b， c) 这个联合索引本身有序，且索引树上已具备查询所需的所有字段。 此时，排序执行过程如下： 从索引 (a, b， c) 找到第一个满足 a=1 条件的记录，取出其中的 a, b， c 这三个字段的值，作为结果集的一部分直接返回； 从索引 (a, b， c) 取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回； 重复执行步骤 2，直到查到第 N 条记录，或者是不满足 a=1 条件时循环结束。 该查询过程不需要临时表，不需要排序，也不需要回主表取出其它字段数据，直接从索引 (a, b， c) 上取出数据行作结果集。 优点： 联合索引：本身有序。 覆盖索引：索引上的信息足够满足查询请求，则不需要再回到主键索引上去取数据。 缺点： 索引维护有代价，使用需适当。 随机消息的显示skip。 SQL 语句逻辑相同，而性能差异巨大条件字段函数操作的影响关键对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 示例隐式类型转换的影响隐式类型转换的规则 判断方法 1select “10” &gt; 9； 若结果为 1，表明做的是数字比较，规则为“将字符串转成数字” 若结果为 0，表明做的是字符串比较，规则为“将数字转成字符串” 通过验证得知，MySQL 的转换规则为“字符串转换成数字” 数据类型转换导致全索引扫描的原因隐式数据类型转换，本质上相当于对索引字段使用了数据类型转换函数，影响与条件字段函数操作的影响相同。 隐式字符编码转换自动类型转换原则在程序设计语言里面，做自动类型转换的时候，为了避免数据在转换过程中由于截断导致数据错误，也都是“按数据长度增加的方向”进行转换的。 隐式字符编码转换导致全索引扫描的原因隐式字符编码转换，相当于对索引字段使用了字符编码转换函数，影响与条件字段函数操作的影响相同。 优化思路 修改字段的类型/字符集，避免类型/字符集转换； 修改 SQL 语句，将转换函数加在输入参数（右值）上，避免破坏字段有序性; SQL 语句中，尽量不要在字段上做任何操作，以免破坏索引。避免类似写法： 12345SELECT * FROM T WHERE id + 1 = 1000;-- 该 where 条件的写法，将导致无法使用 id 的索引查找，MySQL 也不会主动重写这个语句SELECT * FROM T WHERE id = 1000 - 1;-- 正确做法 只查一行，执行慢查询长时间不返回执行查询语句后，长时间等待没有返回结果，大概率是表被锁住了，可执行 show processlist 命令，查看当前语句处于什么状态。 等 MDL 锁 现象 若执行 show processlist 命令， State 为 “Waiting for table metadata lock”，表示有一个线程正在表上请求或者持有 MDL 写锁，把查询语句堵住了。 处理方法 找到持有 MDL 锁的进程，将其 kill 掉 MySQL 启动时设置 performance_schema=on（相比于设置为 off 会有 10% 左右的性能损失） 查询 sys.schema_table_lock_waits 这张表，找到阻塞进程 1SELECT blocking_id FROM sys.schema_table_lock_waits; kill 阻塞进程 等 flush MySQL 里面对表做 flush 操作的用法:1234-- 用法一：指定表 t 的话，表示只关闭表 tflush tables t with read lock;-- 用法二：没有指定具体的表名，表示关闭 MySQL 里所有打开的表flush tables with read lock; 正常情况下，两种用法执行起来都很快，除非它们被别的线程堵住了。 现象 1select * from information_schema.processlist where id=&lt;pid&gt;; 线程的状态是 “Waiting for table flush”。 处理方法 show processlist 找到阻塞 flush 线程的线程； kill 等行锁 处理方法 通过 sys.innodb_lock_waits 表找谁占着写锁（blocking_pid） 1select * from t sys.innodb_lock_waits where locked_table=`'test'.'t'`\G KILL ，直接断开这个连接。 连接被断开的时候，会自动回滚这个连接里面正在执行的线程，也就释放了行锁。 若使用 KILL QUERY ，只是停止 blocking_pid 线程当前正在执行的语句，而占有行锁的是 update 语句，这个语句已经是之前执行完成了的，此时执行 KILL QUERY 无法让这个事务去掉行锁。 查询慢坏查询不一定是慢查询。可能在数据量大起来之后，执行时间才开始快速上涨。 一致性读事务执行过程中，查询某行数据遇锁（其他事物频繁执行更新操作，生产大量 redo log），当锁释放后，为保持一致性读，进行大量计算找到对应数据版本。 幻读只改一行，锁很多“饮鸩止渴” 提高性能的方法MySQL 保持数据不丢binlog 的写入机制过程 事务执行过程中，先把日志写到 binlog cache 事务提交的时候，再把 binlog cache 写到 binlog 文件中，并清空 binlog cache redo log 的写入机制总结WAL 机制主要得益于 redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快 组提交机制，可以大幅度降低磁盘的 IOPS 消耗 MySQL 出现 IO 性能瓶颈的提升方法 设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog 的写盘次数。 风险：此法基于“额外的故意等待”实现，可能会增加语句的响应时间，但没有丢失数据的风险。 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）。 风险：主机掉电时会丢 binlog 日志。 将 innodb_flush_log_at_trx_commit 设置为 2。 风险：主机掉电的时候会丢数据。 不建议 innodb_flush_log_at_trx_commit 设置成 0。因为把这个参数设置成 0，表示 redo log 只保存在内存中，这样的话 MySQL 本身异常重启也会丢数据，风险太大。而 redo log 写到文件系统的 page cache 的速度也是很快的，所以将这个参数设置成 2 跟设置成 0 其实性能差不多，但这样做 MySQL 异常重启时就不会丢数据了，相比之下风险会更小。 数据库的 crash-safe 如果客户端收到事务成功的消息，事务就一定持久化了 如果客户端收到事务失败（比如主键冲突、回滚等）的消息，事务就一定失败了 如果客户端收到“执行异常”的消息，应用需要重连后通过查询当前状态来继续后续的逻辑。此时数据库只需要保证内部（数据和日志之间，主库和备库之间）一致就可以了 MySQL 保持主备一致主备切换的基本原理假设 A 为主库，B 为备库。 未切换前，客户端的读写都直接访问 A，而备库 B 只是将 A 的更新操作都同步到本地并执行，保证了 A 和 B 的数据相同。 切换后，客户端读写直接访问 B，此时 A 做备库并同步 B 的操作在本地执行。 [注]建议将备库设置为 readonly 模式，即使备库并没有被直接访问。因为： 备库上可能会执行一些运营类的查询语句，设置只读可避免误操作 防止切换逻辑 bug，比如切换过程中出现双写，导致主备不一致 以是否 readonly 状态判断数据库的主/从角色 主备流程M-S 结构：备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。 一个事务日志同步的完整过程是这样的： 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量； 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程： io_thread（与主库建立连接） 和 sql_thread； 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog 发送给 B； 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）； sql_thread 读取中转日志，解析出日志里的命令，并执行。 binlog 三种格式分类 statement 当 binlog_format=’statement‘ 时，binlog 里面记录的就是 SQL 语句的原文，包括注释。 由于 statement 格式下，记录到 binlog 里的是语句原文，因此可能会出现这样一种情况：在主库执行这条 SQL 语句的时候，用的是索引 a；而在备库执行这条 SQL 语句的时候，却使用了索引 b。因此，MySQL 认为这样写是有风险的。 row 当 binlog_format=‘row’ 时，binlog 不会 SQL 原文而是记录 event（包含：记录更新表的 Table_map event、记录更新事务的 event）。 更新事务的 event 中记录了主库中真实更新的主键 id，所以备库不会有选错索引导致主备更新不同行的问题。 mixed（statement 和 row 的混合） 存在原因： 有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式； row 格式很占空间； 比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。一方面，占用了更大的空间；另一方面，写 binlog 需要耗费大量 IO 资源，影响执行速度。 mixed 格式是一种折中方案：MySQL 自行对 SQL 语句进行判断，若可能引起主备不一致，就用 row 格式；否则，就用 statement 格式。 故线上 MySQL 设置的 binlog 格式至少应该为 ‘mixed’，既避免了 ‘statement’ 数据不一致的风险，又在不需要 ‘row’ 模式时提高空间和时间效率。 row 格式越来越常见的原因便于数据恢复。例如： 执行 delete 语句 row 格式的 binlog 会把被删掉的行的整行信息保存起来。若发现误删，直接将 binlog 中记录的 delete 语句转成 insert，把被错删的数据插入回去即可恢复。 执行 insert 语句 row 格式的 binlog 会记录所有插入的字段信息。若发现误插入，直接将 binlog 中记录的 insert 语句转成 delete，把插入的数据删除即可。 执行 update 语句 row 格式的 binlog 会记录修改前整行的数据和修改后的整行数据。若误更新，只需要将更新 event 中前后的两行数据对调一下，再去数据库里面执行即可恢复。 binlog 恢复数据的标准做法用 mysqlbinlog 工具解析出来，然后把解析结果整个发给 MySQL 执行。 12-- 将 &lt;binlog 文件&gt; 里面从 &lt;binlog 起始位置&gt; 字节到 &lt;binlog 结束位置&gt; 字节中间的内容解析出来，放到 MySQL 去执行mysqlbinlog --start-position=&lt;binlog 起始位置&gt; --stop-position=&lt;binlog 结束位置&gt; | mysql -h127.0.0.1 -P13000 -u$user -p$pwd; 循环复制实际生产上使用比较多的是双 M 结构：A 和 B 互为主备关系，切换主备时就不用再修改主备关系。但互为主备关系，则可能发生双方互相发送更新生成 binlog 的循环复制问题。 解决循环复制 规定每个库的 server id 必须不同，若相同则不能设定为主备关系； 备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog； 每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。 日志的执行流程 A 的 binlog 中更新事务记录标注着 A 的 server id； A 的 binlog 传到 B 执行一次后，B 生成的 binlog 的 server id 是 A 的 server id，再传回 A； A 判断收到的 binlog 的 server id 与 自身相同，弃之。 MySQL 保持高可用skip。 备库延迟skip。 主库出错，备库如何操作skip。 读写分离的坑skip。 判断数据库是否出问题skip。 数据误删误删类型使用 delete 语句误删数据行Flashback：通过闪回恢复数据的工具 原理 修改 binlog 的内容，拿回原库重放。 使用前提 确保 binlog_format=row 和 binlog_row_image=FULL。 恢复操作 对于单个事务： 对于 insert 语句，对应的 binlog event 类型是 Write_rows event，把它改成 Delete_rows event 即可； 对于 delete 语句，也是将 Delete_rows event 改为 Write_rows event； 对于 Update_rows，binlog 里面记录了数据行修改前和修改后的值，对调这两行的位置即可。 对于多个事务： Flashback 解析 binlog 后，需要将事务的顺序逆转过来再执行。 注意事项 不建议直接在主库上执行恢复操作。 恢复数据比较安全的做法： 恢复出一个备份，确认过再恢复回主库； 找一个从库作为临时库，在这个临时库上执行这些操作，然后再将确认过的临时库的数据，恢复回主库。 原因：一个在执行线上逻辑的主库，数据状态的变更往往是有关联的。可能由于发现数据问题的时间晚了一点儿，就导致已经在之前误操作的基础上，业务代码逻辑又继续修改了其他数据。所以，如果这时候单独恢复这几行数据，而又未经确认的话，就可能会出现对数据的二次破坏。 事前预防误删的措施： 把 sql_safe_updates 参数设置为 on。 此时，如果忘记在 delete / update 语句中写 where 条件，或者 where 条件里面没有包含索引字段，这条语句的执行就会报错； 该设置下进行全表删除： 在 delete 语句中加上 where 条件，比如 where 1=1； 注：delete 全表效率很低，需要生成回滚日志、写 redo、写 binlog。 从性能角度考虑，全表删除应优先考虑使用 truncate table 或者 drop table 命令，但无法通过 Flashback 来恢复。 因为，即使配置了 binlog_format=row，执行 truncate/drop 时，记录的 binlog 还是 statement 格式。binlog 里面就只有一个 truncate/drop 语句，这些信息是恢复不出数据的。 代码上线前，必须经过 SQL 审计。 使用 drop table 或者 truncate table 语句误删数据表/数据库前提条件： 线上有定期的全量备份 实时备份 binlog 假如有人中午 12 点误删了一个库，恢复数据的流程如下： 取最近一次全量备份，假设这个库是一天一备，上次备份是当天 0 点； 用备份恢复出一个临时库； 从日志备份里面，取出凌晨 0 点之后的日志； 把这些日志，除了误删除数据的语句外，全部应用到临时库。 注意： 为了加速数据恢复，如果这个临时库上有多个数据库，可以在使用 mysqlbinlog 命令时，加上“–database”参数，用来指定误删表所在的库，以避免在恢复数据时还要应用其他库日志的情况。 在应用日志的时候，需要跳过 12 点误操作的那个语句的 binlog： 如果原实例没有使用 GTID 模式 只能在应用到包含 12 点的 binlog 文件的时候，先用“–stop-position”参数执行到误操作之前的日志，然后再用“–start-position”从误操作之后的日志继续执行； 如果实例使用了 GTID 模式 假设误操作命令的 GTID 是 gtid1，那么只需要执行 set gtid_next=gtid1;begin;commit; 先把这个 GTID 加到临时实例的 GTID 集合，之后按顺序执行 binlog 的时候，就会自动跳过误操作的语句。 mysqlbinlog 方法恢复数据不够快的主要原因有两个： 如果是误删表，最好就是只恢复出这张表，也就是只重放这张表的操作，但是 mysqlbinlog 工具并不能指定只解析一个表的日志； 用 mysqlbinlog 解析出日志应用，应用日志的过程就只能是单线程。 加速恢复数据： 并行复制 在用备份恢复出临时实例之后，将这个临时实例设置成线上备库的从库，这样： 在 slave``` 之前，先通过执行 ```change replication filter replicate_do_table 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195 2. 这样做也可以用上并行复制技术，来加速整个数据恢复过程。2. 延迟复制备库 一般的主备复制结构存在的问题是，如果主库上有个表被误删了，这个命令很快也会被发给所有从库，进而导致所有从库的数据表也都一起被误删了。 延迟复制的备库是一种特殊的备库，通过 ```CHANGE MASTER TO MASTER_DELAY = N``` 命令，可以指定这个备库持续保持跟主库有 N 秒的延迟。 比如你把 N 设置为 3600，这就代表了如果主库上有数据被误删了，并且在 1 小时内发现了这个误操作命令，这个命令就还没有在这个延迟复制的备库执行。这时候到这个备库上执行 ```stop slave```，再通过之前介绍的方法，跳过误操作命令，就可以恢复出需要的数据。这样的话，你就随时可以得到一个，只需要最多再追 1 小时，就可以恢复出数据的临时实例，也就缩短了整个数据恢复需要的时间。预防误删库 / 表的方法：1. 账号分离，避免写错命令 - 只给业务开发同学 DML 权限，而不给 truncate/drop 权限。在有 DDL 需求的时候，通过开发管理系统得到支持； - DBA 团队成员，日常也都规定只使用只读账号，必要的时候才使用有更新权限的账号。2. 制定操作规范，避免写错要删除的表名。 - 在删除数据表之前，必须先对表做改名操作。然后，观察一段时间，确保对业务无影响以后再删除这张表。 - 改表名的时候，要求给表名加固定的后缀（比如加 _to_be_deleted），然后删除表的动作必须通过管理系统执行。并且，管理系删除表的时候，只能删除固定后缀的表。#### 使用 rm 命令误删整个 MySQL 实例1. 对于一个有高可用机制的 MySQL 集群来说只要不是恶意地把整个集群删除，而只是删掉了其中某一个节点的数据的话，HA 系统（High Available，高可用性集群）就会开始工作，选出一个新的主库，从而保证整个集群的正常工作。2. 避免 MySQL 集群挂掉：尽量将备份跨机房，最好是跨城市保存。## kill 不掉的语句### 两种 kill 命令- kill query + 线程 id 终止这个线程中正在执行的语句。- kill (connection) + 线程 id 断开这个线程的连接，当然如果这个线程有语句正在执行，也是要先停止正在执行的语句的。### kill 执行过程以 kill query thread_id_B 为例：1. 把 session B 的运行状态改成 THD::KILL_QUERY(将变量 killed 赋值为 THD::KILL_QUERY)；2. 给 session B 的执行线程发一个信号。 发送信号是因为：假设 session B 处于锁等待状态，如果只是把 session B 的线程状态设置 THD::KILL_QUERY，线程 B 无法知道这个状态变化，还是会继续等待。所以发送信号通知 session B 退出等待，来处理这个 THD::KILL_QUERY 状态。 表明： 1. 一个语句执行过程中有多处“埋点”，在这些“埋点”的地方判断线程状态，如果发现线程状态是 THD::KILL_QUERY，才开始进入语句终止逻辑； 2. 如果处于等待状态，必须是一个可以被唤醒的等待，否则根本不会执行到“埋点”处； 3. 语句从开始进入终止逻辑，到终止逻辑完全完成，是有一个过程的。### kill 失效#### kill 失效现象使用了 kill 命令，却没能断开这个连接。再执行 show processlist 命令，看到这条语句的 Command 列显示的是 Killed。show processlist 的特别逻辑：如果一个线程的状态是KILL_CONNECTION，就把Command列显示成Killed。#### 情况一：线程没有执行到判断线程状态的逻辑#### 情况二：终止逻辑耗时较长常见场景：1. 超大事务执行期间被 kill 此时，回滚操作需要对事务执行期间生成的所有新数据版本做回收操作，耗时很长。2. 大查询回滚 如果查询过程中生成了比较大的临时文件，加上此时文件系统压力大，删除临时文件可能需要等待 IO 资源，导致耗时较长。3. DDL 命令执行到最后阶段被 kill 此时，系统需要删除中间过程的临时文件，也可能受 IO 资源影响耗时较久。### 关于客户端的误解#### “Ctrl+C” 可以直接终止线程 -- X正解：“Ctrl+C” 不能直接终止线程。原因：1. 客户端的操作只能操作到客户端的线程，客户端和服务端只能通过网络交互，是不可能直接操作服务端线程的。2. 由于 MySQL 是停等协议，所以当前线程执行的语句还没有返回的时候，再往这个连接里面继续发命令也是没有用的。实际上，执行 “Ctrl+C” 时 MySQL 客户端会另外启动一个连接，然后发送一个 kill query 命令。#### 库里面的表越多，连接越慢 -- X正解：本地客户端执行操作慢，而非连接慢或服务端慢。原因：1. 前提 客户端与服务端建立连接的涉及操作只有： TCP 握手、用户校验、获取权限。与库里的表个数无关。2. 根源 当使用**默认参数连接**的时候，MySQL 客户端会提供一个本地库名和表名补全的功能（Tab 键自动补全表名或者显示提示）。为了实现这个功能，客户端在连接成功后，需要多做一些操作： 1. 执行 show databases； 2. 切到 db1 库，执行 show tables； 3. 把这两个命令的结果用于构建一个本地的哈希表。（此过程耗时较长，尤其在表数很多的时候） 自动补全关闭： 1. 在连接命令中加上 “-A”，就可以关掉这个自动补全的功能，然后客户端就可以快速返回了； 2. 在连接命令中加上 “–quick”(或 “-q”) 参数，也可以跳过这个阶段。#### “–quick” 是一个让服务端加速的参数 -- X正解：“–quick” 只能加快客户端响应，反而可能降低服务端的性能。原因：使用 “–quick” 参数，客户端会使用不缓存的方式。此时，如果客户端本地处理缓慢，就会导致服务端的发送结果被阻塞，让服务端变慢。“–quick” 效果：1. 跳过表名自动补全功能；2. mysql_store_result 需要申请本地内存来缓存查询结果，如果查询结果太大，会耗费较多的本地内存，可能会影响客户端本地机器的性能；3. 不会把执行命令记录到本地的命令历史文件。可见，“–quick” 是让客户端变得更快。MySQL 客户端接收服务端返回结果的方式有两种：1. 本地缓存 在本地开一片内存，先把结果存起来。如果你用 API 开发，对应的就是 mysql_store_result 方法。2. 不缓存 读一个处理一个。如果你用 API 开发，对应的就是 mysql_use_result 方法。## 查大量数据的内存问题### 全表扫描对 server 层的影响#### MySQL S/C 收发数据流程MySQL 是**边读边发的**，而非服务端保存一个完整的结果再集一次性发送。如果客户端接收得慢，会导致 MySQL 服务端由于结果发不出去，使事务的执行时间变长。取数据和发数据的流程：1. 获取一行，写到 net_buffer 中。这块内存的大小是由参数 net_buffer_length 定义的，默认是 16k；2. 重复获取行，直到 net_buffer 写满，调用网络接口发出去；3. 如果发送成功，就清空 net_buffer，然后继续取下一行，并写入 net_buffer；4. 如果发送函数返回 EAGAIN 或 WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。可见：1. 一个查询在发送过程中，占用的 MySQL 内部的内存最大就是 net_buffer_length 这么大，并不会达到 200G；2. socket send buffer 也不可能达到 200G（默认定义 /proc/sys/net/core/wmem_default），如果 socket send buffer 被写满，就会暂停读数据的流程。#### 线程 &quot;sending&quot; 状态线程状态：- &quot;Sending to client&quot; 服务器端的网络栈已写满，线程处于“等待客户端接收结果”的状态。 如果客户端使用–quick 参数，会使用 mysql_use_result 方法：读一行处理一行。假设客户端在遇上逻辑比较复杂的业务处理较慢，就容易产生该状态。- “Sending data” 线程“正在执行”。并不一定是指“正在发送数据”，而可能是处于执行器过程中的任意阶段。 查询语句的状态变化： 1. MySQL 查询语句进入执行阶段后，首先把状态设置成“Sending data”； 2. 然后，发送执行结果的列相关的信息（meta data) 给客户端； 3. 再继续执行语句的流程； 4. 执行完成后，把状态设置成空字符串。故对于正常的线上业务来说，当一个查询的返回结果：- 不会很多 建议使用 mysql_store_result 这个接口，直接把查询结果保存到本地内存。- 很多 建议使用 mysql_use_result 接口。同时，优化查询结果，并评估这么多的返回结果是否合理。### 全表扫描对 InnoDB 的影响InnoDB 引擎内部，由于有淘汰策略，大查询也不会导致内存暴涨。并且，由于 InnoDB 对 LRU 算法做了改进，冷数据的全表扫描，对 Buffer Pool 的影响也能做到可控。#### Buffer Pool作用：- 保存更新结果，配合 redo log，避免随机写盘- 缓存数据页，加速查询内存命中率：- Buffer Pool 对查询的加速效果的指标- 查看 ```SQL show engine innodb status; -- Buffer pool hit rate 即当前的命中率 innodb_buffer_pool_size： 设置 Buffer Pool 的大小 建议大小：物理内存的 60%~80%（内存用尽原则） InnoDB 内存管理 最近最少使用 (Least Recently Used, LRU) 算法：淘汰最久未使用的数据 链表大小固定，表头是最近刚刚被访问过的数据页； 最新请求的数据页会被移至表头； 请求数据页不在链表中，向 BP 申请新数据页移至表头。此时若链表已满，表尾会被情况并放入新数据页内容，再移至表头。 若一次性读取一个较大且久未访问的数据表 当前 BP 中的数据会被全部淘汰掉，导致 BP 的内存命中率急剧下降，磁盘压力增加，SQL 语句响应变慢，对正在做业务服务的库效率影响很大。 为避免该情况，InnoDB 对 LRU 算法做了改进。 InnoDB 改进后的 LRU 算法： 按照 5:3 的比例把整个 LRU 链表分成了 young 区域和 old 区域，LRU_old 指向 old 区域的第一个位置，即整个链表的 5/8 处。 执行过程： 请求访问数据页在 young 区域，将数据页移至 young 区域头部，即表头； 请求的访问数据页不在当前整个链表中，淘汰表尾数据页，新数据页放至 LRU_old 处； old 区域中的每个数据页，每次被访问的时候都要做下面这个判断： 若其在 LRU 链表中存在的时间 &gt; 1 秒，将其移动到链表头部； 如其在 LRU 链表中存在的时间 &lt; 1 秒，位置保持不变。 1 秒这个时间，由参数 innodb_old_blocks_time 控制，其默认值是 1000，单位毫秒。 改进后的 LRU 算法，既使用了 BP，又使 young 区域中的常用数据页不受大规模历史数据查询的影响，保证了 BP 响应正常业务的查询命中率。 查询大量数据的常用做法一次性取 好处：对服务端只全表，只扫描一遍； 坏处：可能会出现大事务。 建议做法分批次取，然后每一批拿到最大的一个id（主键值），下一批查询的时候用 where Id &gt; N。 join 的使用和优化Index Nested-Loop JoinNLJ 执行过程以下方 SQL 为例，t1 和 t2 表结构相同，都有一个主键索引 id 和一个索引 a： 123select * from t1 straight_join t2 on (t1.a=t2.a);-- straight_join 让 MySQL 使用固定的连接方式执行查询，这样优化器只会按照我们指定的方式去 join：在该语句中，t1 是驱动表，t2 是被驱动表。-- 直接使用 join 语句，MySQL 优化器可能会选择表 t1 或 t2 作为驱动表 该 SQL 执行过程： 从表 t1 中读入一行数据 R； 从数据行 R 中，取出 a 字段到表 t2 里去（索引）查找； 取出表 t2 中满足条件的行，跟 R 组成一行，作为结果集的一部分； 重复执行步骤 1 到 3，直到表 t1 的末尾循环结束。 此过程与嵌套查询类似，并可以用上被驱动表的索引。故称 “Index Nested-Loop Join”，简称 NLJ。 NLJ 选择驱动表应该让小表来做驱动表。 假设被驱动表的行数是 M。每次在被驱动表查一行数据，要先搜索索引 a，再搜索主键索引。每次搜索一棵树近似复杂度是以 2 为底的 M 的对数，记为 $\log_2M$，所以在被驱动表上查一行的时间复杂度是 $2·\log_2M$。 假设驱动表的行数是 N，执行过程就要扫描驱动表 N 行，然后对于每一行，到被驱动表上匹配一次。 因此整个执行过程，近似复杂度是 $N + N·2·\log_2M$。可见，N 对扫描行数的影响更大，因此应该让小表来做驱动表。 Simple/Block Nested-Loop Join以下方 SQL 为例，t1 和 t2 表结构相同，都有一个主键索引 id 和一个索引 a，字段 b 上无索引： 1select * from t1 straight_join t2 on (t1.a=t2.b); 由于表 t2 的字段 b 上没有索引，因此再用图 2 的执行流程时，每次到 t2 去匹配的时候，就要做一次全表扫描。该算法被称作 “Simple Nested-Loop Join”。假设 t1 行数为 M，t2 行数为 N，那么总共需要扫描 M*N 行。 当 Simple Nested-Loop Join 中两个表的行数都非常大，称作 “Block Nested-Loop Join” 的算法，简称 BNL。 BNL 执行过程由于被驱动表上没有可用的索引： 把表 t1 的数据读入线程内存 join_buffer 中，由于语句中写的是 select ，因此是*把整个表 t1 放入了内存； 扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回。 与 Simple Nested-Loop Join 相比，二者时间复杂度相同，但 BNL 中的判断 M*N 次判断操作在内存中进行，速度会快很多。 join_bufferjoin_buffer 是一个无序数组，其大小是由参数 join_buffer_size 设定的，默认值是 256k。 若表 t1 很大，join_buffer 一次性放不下 t1 的所有数据，会将数据分段放。此时执行过程变成： 扫描表 t1，顺序读取数据行放入 join_buffer 中，待 join_buffer 满了，继续第 2 步； 扫描表 t2，把 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回； 清空 join_buffer； 继续扫描表 t1，顺序读取最后的 12 行数据放入 join_buffer 中，继续执行第 2 步。 BNL 选择驱动表假设驱动表的行数是 N，被驱动表的行数是 M N 小于 join_buffer 的大小： 两个表都做一次全表扫描，所以总的扫描行数是 M+N； 内存中的判断次数是 M*N。 此时，选择大表还是小表做驱动表，执行耗时是一样的。 N 大于 join_buffer 的大小，驱动表被分成 k 段。因为 N 越大 k 越大，所以 k 可表示为 μ * N，μ ∈ (0,1)： 大表扫描行数为 N，小表扫描行数为 μ N M，扫描总行数为 N + μ N M； 内存中判断的次数为 N * M。 可见： N 越小越好，故应选择小表做驱动表。 k（分段数）越小越好，所以 join_buffer_size 越大越好。 总结是否用 joinexplain 查询语句，查看 Extra 字段： NLJ：可以使用。 BNL：尽量不使用。扫描行数过多，太占用系统资源。 join 驱动表选择总是用“小表”： 小表 两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表。 小表并非数据行数较少的那个表。 临时表与内存表的区别 内存表 使用 Memory 引擎的表创键。 表的数据都保存在内存里，系统重启的时候会被清空，但是表结构还在。 临时表 可以使用各种引擎（包括 Memory 引擎）类型创建。 使用 InnoDB 引擎或者 MyISAM 引擎的临时表，写数据的时候是写到磁盘上的。 临时表的特征 建表语法：create temporary table …。 一个临时表只能被创建它的 session 访问，对其他线程不可见。在session 结束的时候，会自动删除。 临时表可以与普通表同名。 session A 内有同名的临时表和普通表的时候，show create 语句，以及增删改查语句访问的是临时表。 show tables 命令不显示临时表。 临时表的应用临时表一般用于处理比较复杂的计算逻辑。由于临时表是每个线程自己可见的，所以不需要考虑多个线程执行同一个处理逻辑时，临时表的重名问题。在线程退出的时候，临时表也能自动删除，省去了收尾和异常处理的工作。 join 优化 不同 session 的临时表是可重名。 如果有多个 session 同时执行 join 优化，不需要担心表名重复导致建表失败的问题。 不需要担心数据删除问题。 如果使用普通表，在流程执行过程中客户端发生了异常断开，或者数据库发生异常重启，还需要专门来清理中间过程中生成的数据表。 而临时表由于会自动回收，所以不需要这个额外的操作。 分库分表 一般场景 把一个逻辑上的大表分散到不同的数据库实例上。 分库分表系统一般都有一个中间层 proxy。在这个架构中，分区 key 的选择是以“减少跨库和跨表查询”为依据的。如果大部分的语句都会包含 f 的等值条件，那么就要用 f 做分区键。这样，在 proxy 这一层解析完 SQL 语句以后，就能确定将这条语句路由到哪个分表做查询。 查询语句没有用到分区字段： 在 proxy 层的进程代码中实现排序。 这种方式的优势是处理速度快，拿到分库的数据以后，直接在内存中参与计算。 不过，这个方案的缺点也比较明显： 需要的开发工作量比较大。如果涉及到复杂的操作，比如 group by，甚至 join 这样的操作，对中间层的开发能力要求比较高； 对 proxy 端的压力比较大，尤其是很容易出现内存不够用和 CPU 瓶颈的问题。 各分库操作再汇总操作。 把各个分库拿到的数据，汇总到一个 MySQL 实例的一个临时表中，然后在这个汇总实例上做逻辑操作。由于分库操作每个分库的计算量往往不饱和，所以会直接把临时表放到某一个分库上。 临时表可重名原因物理原因使用 InnoDB 引擎创键表时，会创建一个 frm 文件保存表结构定义，还要有地方保存表数据。 frm 文件 存放在临时文件目录下，文件名的后缀是.frm，前缀是“#sql{进程 id}{线程 id} 序列号”。可以使用 select @@tmpdir 命令，来显示实例的临时文件目录。 表中数据的存放方式 在 5.6 以及之前的版本里，MySQL 会在临时文件目录下创建一个相同前缀、以 .ibd 为后缀的文件，用来存放数据文件； 5.7 版本开始，MySQL 引入了一个临时文件表空间，专门用来存放临时文件的数据。因此，我们就不需要再创建 ibd 文件了。 可见，不同 session 创键名为 t 的同名临时表，在存储上两者磁盘文件名是不同的。 table_def_key 原因MySQL 维护数据表，除了物理上要有文件外，内存里面也有一套机制区别不同的表，每个表都对应一个 table_def_key： 普通表的 table_def_key 由“库名 + 表名”得到的，所以如果你要在同一个库下创建两个同名的普通表，创建第二个表的过程中就会发现 table_def_key 已经存在了。 临时表 table_def_key 在“库名 + 表名”基础上，又加入了“server_id+thread_id”。 可见，不同 session 创键名为 t 的同名临时表，两者的 table_def_key 是不同的。 总结不同 session 创键名为 t 的同名临时表，由于它们的 table_def_key 不同，磁盘文件名也不同，因此可以并存。 临时表与主备复制bing log 记录临时表操作在实现上，每个线程都维护了自己的临时表链表。每次 session 内操作表时： 先遍历链表，检查是否有这个名字的临时表，如果有就优先操作临时表； 如果没有临时表，操作普通表； 在 session 结束的时候，对链表里的每个临时表，执行 “DROP TEMPORARY TABLE + 表名”操作。 为了备库需要，binlog 中会记录 “DROP TEMPORARY TABLE” 这条命令。如果不记录临时表的操作，当主库普通表中存在部分由临时表得来的数据，备库在执行时将无法获取这部分数据。因此，主库创建临时表的语句会传到备库执行，备库的同步线程就会创建这个临时表。主库在线程退出的时候，会自动删除临时表，但是备库同步线程是持续在运行的。所以，此时需要在主库上再写一个 DROP TEMPORARY TABLE 传给备库执行。 由于 binlog_format=row 时，binlog 中直接记录操作相关的数据，所以此时跟临时表有关的语句，就不会记录到 binlog 里；而当 binlog_format=statment/mixed 时，binlog 中会记录临时表的操作。 drop table 的改写drop table 命令可以一次删除多个表。比如，在上面的例子中，设置 binlog_format=row，如果主库上执行 “drop table t_normal, temp_t”这个命令，那么 binlog 中就只能记录：DROP TABLE t_normal / generated by server /因为备库上并没有表 temp_t，将这个命令重写后再传到备库执行，才不会导致备库同步线程停止。所以，drop table 命令记录 binlog 的时候，就必须对语句做改写。“/ generated by server /”说明了这是一个被服务端改写过的命令。 主库上不同线程的同名的临时表是没关系的如何传到备库执行MySQL 在记录 binlog 的时候，会把主库执行这个语句的线程 id 写到 binlog 中。这样，在备库的应用线程就能够知道执行每个语句的主库线程 id，并利用这个线程 id 来构造临时表的 table_def_key。 如主库 M 上的两个 session 创建了同名的临时表 t1，这两个 create temporary table t1 语句都会被传到备库 S 上： session A 的临时表 t1，在备库的 table_def_key 就是：库名 +t1+“M 的 serverid”+“session A 的 thread_id”; session B 的临时表 t1，在备库的 table_def_key 就是 ：库名 +t1+“M 的 serverid”+“session B 的 thread_id”。 由于 table_def_key 不同，这两个表在备库的应用线程里面不会冲突。 MySQL 使用内存临时表的时机UNION执行过程 创建一个内存临时表，该临时表中的字段 = 第一个子查询中的查询字段，且为主键字段； 执行第一个子查询，结果存入临时表； 执行第二个子查询，逐行取出结果试图插入临时表： 若该值已存在于临时表，违反唯一性约束，则插入失败，跳过； 若该值临时表中不存在，则插入； 从临时表中按行取出数据，返回结果，并删除临时表。 内存临时表大小 内存临时表的大小是有限制的，通过参数 tmp_table_size 控制，默认是 16M。 当内存临时表大小到达了上限，系统会将内存临时表转成磁盘临时表，磁盘临时表默认使用的引擎是 InnoDB。 GROUP BY执行过程 创建一个内存临时表，临时表字段 = 待查询字段，聚合键作为主键； 通过表索引找到所需要的数据，插入临时表； 对临时表数据进行排序，返回结果，并删除临时表； 无需排序的情况下，可通过 ORDER BY NULL 取消排序这一步。 优化：索引不论是使用内存临时表还是磁盘临时表，GROUP BY 逻辑都需要构造一个带唯一索引的内存临时表，目的时为了排序。 如果可以确保输入的数据是有序的，就可以拿到 GROUP BY 的结果，不需要临时表，也不需要再额外排序。故可以： MySQL 5.6 及之前的版本 对聚合键、待查数据创建索引。 MySQL 5.7 以后版本 使用 generated column 机制实现列数据的关联更新：创建一个新列，然后在新列上创建一个索引。 优化：直接排序如果遇到数据量很大，又不适合创建索引的场景，此时排序无法避免： 在内存临时表足够大小的情况下，应优先使用内存临时表； 但在数据量远大于内存临时表大小的情况下，可以绕过存入内存临时表这一环，直接走磁盘临时表效率更高。 以下方 SQL 为例： 1SELECT SQL_BIG_RESULT &lt;列名&gt;, ... FROM t GROUP BY ... 由于磁盘临时表是 B+ 树存储，存储效率不如数组，MySQL 优化器从磁盘空间考虑直接用数组存数据。执行过程如下： 初始化 sort_buffer，放入整型字段 = 聚合键字段； 通过扫描表 t 的索引依次取出目标结果存入 sort_buffer 中； 扫描完成后，对 sort_buffer 的字段做排序（如果 sort_buffer 内存不够用，就会利用磁盘临时文件辅助排序）； 排序完成后，就得到了一个有序数组。 总结：内粗临时表使用场景 SQL 执行过程无法即读即得目标结果，需要额外的内存来保存中间结果； join_buffer 是无序数组，sort_buffer 是有序数组，内存临时表是二维表结构。如果 SQL 执行逻辑需要用到二维表特性，就会优先考虑使用临时表。如：UNION、GROUP BY。 InnoDB 和 Memory 引擎skip。 自增主键为何不连续skip。 insert 语句的锁insert … selectinsert 循环写入 insert 唯一键冲突insert into … on duplicate key update快速复制一张表为了避免对源表加读锁，稳妥的方案是先将数据写到外部文本文件，然后再写回目标表。 使用 mysqldump使用 mysqldump 命令将数据导出成一组 INSERT 语句，再执行插入。 导出 INSERT 语句通过以下 mysqldump 命令生成包含 INSERT 语句的 t.sql 文件。 1mysqldump -h$host -P$port -u$user --add-locks=0 --no-create-info --single-transaction --set-gtid-purged=OFF db1 t --where=&quot;a&gt;900&quot; --result-file=/client_tmp/t.sql 参数： –single-transaction 在导出数据的时候不需要对表 db1.t 加表锁，而是使用 START TRANSACTION WITH CONSISTENT SNAPSHOT 的方法； –add-locks 设置为 0 表示在输出的文件结果里，不增加” LOCK TABLES t WRITE;” ； –no-create-info 不需要导出表结构； –set-gtid-purged 设置为 off 表示不输出跟 GTID 相关的信息； –result-file 指定输出文件的路径，其中 client 表示生成的文件是在客户端机器上的。 执行 INSERT 语句通过下面这条命令，执行 t.sql 文件中的 INSERT 语句。 1mysql -h127.0.0.1 -P13000 -uroot db2 -e &quot;source /client_tmp/t.sql&quot; 执行流程： 打 t.sql 开文件，默认以分号为结尾读取逐条 SQL 语句； 将 SQL 语句发送到服务端执行。 使用 csv导出为 .csv 文件MySQL 提供了下面的 SQL 语法，用来将查询结果导出成.csv 文件到服务端本地目录： 1SELECT * FROM t WHERE ... into outfile '/server_tmp/t.csv'; 注意事项： 该语句会将结果保存在服务端。 如果你执行命令的客户端和 MySQL 服务端不在同一个机器上，客户端机器的临时目录下是不会生成 t.csv 文件的。 into outfile 指定了文件的生成位置（/server_tmp/），这个位置必须受参数 secure_file_priv 的限制。参数 secure_file_priv 的可选值是： 如果设置为 empty，表示不限制文件生成的位置，这是不安全的设置； 如果设置为一个表示路径的字符串，就要求生成的文件只能放在这个指定的目录，或者它的子目录； 如果设置为 NULL，就表示禁止在这个 MySQL 实例上执行 select … into outfile 操作。 该语句不会帮你覆盖文件。 你需要确保 /server_tmp/t.csv 这个文件不存在，否则执行语句时就会因为有同名文件的存在而报错。 该语句生成的文本文件中，原则上一个数据行对应文本文件的一行。 如果字段中包含换行符，在生成的文本中也会有换行符。不过类似换行符、制表符这类符号，前面都会跟上“\”这个转义符，这样就可以跟字段之间、数据行之间的分隔符区分开。 将 .csv 文件导入目标表使用 load data 命令： 1load data infile &apos;/server_tmp/t.csv&apos; into table &lt;目标表&gt;; 执行过程： 打开文件 /server_tmp/t.csv，以制表符 (\t) 作为字段间的分隔符，以换行符（\n）作为记录之间的分隔符，进行数据读取； 启动事务。 判断每一行的字段数与表 &lt;目标表&gt; 是否相同： 若不相同，则直接报错，事务回滚； 若相同，则构造成一行，调用 InnoDB 引擎接口，写入到表中。 重复步骤 3，直到 /server_tmp/t.csv 整个文件读入完成，提交事务。 如果 binlog_format=statement，由于 /server_tmp/t.csv 文件只保存在主库所在的主机上，如果只是把 load data 语句原文写到 binlog 中，在备库执行的时候，备库的本地机器上没有这个文件，就会导致主备同步停止。所以，完整流程如下： 主库执行完成后，将 /server_tmp/t.csv 文件的内容直接写到 binlog 文件中； 往 binlog 文件中写入语句 load data local infile ‘/tmp/SQL_LOAD_MB-1-0’ INTO TABLE t； 把这个 binlog 日志传到备库； 备库的 apply 线程在执行这个事务日志时： 先将 binlog 中 t.csv 文件的内容读出来，写入到本地临时目录 /tmp/SQL_LOAD_MB-1-0 中； 再执行 load data 语句，往备库的 db2.t 表中插入跟主库相同的数据。 备库执行的 load data 语句里多了一个“local”，意为“将执行这条命令的客户端所在机器的本地文件 /tmp/SQL_LOAD_MB-1-0 的内容，加载到目标表 t 中”。 故，load data 命令有两种用法： 不加“local” 读取服务端的文件，这个文件必须在 secure_file_priv 指定的目录或子目录下； 加上“local” 读取的是客户端的文件，只要 mysql 客户端有访问这个文件的权限即可。这时候，MySQL 客户端会先把本地文件传给服务端，然后执行上述的 load data 流程。 select … into outfile优点：该方法是最灵活的，支持所有的 SQL 写法。 缺点:每次只能导出一张表的数据，不会生成表结构文件, 所以导数据时还需要单独的命令得到表结构定义。 mysqldump 提供了一个–tab 参数，可以同时导出表结构定义文件和 csv 数据文件。使用方法如下： 1mysqldump -h$host -P$port -u$user ---single-transaction --set-gtid-purged=OFF db1 t --where=&quot;a&gt;900&quot; --tab=$secure_file_priv 该命令会在 $secure_file_priv 定义的目录下，创建一个 t.sql 文件保存建表语句，同时创建一个 t.txt 文件保存 CSV 数据。 物理拷贝直接拷贝 .frm 和 .ibd此法行不通。 一个 InnoDB 表，除了包含这两个物理文件外，还需要在数据字典中注册。直接拷贝这两个文件的话，因为数据字典中还没有注册这个表，系统是不会识别和接受它们的。 可传输表空间MySQL 5.6 及之后的版本引入了可传输表空间(transportable tablespace) 的方法，可以通过导出 + 导入表空间的方式，实现物理拷贝表的功能。 假设我们现在的目标是在 db1 库下，复制一个跟表 t 相同的表 r，具体的执行步骤如下： 执行 create table r like t;，创建一个相同表结构的空表； 执行 alter table r discard tablespace;，这时候 r.ibd 文件会被删除； 执行 flush table t for export;，这时候 db1 目录下会生成一个 t.cfg 文件； 在 db1 目录下执行 cp t.cfg r.cfg;，cp t.ibd r.ibd;；这两个命令（这里需要注意的是，拷贝得到的两个文件，MySQL 进程要有读写权限）； 执行 unlock tables;，这时候 t.cfg 文件会被删除； 执行 alter table r import tablespace;，将这个 r.ibd 文件作为表 r 的新的表空间，由于这个文件的数据内容和 t.ibd 是相同的，所以表 r 中就有了和表 t 相同的数据。 至此，拷贝完毕。 注意事项： 在第 3 步执行完 flsuh table 命令之后，db1.t 整个表处于只读状态，直到执行 unlock tables 命令后才释放读锁； 在执行 import tablespace 的时候，为了让文件里的表空间 id 和数据字典中的一致，会修改 r.ibd 的表空间 id。而这个表空间 id 存在于每一个数据页中。因此，如果是一个很大的文件（比如 TB 级别），每个数据页都需要修改，所以你会看到这个 import 语句的执行是需要一些时间的。当然，如果是相比于逻辑导入的方法，import 语句的耗时是非常短的。 grant 后 flush privilege 的问题skip。 分区表的使用skip。 自增 id 用尽skip。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 基础]]></title>
    <url>%2F2020%2F05%2F30%2Fmysql-basic%2F</url>
    <content type="text"><![CDATA[[Updated] 本文整理了 MySQL 基础知识。（基于《SQL基础教程》） MySQL 基础基础架构Server 层包括：连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 连接器负责跟客户端建立连接、获取权限、维持和管理连接： 建立连接 客户端连接服务端，完成 TCP 握手后，连接器开始进行用户认证。 获取权限 用户认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。即使用管理员账号对当前连接用户进行权限修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 连接维持 连接完成后，如果你没有后续的动作，连接就处于空闲状态（可使用 show processlist 命令察看）。在一定时间（由参数 wait_timeout 控制，默认 8h）内一直处于空闲，连接器会自动断开。 连接类型： 短连接 每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 长连接 连接成功后，如果客户端持续有请求，则一直使用同一个连接。由于建立连接的过程通常是比较复杂的，所以应尽量减少建立连接的动作，即尽量使用长连接。 如何解决长连接累积消耗内存导致 OOM？ MySQL 在执行过程中临时使用的内存资源是管理在连接对象里面的，直至连接断开时才释放，因此长连接累积可能导致 OOM。解决方案： 定期断开长连接 使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 重新初始化连接资源 MySQL 5.7 之后的版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。该过程无需重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存 不建议使用的原因 只要对一个表执行更新操作，该表上所有的查询缓存都会被清空。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非业务是静态表，很少更新数据，如：系统配置表。 MySQL 8.0 后的版本以移除查询缓存功能。 分析器 词法分析 识别 SQL 语句的成分（关键字、表名、列名、运算符、函数……）. 语法分析 根据语法规则和词法分析的结果生成一颗解析树，通过检查解析树的合法性判断你输入 SQL 是否满足 MySQL 语法。比如：表和列名是否存在，别名歧义等。 优化器 在表里面有多个索引的时候，决定使用哪个索引； 在一个语句有多表关联（join）的时候，决定各个表的连接顺序。 执行器 判断当前连接对操作表的权限； 打开表，根据表的引擎定义调用引擎借口进行操作（每取一行调用一次引擎接口）。 引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。 InnoDBInnoDB 是现在最常用的存储引擎，MySQL 5.5.5 版本开始成为默认存储引擎。InnoDB 基于 B+ 树，采用聚集索引，即索引文件本身就是数据文件。 MyISAMMyISAM 也是基于 B+ 树，但采用非聚集索引，即索引和实际存储数据分开，其索引指针指向存储实际数据地址。 InnoDB 和 MyISAM 区别 InnoDB MyISAM 事务支持 支持 不支持 存储结构 表结构、数据文件 表定义文件.frm、数据文件.MYD、索引.MYI 操作效率 INSERT、DELETE、UPDATE 更安全更快 SELECT 更快 全表行数 不保存，需要全表扫描 保存，直接读出；使用 WHERE 需要全表扫描 索引 InnoDB 5.6+ 支持全文索引，不支持压缩索引 支持全文索引、压缩索引 主键 无主键时，自动创建自增索引作主键 允许无任何主键和索引 外键 支持 不支持 清表操作 逐行删除 重建表 锁支持 行级锁、表级锁 只表级锁 适用场景 i. 并发，要求可靠性高、事务处理 i. 查询、插入频繁，修改不频繁 ii. 修改频繁 ii. 常做全表 COUNT 事务事务定义 begin/start transaction 和 commit 之间的操作集合是一个事务。【注】begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动 没有显式地使用 begin/commit 时，每个 SQL 语句本身就是一个事务，语句完成的时候会自动提交。 事务特性：ACIDAtomicity 原子性 定义 事务是一个不可再分割的工作单位，事务中的操作要么都发生，要么都不发生。 实现 日志 Consistency 一致性 定义 事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。这是说数据库事务不能破坏关系数据的完整性以及业务逻辑上的一致性。 实现 设置约束和触发器 事务隔离 锁机制 Isolation 隔离性 定义 多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务运行效果。 实现 事务隔离 Durability 持久性 定义 事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。 实现 WAL（Write-Ahead Logging） 总结 DBMS 采用日志系统来保证事务的原子性、一致性和持久性。 日志系统记录了事务对数据库所做的更新，如果某个事务在执行过程中发生错误，就可以根据日志，撤销事务对数据库已做的更新，使数据库退回到执行事务前的初始状态。 DBMS 采用锁机制来实现事务的隔离性。 当多个事务同时更新数据库中相同的数据时，只允许持有锁的事务能更新该数据，其他事务必须等待，直到前一个事务释放了锁，其他事务才有机会更新该数据。 日志系统 key word redo log binlog 保证事务的原子性、一致性、持久性 redolog格式一个大小固定的循环队列，记录“哪个数据页做了哪些改动”。 WAL 关键 Write-Ahead Logging：先写日志，再写磁盘 过程 记录需要更新，先写入 redo log，并在内存更新 InnoDB 择机（系统空闲时）将更新记录更新到磁盘 因为 redo log 是固定大小的（类似循环队列），当没有空间存储更新记录时（队满，无法入队），InnoDB 会将 redo log 数据清除一部分（出队）并刷入磁盘 作用 保证 crash-safe：即使数据库发生异常重启，之前提交的记录都不会丢。 binlog两种格式 statement 格式 记录完整的 sql 语句。 row 格式 记录数据行的内容，包含两部分：更新前、更新后。 特点 binlog 日志只能用于归档 只有 binlog 的 MySQL 不具备 crash-safe 能力 redolog 和 binlog 区别 功能层次 redo log 是引擎层 InnoDB 引擎特有的 binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用 记录内容 redo log 是物理日志，记录的是“在某个数据页上做了什么修改” binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ” 空间大小 redo log 是循环写的，空间固定会用完 binlog 是可以追加写入的，即 binlog 文件写到一定大小后会切换到下一个新文件，并不会覆盖以前的日志 redo log 两阶段提交redo log 的写入有两个步骤：prepare 和 commit，即“两阶段提交”。 过程以 update 语句将某行某字段 c 加 1 为例： 执行器向引擎取该行数据 行数据所在数据页存在内存，直接返回给执行器；若不存在，从磁盘取出所在数据页，再向执行器返回 执行器将值加 1，得到新的行数据，再调用引擎接口写入新的行数据 引擎将行数据更新到内存，同时将更新操作写入 redo log，此时，redo log 处于 prepare 状态，并告知执行器执行完成，可以提交事务 执行器生成该更新操作的 binlog，并将其写入磁盘 执行器调用引擎提交事务借口，将刚写入的 redo log 改成 commit 状态，至此更新完成 目的redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑一致。 不用两阶段提交情况下，假设执行 update 语句某行某字段 c 值由 0 更新为 1 ，过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，可能发生以下错误： 先写 redo log 后写 binlog。 假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。 redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。 但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。 如果后期需要用这个 binlog 来恢复临时库的话，由于这个 binlog 丢失该 update 语句，所恢复的临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，异于原库。 先写 binlog 后写 redo log。 如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。 如果后期用这个 binlog 恢复数据时就多了一个更新事务，恢复出来该行 c 的值就是 1，与原库的值不同。 数据误删恢复过程 首先找到最近一次全量备份，从该全量备份恢复到临时库 从备份时间点开始，将备份的 binlog 依次取出来，重放到误删表之前的那个时刻 此时临时库就跟误删之前的线上库一样了，再将表数据从临时库取出，按需恢复到线上库即可 事务隔离MVCC定义MVCC(Mutil-Version Concurrency Control)，即多版本并发控制，在 RDBMS 中实现对数据库的并发访问。 在 MySQL 里，有两个“视图”的概念： 视图 view view 是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。 创建视图的语法是 create view … ，其查询方法与表一样。 一致性读视图 consistent read view InnoDB 在实现 MVCC 时用到的一致性读视图，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。 快照在可重复读隔离级别下，事务在启动时会拍一个快照，该快照是基于整个库的。基于整个库意味着：一个事务内，整个库的修改对于该事务都是不可见的(对于快照读的情况)。如果在事务内 select 表 t1，另外的事务执行了DDL 表 t1，根据发生时间，要么锁住要么报错。 InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。 多版本 transaction id InnoDB 中每个事务都有唯一的事务 ID，即 transaction id，在事务开始的时候向 InnoDB 的事务系统申请，按申请顺序严格递增的。 row trx_id 每行数据都有多个版本。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。 即将“更新该数据行的事务 ID ”作为“产生该数据行版本的 ID”。 版本查看 数据每次更新会产生 undo log 并且记录当前版本的 row trx_id。通过当前版本 + undo log 依次往前计算，可以顺次得到之前的版本。 数据版本的可见性规则视图数组和高水位，就组成了当前事务的一致性视图（read-view）： 视图数组 InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。 高低水位 数组里面事务 ID 的最小值记为低水位；当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。 基于数据的 row trx_id 和这个一致性视图的对比结果得到数据可见性。对于当前事务的启动瞬间来说，一个数据行的 row trx_id 有以下几种可能： [已提交事务|&lt;低水位&gt;|未提交事务集合|&lt;高水位&gt;|未开始事务] 如果在 “已提交事务” 部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果在 “未开始事务” 部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果在 “未提交事务集合” 部分，那就包括两种情况 若 row trx_id 在图数组中，表示这个版本是由还没提交的事务生成的，不可见； 若 row trx_id 不在视图数组中，表示这个版本是已经提交了的事务生成的，可见。 更新逻辑 “当前读”（current read） 更新数据都是先读后写的，而这个读，只能读当前（数据行的最新版本）的值，称为“当前读”（current read）。 根据两阶段锁协议，当前事务的当前读必须要读取最新版本，而且必须加锁，所以要等到其他事务更新完毕，当前事务才能完成当前读。 除了 update 语句外，select 语句如果加锁（lock in share mode 或 for update），也是当前读。 总结 InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。 普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。 可重复读实现的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。 读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是： 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 可见性总结 对于可重复读，查询只承认在事务启动前就已经提交完成的数据； 对于读提交，查询只承认在语句启动前就已经提交完成的数据； 对于当前读，总是读取已提交完成的最新版本。 问题 为什么表结构不支持“可重复读”？ 因为表结构没有对应的行数据，也没有 row trx_id，因此只能遵循当前读的逻辑。 为什么 rr 能实现可重复读而 rc 不能？ 快照读的情况下, rr 不能更新事务内的 up_limit_id（高水位）, 而 rc 每次会把 up_limit_id 更新为快照读之前最新已提交事务的 transaction id,则 rc 不能可重复读 当前读的情况下, rr 是利用 record lock + gap lock 来实现的，而 rc 没有 gap，所以 rc 不能可重复读 事务执行问题脏读（dirty read） 关键：读取未提交数据。 事务 A 读取了事务 B 未提交数据后，事务 B 发生执行错误并进行回滚，那么事务 A 读取数据即为脏数据。 解决：读提交、可重复读、串行化。 丢失更新 关键：更新覆盖。 第一类： 事务 A 和事务 B 都瞄准了同一数据行，事务 A 回滚时将事务 B 的更新数据覆盖。 解决：可重复读、串行化。 第二类： 事务 A 和事务 B 都瞄准了同一数据行，并先后执行了更新操作，那么事务 A 的更新数据就有可能被事务 B 覆盖。 解决：悲观锁、乐观锁。 不可重复读（non-repeatable read） 关键：前后多次读取，数据内容不一致。 事务 A 执行内容较多，在较长的时间间隔先后读取了同一行数据，期间该行数据被其他事务执行了更新操作（insert / update），导致前后读取的数据不一致，无法取到重复的数据。 解决：行级锁，锁定该行。 幻读（phantom read） 关键：前后多次读取，数据总量不一致。 事务 A 执行内容较多，在较长的时间间隔先后查询了数据总量，期间其他事务执行了增减数据的操作（insert / update），导致前后统计的数据总量不一样，仿佛产生幻觉。 解决：表级锁，锁定整张表。 隔离级别为解决事务执行过程中可能出现的问题，产生了“隔离级别”的概念。 读未提交（read uncommitted）一个事务还未提交，它所做的变更就可以被别的事务看到。 读提交（read committed）一个事务提交之后，它所做的变更才可以被别的事务看到。 可重复读（repeatable read）一个事务执行过程中看到的数据始终一致，未提交的更改对其他事务是不可见的。 串行化（serializable）对应一个记录会加读写锁，出现冲突的时候，后访问的事务必须等前一个事务执行完成才能继续执行。 隔离级别程度 ↓程度递增→ 脏读 丢失更新 不可重复读 幻读 读未提交 √ √ √ √ 读提交 X √ √ √ 可重复读 X X X √ 串行化 X X X X 注意： √ - 允许，X - 禁止 隔离剂级别越高越安全，但性能越低 索引 索引的作用：提高数据查询的效率，类似书的目录 索引常见模型 索引模型是数据库底层存储的核心，有助于理解分析数据库的适用场景 哈希表 特点：键值对 优点：由哈希算法直接由值得到位置 缺点：可能发生冲突。冲突键一般用链表解决，退化为普通查询 适用场景：等值查询 效率：无冲突O(1);冲突O(n) 有序数组 特点：数据按顺序存储 优点：查询效率高，等值查询和范围查询皆适用 缺点：更新效率低 适用场景：只适用于静态存储引擎 效率：二分法O(log(N)) 搜索树二叉树是树搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树 不使用二叉树原因索引不止存在内存中，还要写到磁盘上。对于高为 h 的搜索树，一次查询可能需要访问 h 个数据块，即读 h 次盘。而二叉树的高度往往过高，会导致查询过程中在磁盘读取上消耗过多时间 为何使用 N 叉树为了查询过程访问尽量少的数据块，适用 N 叉树降低树高，“N” 取决于数据块的大小 B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数，被广泛应用在数据库引擎中 InnoDB 的索引模型 在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同 索引模型：B+树在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表，数据存储在 B+ 树中 索引类型 主键索引的叶子节点存的是整行数据在 InnoDB 里，主键索引也被称为聚簇索引（clustered index） 非主键索引的叶子节点内容是主键的值在 InnoDB 里，非主键索引也被称为二级索引（secondary index） 基于主键索引和普通索引的查询的区别 主键索引只要根据主键值搜索主键索引树即可拿到数据 普通索引先搜索索引树拿到主键值，再到主键索引树搜索一次(回表) 基于非主键索引的查询需要多扫描一棵索引树，故应该尽量使用主键查询 索引维护为维护索引有序性，根据 B+ 树模型，一个数据页满了会进行页分裂(新增一个数据页)，导致性能下降，空间利用率降低大概 50%；反之，相邻数据页利用率很低时会进行页合并 自增索引自增主键是指自增列上定义的主键 定义： 1NOT NULL PRIMARY KEY AUTO_INCREMENT 特点： 插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值 是否应该使用自增索引 结论：从性能和存储空间方面考量，自增主键往往是更合理的选择，K-V 场景除外 适用场景 性能角度：递增插入。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂；若使用业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高 空间角度：主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小 特殊场景 K-V 场景 只有一个索引 该索引必须是唯一索引 由于没有其他索引，故不用考虑其他索引的空间问题，此时优先考虑“尽量使用主键查询”原则 覆盖索引覆盖索引：待查值已在索引树上（即索引树已覆盖查询需求），因此可以直接提供查询结果，而不需要回表。使用覆盖索引可以减少树的搜索次数（IO磁盘读写次数），显著提升查询性能 最左前缀原则B+Tree 这种索引结构，可以利用索引的”最左前缀”来定位记录：可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 联合索引内的字段顺序安排 关键：索引的复用能力。 因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引。因此，第一原则是，若通过调整索引字段顺序可以少维护一个索引，那么该顺序往往就是需要优先考虑采用的。 索引下推MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 普通索引和唯一索引的选择从查询语句看 对于普通索引 查找到第一个满足条件的记录后，继续查找下一个记录，直至碰到第一个不满足条件的记录 对于唯一索引 由于索引定义了唯一性，查找到第一个满足条件的记录即停止检索 因为 InnoDB 的数据是按数据页为单位来读写的，寻找记录时会将记录所在的整个数据页（对于整型字段，一个数据页可以放近千个 key）读取到内存，故普通索引虽然会比唯一索引多出几次寻找和判断，但是对于现在的 CPU 来说计算性能差距微乎其微（记录跨两个数据页的情况除外，发生概率较低）。 change buffer当需要更新一个数据页： 数据页在内存中则直接更新 若数据页不在内存中，为不影响数据一致性，InnoDB 会将更新操作缓存在 change buffer 中，而不会从磁盘中读取所在数据页进行更新 在下次查询需要访问该数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作，再查询该数据页，以保证数据逻辑的正确性 锁机制根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。 全局锁全局锁作用对整个数据库实例加锁，使整个数据库处于只读状态，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 加全局读锁命令：FTWRL1Flush tables with read lock; 使用场景全库逻辑备份，即“把整库每个表都 select 出来存成文本”。 缺点 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。 为何加锁不加锁的话，备份系统备份得到的库不是一个逻辑时间点，视图逻辑不一致。 不建议使用 “set global readonly=true” 设置全库只读的原因 修改 global 变量的方式影响面更大。 在有些 DBMS 中，readonly 的值会被用来做其他逻辑，比如判断一个库是主库还是备库。 异常处理机制上有差异。 如果执行 FTWRL 命令之后由于客户端发生异常断开，MySQL 会自动释放全局锁，整个库恢复可以正常更新的状态。 而将整个库设置为 readonly 之后，如果客户端发生异常，数据库会一直保持 readonly 状态，就会导致整个库长时间处于不可写状态，风险较高。 表级锁表级锁分类 表锁 元数据锁（meta data lock，MDL) 表锁 作用 若某线程 “lock tables t1 read, t2 write;”： 该线程自身只能 “读表 t1、读写 t2”，而不能访问其他表 其他线程 “写表 t1、读写 t2” 的语句都会被阻塞 命令 12345-- 上锁lock tables … read/write;-- 释放锁unlock tables; 使用场景 在没有更细粒度的锁的情况下，用于处理并发，如 MyISAM 引擎。而对于拥有更细粒度的行锁 InnoDB 一般使用行锁控制并发，因为行锁的影响面小于表锁。 MDL 作用 MDL 不需要显式使用，在访问一个表的时候会被自动加上，以保证读写的正确性。 MySQL 5.5+ 版本引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作（如：加字段）的时候，加 MDL 写锁。 特点 读锁之间不互斥 可以有多个线程同时对一张表增删改查。 写锁 与 读/写锁 都互斥 如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行，以保证变更表结构操作的安全性。 事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。 如何安全地给小表加字段? 解决长事务 DDL 要变更时有长事务在执行，可以考虑先暂停 DDL，或者 kill 掉这个长事务。 在 DDL 语句里面设定等待时间 在指定的等待时间里拿不到 MDL 写锁，先放弃更高改，后期再进行重试，以免阻塞后面的业务语句。 行锁两阶段锁 定义 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。 建议 如果事务中需要锁多个行，应该把最可能造成锁冲突、最可能影响并发度的锁尽量在事务中往后放，使行锁的时间最短，以最大程度地减少了事务之间的锁等待，提升了并发度。 死锁当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。 死锁解决策略 直接进入等待，直到超时。 设置 超时时间可以通过参数 innodb_lock_wait_timeout 来设置（InnoDB 中默认值是 50s）。 缺点 等待时长过长，影响线上服务效率和用户体验 等待时长过短，误伤时间稍长的锁等待 主动死锁检测 检测发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。 设置 死锁检测通过将参数 innodb_deadlock_detect 设置为 on 开启。 缺点 死锁检测要耗费大量的 CPU 资源。 假设有 1000 个并发线程要同时更新同一行，每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，一次检测时间复杂度是 O(n)，那么整体死锁检测操作达到 100 万量级。 虽然最终检测的结果可能是没有死锁，但是期间要消耗大量的 CPU 资源。则现象为：现虽然 CPU 利用率很高，但是每秒却执行不了几个事务。 应对死锁检测缺点的策略 确保业务无死锁的情况下，临时关闭死锁检测 操作有风险，可能会出现大量超时，导致业务有损；而出现死锁，进行事务回滚，再通过业务重试恢复的过程是业务无损的。 控制并发度 并发度控制在合适的数量级时，死锁检测的成本很低。 客户端并发控制 客户端数量很多，即使每个客户端都控制在较少的并发线程，汇总到服务端的数量级也很大。因此，并发控制主要做在服务端。 服务端并发控制 对于相同行的更新操作进行排队。措施： 使用中间件（消息队列） 修改 MySQL 源码（由 server 层进入引擎之前排队）]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[systemctl 与 service]]></title>
    <url>%2F2019%2F11%2F21%2Fsystemctl%E4%B8%8Eservice%2F</url>
    <content type="text"><![CDATA[[Updated] 本文整理了 systemctl 与 service 的不同。 systemctl 与 serviceserviceservice 解析通过命令查看 service 命令的脚本内容： cat /usr/sbin/service # 通过 “whereis service” 定位脚本位置 可知，其大致功能是： 通过 service 命令，传入服务程序名和服务程序所支持的命令（最起码支持 start stop）； service 脚本在 /etc/init.d 下找到相应的程序脚本； 执行相应命令参数的程序脚本。 service 用法通过 man 函数查找 service 命令： man service 命令用法： service &lt; option &gt; | --status-all | [ service_name [ command | --full-restart ] ] 常用命令 含义 service &lt;服务名&gt; start 启动服务 service &lt;服务名&gt; stop 停止服务 service &lt;服务名&gt; restart 重启服务 systemctlsystemctl 解析CentOS 7 之后，CentOS 使用 systemd 服务管理系统启动和系统服务，具体通过 systemctl 命令来控制。 systemctl 用法用法： systemctl [OPTIONS...] COMMAND [NAME...] 常用命令 含义 systemctl start &lt;服务名&gt; 启动服务 systemctl stop &lt;服务名&gt; 停止服务 systemctl restart &lt;服务名&gt; 重启服务 systemctl status &lt;服务名&gt; 查看服务运行状态 systemctl reload &lt;服务名&gt; 重载配置文件]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>systemctl</tag>
        <tag>service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 博客从 github 迁至阿里云服务器]]></title>
    <url>%2F2019%2F11%2F20%2Fhexo-migration%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 hexo blog 从 github 迁移到阿里云服务器的过程。 起因国内访问 github 速度较慢且时有抽风，恰逢双十一大促购入阿里云 ESC，尝试迁移至云上。 以下是云服务器和客户端本机的配置过程。 云服务器配置过程云服务器基本信息 选择系统： CentOS 7 安全组配置： 阿里云默认不开放 HTTP（80）端口，故需要在阿里云服务器控制台手动添加安全组规则，授权 HTTP（80）端口的访问，否则将无法访问服务器的 web 应用。 git使用 git 配置云服务器代码仓库，创建 git 用户统一管理。 安装yum install git 配置 创建 git 用户 12345678# 1. 创建 git 用户，并设置密码sudo adduser gitsudo passwd git# 2. 为 git 用户设置权限chmod 740 /etc/sudoers # 先临时给 sudoers 文件添加写权限，修改完毕后及时收回vi /etc/sudoers # 添加 “git ALL=(ALL) ALL”chmod 400 /etc/sudoers 配置 ssh 123456789101112# 1. 生成 ssh 秘钥mkdir ~/.sshcd ~/.sshssh-keygen # .ssh 文件夹下会生成 id_rsa 和 id_rsa.pub 文件# 2. 设置密钥认证，使客户端可免密连接vi authorized_keys&lt;! 文件内输入本地客户端的 ssh 公钥，即本地客户端 ～/.ssh/id_rsa.pub 文件的内容&gt;# 3. 设置只有 git 用户对 .ssh 文件有读写权限chmod 600 ~/.ssh/authorized_keys chmod 700 ~/.ssh 创建 git 仓库 1234cd ~mkdir repo &amp; cd repo git init --bare hexoBlog.gitsudo chown -R git:git hexoBlog.git # 递归修改文件用户组 设置 hooks 1vi ~/repo/hexoBlog.git/hooks/post-receive 编辑内容为： 1git --work-tree=/home/www/blog --git-dir=/home/git/repo/hexoBlog.git checkout -f 保存退出后，给 post-receive 添加执行权限： 1sudo chmod u+x ~/repo/hexoBlog.git/hooks/post-receive 测试本地客户端通过 ssh 连接能够免密登录： ssh git@&lt;域名/服务器公网 ip 地址&gt; NginxNginx 安装 安装 epel-release，以自动配置 yum 的软件仓库 sudo yum install -y epel-release 安装 sudo yum install -y Nginx Nginx 测试 云服务器中，设置开机启动 Nginx： sudo systemctl enable nginx 使用以下命令启动 Nginx： systemctl nginx start 查看运行状态： sudo systemctl status nginx 可以看到进程处于活跃状态和其他相关信息。 在任意浏览器中，访问云服务器公网 ip，若出现 Nginx 欢迎界面，说明 Nginx 安装成功。若没有出现 Nginx 欢迎界面，首先应排查是否安全组未配置，其次观察是否为端口问题（占用或防火墙），再者可观察安装版本，换源/换版本重装。 Nginx 配置注意：因为 yum 源和安装方式不一， Nginx 版本或有不同，配置文件的位置可能会不一样，注意找到对应的文件进行修改。 查看 nginx.conf 下包含的配置文件路径 找到配置文件文件夹在 “conf.d”（即 include 的目录）。 在 “conf.d” 下创建新配置文件 “hexoBlog.conf”，添加以下内容 123456789server &#123; listen 80; # 监听端口 server_name &lt;域名/服务器公网 ip 地址&gt;; location / &#123; root /home/git/www/hexo; # hexo blog 根目录 index index.html; # 索引页，与 hexo 生成的静态索引页应同名 &#125;&#125; 重新加载服务，加载更新后的配置文件： systemctl nginx reload 此处使用 include 方式增加配置文件原因有二： 不破坏原有默认配置文件； 便于以后增删网站配置文件。 本机配置修改hexo修改 hexo 目录下的 _config.yml 文件中的 deploy 模块即可： 1234deploy: type: git repository: git@&lt;域名/服务器公网 ip 地址&gt;:/home/git/repo/hexoBlog.git # git 仓库地址 branch: master 测试发布文章 新建文章 hexo new &quot;test&quot; 生成静态文件 hexo clean # 清除缓存文件和已生成的静态文件 hexo g -d # 生成静态文件并部署，“hexo generate --deploy”的简写 任意客户端通过域名和 ip 进入可看见博客首页即成功。 Issue list 无法访问资源 403 Forbidden. 查看 /etc/Nginx/Nginx.conf 文件中的 “user”，是否为博客所在目录所属用户，修正即可。 若安装 Nginx 时使用 sudo，user 会默认为 root。 定位不到博客文件 查看 conf 文件的 root 目录是否设置正确； 查看 conf 文件的 index 索引页是否设置正确。 博客页面由 hexo 静态生成，对照博客目录下静态生成的页面，查看 index 是否设置错误。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>云服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel 快捷与函数]]></title>
    <url>%2F2019%2F09%2F21%2FExcel%E5%BF%AB%E6%8D%B7%E4%B8%8E%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 EXCEL 的高频快捷用法和常用函数 Excel 快捷与函数1 快捷1.1 常用快捷操作 快捷键 功能 Ctrl+方向键 跳转至选中方向边缘的数据 Ctrl+Shift+方向键 框选当前位置到选中方向边缘的所有数据 Ctrl+空格键 框选当前列 Shift+空格键 框选当前行 Ctrl+f 查找 Ctrl+h 替换 Ctrl+Alt+v 选择性粘贴 Ctrl+t 隔行上色 1.2 自动填充自动填充数据和函数： 选中单元格，瞄准右下角时鼠标变为十字准星，向某一方向框选区域进行自动填充 选中单元格，瞄准右下角时鼠标变为十字准星，双击自动填充至边缘 框选区域，输入填充内容，快捷键 Ctrl+Enter 自动填充 智能填充： 在第一个单元格手工输入目标值； 框选填充区域； 快捷键 Ctrl+E，自动识别填充规则并进行填充。 1.3 锁定：$锁定行或列，常配合自动填充使用。 2 函数2.1 假设函数2.1.2 IF根据条件进行真假判断，返回真假对应结果： IF(判断条件, 条件为“真”的返回值， 条件为“假”的返回值) 判断条件可以为： 单个条件 AND(条件 1， 条件 2，……) OR(条件 1， 条件 2，……) 示例 说明 =IF(AND(A1 &gt; 1, A2 &gt; 1), 1, 0) A1、A2 都大于 1 则返回 1，否则返回0 2.1.3 COUNT计算包含数字的单元格个数以及参数列表中数字的个数: COUNT(value1, [value2], …) 示例 说明 =COUNT(A1:A5) 返回单元格区域 A1 到 A5 中包含数字的单元格的个数 2.1.4 COUNTIFS统计满足所有条件的次数： COUNTIFS(criteria_range1, criteria1, [criteria_range2, criteria2],…) 示例 说明 =COUNTIFS(A2:A17, “=1”, B2:B17, “&gt;1/1/2010”, C2:C17, “&lt;” &amp; D2) 返回满足“A=1，B 日期晚于 2010-1-1，C &lt; 单元格 D2 数字”条件的单元格的个数 2.1.5 SUMIF对范围中符合指定条件的值求和： SUMIF(range, criteria, [sum_range]) 示例 说明 =SUMIF(A1:A5, “&gt;100”) 返回 A1:A5 区域大于 100 的数值之和 =SUMIF(A1:A5, “=” &amp; C1, B1:B5) 返回 A1:A5 中等于 C1 对应的 Bi 之和 2.1.6 SUMIFS计算满足多个条件的全部参数的总量： SUMIFS(sum_range, criteria_range1, criteria1, [criteria_range2, criteria2], …) 示例 说明 =SUMIFS(A1:B5, B1:B5, “=A*”, C1:C5, “B”) 返回 A1:B5 区域中满足条件“Bi 为 ‘Axx..’ 且 Ci 为 ‘B’” 的 Ai 之和 2.2 数值函数2.2.1 SUM对选中区域数值求和： SUM(number1, [number2], …) 示例 说明 =SUM(A1:B5, D1:F5) 返回 A1:B5 和 D1:F5 区域值之和 快捷键：Alt+= 2.2.2 SUMPRODUCT返回对应的区域或数组的乘积（默认乘法，还可通过公式指定加、减、除）之和（执行完所有操作，最终操作总为求和）： SUMPRODUCT(array1, [array2], [array3], …) 示例 说明 =SUMPRODUCT(A1:A5, B1:B5) 对 A1:A5, B1:B5 对应乘积求和（常见：加权平均） 2.2.3 ROUND（UP/DOWN）对数字按指定位数（向上/向下）四舍五入： ROUND(number, num_digits) num_digits 大于 0（零），则将数字四舍五入到指定的小数位数。 num_digits 等于 0，则将数字四舍五入到最接近的整数。 num_digits 小于 0，则将数字四舍五入到小数点左边的相应位数。 示例 说明 =ROUND(3.14, 1) 四舍五入到一个小数位，返回 3.1 =ROUND(13.14, -1) 四舍五入到小数点左侧一位，返回 10 2.2.4 SUBTOTAL返回列表或数据库中的分类汇总： SUBTOTAL(function_num, ref1, [ref2],…) 其中，function_num 与函数对应表如下： Function_num 对应函数 1 101 AVERAGE 2 102 COUNT 3 103 COUNTA 4 104 MAX 5 105 MIN 6 106 PRODUCT 7 107 STDEV 8 108 STDEVP 9 109 SUM 10 110 VAR 11 111 VARP 示例 说明 =SUBTOTAL(9/109, A1:A5) 对 A1:A5 执行 9/109 代表的函数（求和） 2.2.5 LARGE返回数据集中第 k 个最大值： LARGE(array, k) 示例 说明 =LARGE(A1:B5, 5) 返回 A1:B5 区域第 5 大的值 2.3 定位查找函数2.3.1 VLOOKUP按行查找项目： VLOOKUP （查阅值、查阅值所在的区域、区域中包含返回值的列号、匹配模式） 查阅值：要查找的值 查阅值所在的区域：包含查阅值。查阅值应该始终位于所在区域的第一列 区域中包含返回值的列号：以 A2:C5 区域为例，A 为第一列，B 为第二列进行计数，依此类推 匹配模式：近似匹配（TRUE）或完全匹配（FALSE） 示例 说明 =VLOOKUP(A1, B1:D5, 2, FALSE) 在 Bi 列中找到精确等于 A1 的行，返回对应的 Ci（以 B 为首列，C 为第 2 列） =IF(VLOOKUP(A1, B1:D5, 2, FALSE)=’C’, ‘找到’, ‘未找到’) 在 Bi 列中找到精确等于 A1 的行，取出对应的 Ci，若 Ci 为 ‘C’，返回’找到’, 否则返回’未找到’。 2.3.2 LOOKUP在单行区域或单列区域（称为“向量”）中查找值，然后返回第二个单行区域或单列区域中相同位置的值： LOOKUP(lookup_value, lookup_vector, [result_vector]) lookup_value：LOOKUP 在第一个向量中搜索的值，可以是数字、文本、逻辑值、名称或对值的引用 lookup_vector：只包含一行或一列的区域 注： lookup_vector 中的值必须按升序排列：…, -2, -1, 0, 1, 2, …, A-Z, FALSE, TRUE；否则，LOOKUP 可能无法返回正确的值。 文本不区分大小写。 如果 LOOKUP 函数找不到 lookup_value，则该函数会与 lookup_vector 中小于或等于 lookup_value 的最大值进行匹配。 如果 lookup_value 小于 lookup_vector 中的最小值，则 LOOKUP 会返回 #N/A 错误值 result_vector：只包含一行或一列的区域，与 lookup_vector 大小必须相同 示例 说明 =LOOKUP(C1, A1:A5, B1:B5) 返回 A1:A5 中等于 C1 的行对应的 Bi 2.3.3 MATCH在选中单元格中搜索特定的项，然后返回该项在此区域中的相对位置： MATCH(lookup_value, lookup_array, [match_type]) Match_type 说明 1 或缺省 查找小于或等于 lookup_value 的最大值。 lookup_array 参数中的值必须以升序排序 0 查找完全等于 lookup_value 的第一个值。 lookup_array 参数中的值可按任何顺序排列 -1 查找大于或等于 lookup_value 的最小值。 lookup_array 参数中的值必须按降序排列 示例 说明 =MATCH(“b”, {“a”,”b”,”c”}, 0) 返回 “b” 在 {“a”,”b”,”c”} 中的相对位置 2.3.4 INDEX返回表格或区域中的值或值的引用： INDEX(array, row_num, [column_num]) 数组只包含一行或一列：相应的 row_num 或 column_num 参数是可选的 数组具有多行和多列, 并且仅使用 row_num 或 column_num：返回数组中整个行或列的数组 同时使用 row_num 和 column_num 参数：返回 row_num 和 column_num 交叉处的单元格中的值。该值必须在数组中，否则将返回 #REF! 错误。 示例 说明 =INDEX(A1:B5, 2, 3) 返回 A1:B5 区域第 2 行第 3 列交叉处的值 2.4 格式函数2.4.1 IS检验指定值并根据结果返回 TRUE 或 FALSE： IS**(value) 函数 说明 ISBLANK 值为空白单元格 ISERR 值为任意错误值（除去 #N/A） ISERROR 值为任意错误值（#N/A、#VALUE!、#REF!、#DIV/0!、#NUM!、#NAME? 或 #NULL!） ISLOGICAL 值为逻辑值 ISNA 值为错误值 #N/A（值不存在） ISNONTEXT 值为不是文本的任意项。 （请注意，此函数在值为空单元格时返回 TRUE） ISNUMBER 值为数字 ISREF 值为引用 ISTEXT 值为文本 2.4.2 LEFT/RIGHT/MID文本字符串[从左起 | 从右起 | 字符串中的指定位置]第一个字符开始返回指定个数的字符： LEFT/RIGHT(text, [num_chars])MID(text, start_num, num_chars) 示例 说明 =LEFT/RIGHT(A1, 3) A1 左/右边第 1 个字符起，返回 3 个字符 =MID(A1, 3, 5) A1 第 3 个字符起，返回 5 个字符]]></content>
      <categories>
        <category>Excel</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL基础速览]]></title>
    <url>%2F2019%2F07%2F02%2Fsql%2F</url>
    <content type="text"><![CDATA[[Updated] 本文梳理了 SQL 相关基础知识（基于《SQL基础教程》） SQL 基础 SQL 概要标准 SQL国际标准化组织（ISO）为 SQL 制定的相应标准，适用于各种 RDBMS SQL 语句及其种类由关键字、表名、列名等组合而成一条 SQL 语句描述操作的内容 DDL（Data Definition Language，数据定义语言）用来创建或者删除存储数据用的数据库以及数据库中的表等对象。包含指令： CREATE： 创建数据库和表等对象 DROP： 删除数据库和表等对象 ALTER： 修改数据库和表等对象的结构 DML（Data Manipulation Language，数据操纵语言）用来查询或者变更表中的记录。包含指令： SELECT：查询表中的数据 INSERT：向表中插入新数据 UPDATE：更新表中的数据 DELETE：删除表中的数据 DCL（Data Control Language，数据控制语言）用来确认或者取消对数据库中的数据进行的变更。除此之外，还可以对 RDBMS 的用户是否有权限操作数据库中的对象（数据库表等）进行设定。包含指令： COMMIT： 确认对数据库中的数据进行的变更 ROLLBACK： 取消对数据库中的数据进行的变更 GRANT： 赋予用户操作权限 REVOKE： 取消用户的操作权限 SQL 的基本书写规则 以分号（；）结尾 SQL 不区分关键字大小写；表中数据区分大小写 SQL 一般书写原则： 关键字大写 表名首字母大写 其余（列名）小写 SQL 子句的顺序不能改变，也不能互相替换 表的创建数据库的创建（CREATE DATABASE语句）1CREATE DATABASE &lt;数据库名&gt;; 表的创建（CREATE TABLE语句）123456CREATE TABLE &lt;表名&gt;(&lt;列名1&gt; &lt;数据类型&gt; &lt;该列所需约束&gt;, &lt;列名2&gt; &lt;数据类型&gt; &lt;该列所需约束&gt;, …… &lt;列名n&gt; &lt;数据类型&gt; &lt;该列所需约束&gt;, &lt;该表约束1&gt;, &lt;该表约束2&gt;, ……); NOT NULL 约束只能以列为单位进行设置 列的数据类型必须指定 列的约束可以定义时设置，也可以语句末尾设置 命名规则 数据库/表/列名：半角英文字母、数字、下划线(_) 标准 SQL 中名称必须以半角英文字母开头 名称不能重复 数据类型的指定 INTEGER：整数 CHAR：定长字符串 括号中指定可存储字符串长度 超出部分无法输入到列中 不足部分由半角空格进行补足。如：char(10)存”123”，存储字符占10个字节。取数据的时候，char类型的要用trim()去掉多余的空格 比 VARCHAR 效率高，空间换时间 VARCHAR：可变长字符串 括号指定最大长度 不足部分不会使用半角空格进行补足，会自动裁剪。如：varchar(10)存”123”，存储字符占3个字节 比 CHAR 节省空间 DATE：日期 含年/月/日 Oracle 中 DATE 型还包含时分秒 约束的设置 键：指定特定数据时使用的列的组合 主键（primary key）：可以唯一确定一行数据的列（故无法重复） 表的删除和更新表的删除（DROP TABLE语句）1DROP TABLE &lt;表名&gt;; 删除表无法回复，只能重建 避免到需要恢复数据的场景 表定义的更新（ALTER TABLE语句）添加列 1ALTER TABLE &lt;表名&gt; ADD COLUMN &lt;列的定义&gt;; 删除列 1ALTER TABLE &lt;表名&gt; DROP COLUMN &lt;列名&gt;; 表定义变更（ALTER TABLE）之后无法恢复 不同数据库提供不同变更表名（RENAME）（非标准 SQL）指令。如：MySQL 中， 1RENAME TABLE &lt;原表名&gt; to &lt;新表名&gt;; 查询基础SELECT语句基础列的查询12SELECT &lt;列名&gt;，…… FROM &lt;表名&gt;； &lt; * &gt; 按表定义列的顺序列出所有列 不建议使用 &lt; * &gt;；建议列出所有列名，以提高 SQL 可读性 为列或表设定别名123SELECT 列名1 AS 别名1, 列名2 AS 别名2 FROM 表名 AS 表别名; 别名可使用中文，使用时将中文用双引号 &lt;” “&gt; 括起来 别名中空格建议用下划线取代。使用双引号可以设定包含空格的别名，但若忘记使用双引号可能导致错误 Oracle 的 FROM 子句中不能使用 AS（会发生错误），表别名直接跟在表名后即可（即不加 AS） 从结果中删除重复行使用 DISTINCT 实现删除由选择列出的列合成的数据中的重复行 12SELECT DISTINCT &lt;列名&gt;, …… FROM Product; 使用 DISTINCT 时， NULL 也被视为一类数据（显示为空白） DISTINCT 关键字只能用在第一个列名之前 通过 WHERE 子句来指定查询数据的条件123SELECT &lt;列名&gt;, …… FROM &lt;表名&gt; WHERE &lt;条件表达式&gt;; 书写顺序：WHERE 子句必须紧跟在 FROM 子句之后，否则会造成执行错误 执行顺序：首先通过 WHERE 子句查询出符合指定条件的记录，然后再选取出 SELECT 语句指定的列 注释的书写方法 英汉字皆可 单行注释：书写在 “—“ 之后 多行注释：书写在 “/“ 和 “/“ 之间 算术运算符和比较运算符算术运算符SELECT 子句中可以使用常数或者表达式：+ - * ÷ () 所有包含 NULL 的计算，结果肯定是 NULL（包括“NULL/0”的情况，不会报错） FROM 子句在 SELECT 语句中并不是必不可少的，只使用SELECT子句进行计算也是可以的，但使用场景很少。如： 1SELECT (1 + 2) * 3 AS calculation; 存在不允许省略 SELECT 语句中的 FROM 子句的 RDBMS。如：Oracle 比较运算符 =、&lt;、&gt;：等于、小于、大于 &lt;&gt;:不相等。“!=” 非标准 SQL，考虑可移植性和安全问题 , 不建议使用 &lt;=、&gt;=：不大于、不小于。必须不等号在左，等号在右 字符串类型的数据原则上按照字典顺序进行排序。该规则对定长字符串和可变长字符串都适用 SQL 用“IS NULL”和“IS NOT NULL”判断数据是否为NULL。因为SQL 不识别“= NULL”和“&lt;&gt; NULL”，所以不能对 NULL 使用比较运算符 逻辑运算符NOT运算符 NOT 不能单独使用，必须组合其他查询条件表“不是该条件” 为保持程序清晰可读，不滥用 NOT AND 运算符和 OR 运算符 AND 运算符在其两侧的查询条件都成立时整个查询条件才成立，其意思相当于“并且” OR 运算符在其两侧的查询条件有一个成立时整个查询条件都成立，其意思相当于“或者” AND 运算符的优先级高于 OR 运算符 建议使用括号强化优先级，使语句更清晰可读 真值 真值：值为真（TRUE） 或假（FALSE） 其中之一的值 AND 运算的结果与乘法运算（积）的结果一样，称逻辑积 OR 运算的结果与加法运算（和）的结果一样，称逻辑和 SQL 特有情况 — 三值逻辑：除真值外，还存在“不确定”（UNKNOWN）这样的值：P | Q |P AND Q|P OR Q|-|-|-|-真|不确定|不确定|真假|不确定|假|不确定不确定|真|不确定|真不确定|假|假|不确定不确定|不确定|不确定|不确定 建议尽量不使用 NULL，为列设置 NOT NULL 约束，以避免繁琐的条件判断 聚合与排序对表进行聚合查询聚合函数将多行输入汇总为一行输出 COUNT： 计算表中的记录数（行数） COUNT() 特性：不会排除 NULL。故 COUNT()会得到包含 NULL 的数据行数，而 COUNT(&lt;列名&gt;) 会得到 NULL 之外的数据行数 所有的聚合函数，如果以列名为参数，那么在计算之前会把 NULL 排除在外，与“等价为 0”并不相同 SUM： 计算表中 数值列 中数据的合计值 AVG： 计算表中 数值列 中数据的平均值 会事先删除 NULL 同时减少相应数据条数再计算。但也可以选择将 NULL 改变为 0 进行计算 MAX： 求出表中 任意列（如日期） 中数据的最大值 MIN： 求出表中 任意列（如日期）中 数据的最小值 使用聚合函数删除重复值（关键字DISTINCT）12SELECT COUNT(DISTINCT &lt;列名&gt;) FROM &lt;表名&gt;; DISTINCT 必须写在聚合函数参数（即括号）中，因为必须要在执行之前删除列中的重复数据 计算值的种类：在 COUNT() 的参数中使用 DISTINCT 对表进行分组GROUP BY1234SELECT &lt;列名1&gt;, &lt;列名2&gt;, &lt;列名3&gt;, …… FROM &lt;表名&gt; WHERE GROUP BY &lt;列名1&gt;, &lt;列名2&gt;, &lt;列名3&gt;, ……; 聚合键/分组列：GROUP BY 子句中指定的列 书写顺序：SELECT → FROM → WHERE → GROUP BY 执行顺序：FROM → WHERE → GROUP BY → SELECT 聚合键中包含 NULL 时，在结果中会以“不确定”行（空行）的形式表现出来 使用聚合函数和 GROUP BY 注意事项 使用聚合函数时， SELECT 子句中只能存在以下三种元素： 常数 聚合函数 GROUP BY 子句中指定的列名（也就是聚合键） 虽然 MySQL 支持使用GROUP BY子句时， SELECT 子句中出现聚合键之外的列名，但是 MySQL 以外的 DBMS 都不支持这样的语法，因此不建议使用 在 GROUP BY子 句中不能使用 SELECT 子句中定义的别名。根据执行顺序，执行 GROUP BY 子句时，DBMS 还不知道 SELECT 子句中定义的别名 GROUP BY子句结果的显示是无序的。可在 SELECT 语句中进行指定特定顺序 只有 SELECT 子句和 HAVING 子句（以及 ORDER BY 子句）中能够使用聚合函数 DISTINCT 和 GROUP BY：都是通过数据的内部排序处理实现的（执行速度相近），可删除重复数据。根据使用场景，选择能清晰表明语义的写法 为聚合结果指定条件HAVING 子句WHERE 子句只能指定记录（行）的条件，而不能用来指定组的条件。对集合指定条件使用 HAVING 子句： 1234SELECT &lt;列名1&gt;, &lt;列名2&gt;, &lt;列名3&gt;, …… FROM &lt;表名&gt;GROUP BY &lt;列名1&gt;, &lt;列名2&gt;, &lt;列名3&gt;, ……HAVING &lt;分组结果对应的条件&gt; 书写顺序：SELECT → FROM → WHERE → GROUP BY → HAVING 执行顺序：FROM → WHERE → GROUP BY → SELECT HAVING 注意事项 HAVING 子句中只能存在以下三种元素： 常数 聚合函数 GROUP BY 子句中指定的列名（即聚合键） 聚合键所对应的条件应该书写在 WHERE 子句当中，而不应该书写在 HAVING 子句当中。理由如下： HAVING 子句是用来指定“组”的条件的。因此，“行”所对应的条件还是应该写在 WHERE 子句当中，便于理解区分功能 通常情况下，为了得到相同的结果，将条件写在 WHERE 子句中要比写在 HAVING 子句中的处理速度更快，返回结果所需的时间更短 为了理解其中原因，就要从 DBMS 的内部运行机制来考虑。使用 COUNT 函数等对表中的数据进行聚合操作时，DBMS 内部就会进行排序处理。排序处理是会大大增加机器负担的高负荷的处理 A。因此，只有尽可能减少排序的行数，才能提高处理速度。通过 WHERE 子句指定条件时，由于排序之前就对数据进行了过滤，因此能够减少排序的数据量。但 HAVING 子句是在排序之后才对数据进行分的，因此与在 WHERE 子句中指定条件比起来，需要排序的数据量就会多得多。虽然 DBMS 的内部处理不尽相同，但是对于排序处理来说，基本上都是一样的。此外， WHERE 子句更具速度优势的另一个理由是，可以对 WHERE 子句指定条件所对应的列创建索引，这样也可以大幅提高处理速度。创建索引是一种非常普遍的提高 DBMS 性能的方法，效果也十分明显，这对 WHERE 子句来说也十分有利。 对查询结果进行排序ORDER BY子句123SELECT &lt;列名1&gt;, &lt;列名2&gt;, &lt;列名3&gt;, …… FROM &lt;表名&gt; ORDER BY &lt;排序基准列1&gt;, &lt;排序基准列2&gt;, ……; 书写顺序：SELECT → FROM → WHERE → GROUP BY → HAVING → ORDER BY ORDER BY 子句通常写在 SELECT 语句的末尾 未指定 ORDER BY子句中排列顺序时会默认使用升序进行排列；使用 DESC 关键字降序排列 ORDER BY 注意事项 多键排序规则：优先使用左侧的键，如果该列存在相同值的话，再接着参考右侧的键 排序键中包含 NULL 时，会在开头或末尾进行汇总（因为不能对 NULL 使用比较运算符） 在 ORDER BY 子句中可以使用 SELECT 子句中定义的别名。因为 SELECT 子句的执行顺序在 GROUP BY 子句之后， ORDER BY 子句之前 在 ORDER BY 子句中可以使用 SELECT 子句中未使用的列和聚合函数 不要使用列编号指定排序键： 可读性差 SQL-92A 中明确该功能将来会被删除 列编号 — SELECT 子句中的列按照从左到右的顺序进行排列时所对应的编号（1, 2, 3, …） 数据更新数据的插入（INSERT语句的使用方法）INSERT 语句1INSERT INTO &lt;表名&gt; (列1, 列2, 列3, ……) VALUES (值1, 值2, 值3, ……); 原则上，执行一次INSERT语句会插入一行数据,表名后面的列清单和 VALUES 子句中的值清单的列数必须保持一致 很多 RDBMS 都支持多行 INSERT： 1INSERT INTO &lt;表名&gt; (列1, 列2, 列3, ……) VALUES (值1, 值2, 值3, ……), (值1, 值2, 值3, ……) …… ; 列清单的省略 1INSERT INTO &lt;表名&gt; VALUES (值1, 值2, 值3, ……), (值1, 值2, 值3, ……) …… ; 插入NULL：插入 NOT NULL 约束的列会报错 INSERT， DELETE 和 UPDATE 等更新语句也一样，SQL 语句执行失败时都不会对表中数据造成影响 默认插入值在创建表的 CREATE TABLE 语句中设置 DEFAULT 约束来设定默认值： 1234CREATE TABLE ProductIns(&lt;列名&gt; CHAR(4) NOT NULL, &lt;列名&gt; INTEGER DEFAULT 0, -- 销售单价的默认值设定为0;……); 显式方法插入默认值：在 VALUES 中指定 DEFAULT 关键字 1INSERT INTO &lt;表名&gt; (&lt;列名1&gt;, &lt;列名2&gt;, ……) VALUES (DEFAULT, DEFAULT, ……); 隐式方法插入默认值：在列清单和 VALUES 中省略要设定默认值的列 1INSERT INTO &lt;表名&gt; (&lt;列名1&gt;, &lt;列名3&gt;, ……) VALUES (DEFAULT, DEFAULT, ……); -- 列2设定默认值 从其他表中复制数据 创建一张结构一样的表 旧表数据插入新表： 123INSERT INTO 旧表 (&lt;列名1&gt;, &lt;列名2&gt;, ……)SELECT &lt;列名1&gt;, &lt;列名2&gt;, ……FROM 新表; INSERT 语句的 SELECT 语句中，可以使用 WHERE 子句或者 GROUP BY 子句等何 SQL 语法（除 ORDER BY） 指定 ORDER BY 子句也没有任何意义，因为无法保证表内部记录的排列顺序 数据的删除（DELETE语句的使用方法）DELETE语句1234DELETE FROM &lt;表名&gt;; -- 保留数据表，清空表全部数据DELETE FROM &lt;表名&gt; -- 删除表中指定条件数据 WHERE &lt;条件&gt;; 注意事项 DELETE 语句中只能使用 WHERE，而不能使用 GROUP BY、HAVING 和 ORDER BY。因为： GROUP BY 和 HAVING 是从表中选取数据时用来改变抽取数据形式的 ORDER BY 是用来指定取得结果显示顺序的 TRUNCATE：删除表中全部数据1TRUNCATE &lt;表名&gt;; 非标准SQL；Oracle、SQL Server、PostgreSQL、MySQL 和 DB2 不能通过 WHERE 子句指定条件来删除部分数据 数据的更新（UPDATE语句的使用方法）UPDATE 语句123UPDATE &lt;表名&gt; SET &lt;列名&gt; = &lt;表达式/NULL&gt; -- NULL 只限于未设置 NOT NULL 约束的列 WHERE &lt;条件&gt;; 多列更新 法一：所有 DBMS 通用 12345-- 使用逗号对列进行分隔排列UPDATE &lt;表名&gt; SET &lt;列名1&gt; = &lt;表达式1&gt;, &lt;列名2&gt; = &lt;表达式2&gt; WHERE &lt;条件&gt;; 法二：非通用 1234-- 将列用()括起来的清单形式UPDATE Product SET (列名1, 列名2) = (表达式1, 表达式2) WHERE &lt;条件&gt;; 事务什么是事务需要在同一个处理单元中执行的一系列更新处理的集合。例如： 现要求完成往表1插入新数据并更新一些旧数据的任务。要完成该任务，插入和更新两种操作都要完成，则一定要使用事务进行处理（将一起要完成的操作打包进一个事务中进行处理） 创建事务12345事务开始语句;DML语句①;DML语句②;……事务结束语句（ COMMIT或者ROLLBACK） ; 在标准 SQL 中并没有定义事务的开始语句，而是由各个 DBMS 自己来定义的 SQL Server、PostgreSQL： BEGIN TRANSACTIONBEGIN TRANSACTION MySQL： START TRANSACTION Oracle、DB2：无 实际上，几乎所有的数据库产品的事务都无需开始指令。因为大部分情况下，事务在数据库连接建立时就已经开始，并不需要用户再明确发出开始指令 事务结束语句在所有的 RDBMS 中都是通用，只有 COMMIT 和 ROLLBACK 两种： COMMIT — 是提交事务包含的全部更新处理的结束指令，相当于文件处理中的覆盖保存。一旦提交，就无法恢复到事务开始前的状态了 ROLLBACK — 是取消事务包含的全部更新处理的结束指令，相当于文件处理中的放弃保存。一旦回滚，数据库就会恢复到事务开始之前的状态 在不使用指令而悄悄开始事务的情况下，区分各个事务有以下两种模式（通常 DBMS 都可以设置任选其一）： 自动提交模式 — 每条SQL语句就是一个事务（MySQL、SQL Server 和 PostgreSQL 默认使用） 直到用户执行 COMMIT 或者 ROLLBACK 为止算作一个事务（Oracle 默认使用） 若使用 DELETE 语句删除了数据表： 自动提交模式下，无法回滚恢复 非自动提交模式下，可以通过 ROLLBACK 命令取消该事务的处理，恢复表中的数据。但这仅限于明示开始事务，或者关闭自动提交的情况 ACID特性 原子性（Atomicity） 原子性是指在事务结束时，其中所包含的更新处理要么都执行，要么都不执行 一致性（Consistency）/完整性 一致性指的是事务中包含的处理要满足数据库提前设置的约束 隔离性（Isolation） 隔离性指的是保证不同事务之间互不干扰的特性。该特性保证了事务之间不会互相嵌套。此外，在某个事务中进行的更改，在该事务结束之前，对其他事务而言是不可见的 持久性（Durability） 持久性指的是在事务（不论是提交还是回滚）结束后， DBMS 能够保证该时间点的数据状态会被保存的特性。即使由于系统故障导致数据丢失，数据库也一定能通过某种手段进行恢复，如日志系统 复杂查询视图视图：保存好的 SELECT 语句 视图和表区别：表中存储的是实际数据，而视图中保存的是从表中取出数据所使用的SELECT语句 视图的优点： 无需保存数据，节省存储设备的容量 将频繁使用的 SELECT 语句保存成视图，不用重写重新执行，以提高效率 创建视图123CREATE VIEW 视图名称(&lt;视图列名1&gt;, &lt;视图列名2&gt;, ……)AS&lt;SELECT语句&gt; SELECT 语句中列的排列顺序和视图中列的排列顺序相同 多重视图：以视图为基础创建视图。多重视图会降低 SQL 的性能应尽量避免 使用视图查询 首先执行定义视图的 SELECT 语句 根据得到的结果，再执行在 FROM 子句中使用视图的 SELECT 语句 视图的限制 定义视图时不能使用 ORDER BY 子句 视图和表一样， 数据行都是没有顺序的 对视图进行更新 标准 SQL 中规定，想要视图可以被更新，定义视图的 SELECT 语句需要满足某些条件（非通过汇总）： SELECT 子句中未使用 DISTINCT FROM 子句中只有一张表 未使用 GROUP BY 子句 未使用 HAVING 子句 原因：视图和表需要同时进行更新，以保持数据一致性，因此通过汇总得到的视图无法进行更新 删除视图1DROP VIEW 视图名称(&lt;视图列名1&gt;, &lt;视图列名2&gt;, ……)； 子查询子查询和视图子查询就是将用来定义视图的 SELECT 语句直接用于 FROM 子句当中（为查询结果命别名） 子查询作为内层查询会首先执行 原则上子查询必须设定名称（使用 AS 关键字） 标量子查询标量子查询就是返回单一值的子查询 优点：返回的是单一值，可以用在 = 或者 &lt;&gt; 等比较运算符之中 书写位置：能够使用常数或者列名的地方 注意事项：子查询中只能返回单一值 关联子查询（建议刷题理解）与普通的子查询的区别在子查询中添加的 WHERE 子句的条件 为区别表对应不同的场景，在表所对应的列名之前加上表的别名，形式为“&lt;表名&gt;.&lt;列名&gt;” 适合在细分的组内进行比较时使用 结合条件一定要写在子查询中 函数、谓词、CASE表达式函数函数的种类 算术函数：数值计算 字符串函数：字符串操作 日期函数：日期操作 转换函数：转换数据类型和值 聚合函数：数据聚合 算术函数 绝对值函数 1ABS(数值) 求余 1MOD(被除数，除数) [注] SQL Server 不支持 MOD()，而使用 “%” 求余 四舍五入 1ROUND(对象数值，保留小数的位数) 字符串函数 拼接 1字符串1 || 字符串2 || 字符串3 || …… [注] SQL Server 不支持 ||，而使用 “+” 拼接字符串 MySQL 不支持 ||，而使用 CONCAT() 拼接字符串 字符串长度 1LENGTH(字符串) [注] SQL Server 不支持 LENGTH()，而使用 LEN() 拼接字符串 同样是 LENGTH 函数，不同 DBMS 的执行结果也不尽相同。MySQL 中的 LENGTH() 以字节为单位的函数进行计算,此外还存在计算字符串长度的自有函数 CHAR_LENGTH() 大小写转换 1UPPER/LOWER(字符串) [注] UPPER/LOWER 函数只能针对英文字母使用，将参数中的字符串全都转换为大/小写 字符串替换 1REPLACE(对象字符串，替换前的字符串，替换后的字符串) 字符串截取 1SUBSTRING（对象字符串 FROM 截取的起始位置 FOR 截取的字符数） [注] 标准 SQL，但只有 PostgreSQL 和 MySQL 支持该语法 SQL Server 版本： SUBSTRING(对象字符串，截取的起始位置，截取的字符数) Oracle 版本： SUBSTR(对象字符串，截取的起始位置，截取的字符数) 日期函数 当前日期 1CURRENT_DATE [注] SQL Server 不支持 CURRENT_DATE，而使用 CAST(CURRENT_TIMESTAMP AS DATE) 获取当前日期 当前时间 1CURRENT_TIME [注] SQL Server 不支持 CURRENT_TIME，而使用 CAST(CURRENT_TIMESTAMP AS TIME) 获取当前时间 当前日期和时间 1CURRENT_TIMESTAMP 截取日期元素 1EXTRACT(日期元素 FROM 日期) [注] 日期元素： YEAR MONTH DAY HOUR MINUTE SECOND SQL Server 不支持 EXTRACT 函数，而使用 DATEPART 函数 获取当前时间 DATEPART(日期元素 , CURRENT_TIMESTAMP) 转换函数 CAST —— 数据类型转换 1CAST（转换前的值 AS 想要转换的数据类型） [注] 使用场景： 插入与表中数据类型不匹配的数据 在进行运算时由于数据类型不一致发生了错误 进行自动类型转换会造成处理速度低下 COALESCE —— 将NULL转换为其他值 返回可变参数 A 中左侧开始第 1个不是 NULL 的值 1COALESCE(数据1，数据2，数据3……) 谓词 — 返回值是真值的函数LIKE — 模糊查询123SELECT * FROM &lt;表名&gt; WHERE &lt;列名&gt; LIKE '模式'; % 代表“0 个字符以上的任意字符串” _（下划线）代表了“任意 1 个字符” 模式 匹配 abc% abcqwe %abc% qweabcqwe %abc qweabc abc__ abcqw abc eabcq _abc qabc BETWEENT —— 范围查询123SELECT * FROM &lt;表名&gt; WHERE &lt;列名&gt; BETWEEN &lt;上限&gt; AND &lt;下限&gt;; 数据可以是数值、文本或者日期 BETWEEN 的结果包含 &lt;上限&gt; 和 &lt;下限&gt;。不想让结果包含临界值则使用 &lt; 和 &gt; IS (NOT) NULL —— 判断是否为NULL选取出某些值为（不为） NULL 的列的数据只能使用特定的谓词 IS (NOT) NULL 123SELECT * FROM &lt;表名&gt; WHERE &lt;列名&gt; IS (NOT) NULL; IN 谓词 —— OR 的简便用法123SELECT * FROM &lt;表名&gt; WHERE &lt;列名&gt; (NOT) IN (集合元素……/子查询); 使用 IN 和 NOT IN 无法选取出 NULL 数据，因为 NULL 只能使用 IS (NOT) NULL 选取 EXIST 谓词判断是否存在满足某条件的记录 123SELECT * FROM &lt;表名&gt; WHERE &lt;列名&gt; (NOT) EXIST (集合元素……/子查询); 通常指定关联子查询作为 EXIST 的参数 由于 EXIST 只关心记录是否存在，因此子查询中返回哪些列都没有关系（建议统一在 EXIST 的子查询中书写 SELECT *） 与 in 执行时的区别：in 先执行子查询中的查询，再执行主查询；exists 先执行主查询，即外层表的查询，再执行子查询。效率视情况而定 CASE 表达式 — 区分情况执行CASE表达式 搜索 CASE 表达式语法 123456CASE WHEN &lt;求值表达式&gt; THEN &lt;表达式&gt; WHEN &lt;求值表达式&gt; THEN &lt;表达式&gt; WHEN &lt;求值表达式&gt; THEN &lt;表达式&gt; …… ELSE &lt;表达式&gt;END 执行过程： 第一条 WHEN 子句中的“&lt; 求值表达式 &gt;”求值 结果为真（TRUE），返回 THEN 子句中的表达式， 执行完毕；结果不为真，顺次转到下一条 WHEN 子句进行求值…… 若到最后一条 WHEN 子句为止返回结果都不为真，则返回 ELSE 中的表达式 执行完毕 ELSE 子句也可以省略不写，这时会被默认为 ELSE NULL，但不建议省略 END 不能省略 集合运算表的集合运算（以行方向为单位进行操作）Def进行这些集合运算时，会导致记录行数的增减，但不会导致列数的改变 UNION（并集）表 a 和 表 b 的并集 12345SELECT &lt;列名&gt;, …… FROM &lt;a 表名&gt; UNION （ALL）SELECT &lt;列名&gt;, …… FROM &lt;b 表名&gt;; INTERSECT（交集）表 a 和 表 b 的交集 12345SELECT &lt;列名&gt;, …… FROM &lt;a 表名&gt;INTERSECT （ALL）SELECT &lt;列名&gt;, …… FROM &lt;b 表名&gt;; EXCEPT（差集）表 a 和 表 b 的差集，即表 a 除去与表 b 交集部分所剩余的部分 12345SELECT &lt;列名&gt;, …… FROM &lt;a 表名&gt;EXCEPTSELECT &lt;列名&gt;, …… FROM &lt;b 表名&gt;; MySQL 不支持 EXCEPT Oracle 中求差集将 “EXCEPT” 改为 “MINUS” 集合运算注意事项 集合运算符会除去重复的记录。使用 ALL 选项，可以保留重复行 作为运算对象的记录的列数必须相同 作为运算对象的记录中列的类型必须一致 可以使用任何 SELECT 语句，但 ORDER BY 子句只能在最后使用一次 联结（以列为单位对表进行联结）Def将其他表中的列添加过来，进行“添加列”的运算 内联结——INNER JOIN以两张表中都包含的列（联结键）作为桥梁，将只存在于一张表内的列汇集到同一结果之中 12345SELECT &lt;表1别名&gt;.&lt;列名&gt;, …， &lt;表2别名&gt;.&lt;列名&gt;, … FROM &lt;表1&gt; AS &lt;表1别名&gt; INNER JOIN &lt;表2&gt; AS &lt;表2别名&gt; ON 表1.&lt;共同列&gt; = 表2.&lt;共同列&gt; WHERE …… 外联结——OUTER JOIN通过 ON 子句的联结键将两张表进行联结，并从两张表中同时选取相应的列 12345SELECT &lt;表1别名&gt;.&lt;列名&gt;, …， &lt;表2别名&gt;.&lt;列名&gt;, … FROM &lt;表1&gt; AS &lt;表1别名&gt; LEFT/RIGHT JOIN &lt;表2&gt; AS &lt;表2别名&gt; ON 表1.&lt;共同列&gt; = 表2.&lt;共同列&gt; WHERE …… 与内联结区别 内联结只能选取出同时存在于两张表中的数据 对于外联结，只要数据存在于某一张表当中，就能够读取出来 外联结中使用LEFT、RIGHT来指定主表，最终的结果中会包含主表内所有的数据 多表联结通过 ON 子句的联结键将多张表进行联结。 1234567SELECT &lt;表1别名&gt;.&lt;列名&gt;, …， &lt;表2别名&gt;.&lt;列名&gt;, … FROM &lt;表1&gt; AS &lt;表1别名&gt; INNER JOIN &lt;表2&gt; AS &lt;表2别名&gt; ON 表1.&lt;共同列&gt; = 表2.&lt;共同列&gt; INNER JOIN &lt;表3&gt; AS &lt;表3别名&gt; ON 表1.&lt;共同列&gt; = 表3.&lt;共同列&gt; WHERE …… 如下例子中表 1 与表 2 联结，表 1 与表 3 联结，则表 2 与表 3 无需再联结 “表 1 与表 3 联结”改为“表 2 与表 3 联结”，效果一样 SQL 高级处理窗口函数窗口函数的语法12&lt;窗口函数&gt; OVER (PARTITION BY &lt;列清单&gt;) ORDER BY &lt;排序用列清单&gt;) 通过 PARTITION BY 分组后的记录集合称为“窗口”（意指“范围”）。PARTITION BY 并非必需，不指定 PARTITION BY 时，将整个表作为一个大的窗口来使用 OVER 子句中的 ORDER BY 只是用来决定窗口函数按照什么样的顺序进行计算的，对结果的排列顺序并没有影响。对结果排序需要在 SELECT 语句的最后使用 ORDER BY 子句，此时两个 ORDER BY 功能完全不同 窗口函数兼具分组和排序两种功能 能够作为窗口函数使用的函数 能够作为窗口函数的聚合函数（SUM、AVG、COUNT、MAX、MIN） RANK、DENSE_RANK、ROW_NUMBER 等专用窗口函数 以专用窗口函数 RANK 为例 1234SELECT &lt;列1&gt;, &lt;列2&gt;, &lt;列3&gt;， … RANK () OVER (PARTITION BY &lt;列2&gt; ORDER BY &lt;列3&gt;) AS ranking FROM &lt;表名&gt;; PARTITION BY 能够设定排序的对象范围 ORDER BY 能够指定按照哪一列、何种顺序进行排序 专用窗口函数 RANK函数 计算排序时，如果存在相同位次的记录，则会跳过之后的位次。 例：有 3 条记录排在第 1 位时：1 位、1 位、1 位、4 位…… DENSE_RANK函数 同样是计算排序，即使存在相同位次的记录，也不会跳过之后的位次。 例：有 3 条记录排在第 1 位时：1 位、1 位、1 位、2 位…… ROW_NUMBER函数 赋予唯一的连续位次。 例：有 3 条记录排在第 1 位时：1 位、2 位、3 位、4 位…… 专用窗口函数无需参数，因此通常参数括号中都是空的 作为窗口函数使用的聚合函数以专用 AVG() 为例 123SELECT &lt;列1&gt;, &lt;列2&gt;, &lt;列3&gt;， …， AVG (指定列) OVER (ORDER BY &lt;指定列&gt;) AS ranking FROM &lt;表名&gt;; 得到的结果按照 ORDER BY 子句指定列的升序排列，一行一行逐渐添加计算对象，累计进行聚合函数运算 聚合函数作为窗口函数时的最大特征：以当前记录作为基准进行统计 窗口函数的适用范围原则上，窗口函数只能书写在 SELECT 子句中。 在 DBMS 内部，窗口函数是对 WHERE 子句或者 GROUP BY 子句处理后的“结果”进行的操作。大家仔细想一想就会明白，在得到用户想要的结果之前，即使进行了排序处理，结果也是错误的。在得到排序结果之后，如果通过 WHERE 子句中的条件除去了某些记录，或者使用 GROUP BY 子句进行了汇总处理，那好不容易得到的排序结果也无法使用了。反之，之所以在 ORDER BY 子句中能够使用窗口函数，是因为 ORDER BY 子句会在 SELECT 子句之后执行，并且记录保证不会减少。 因此，在 SELECT 子句之外“使用窗口函数是没有意义的。 计算移动平均移动平均（moving average）常用于希望实时把握“最近状态”的场景，如数据的实时跟踪 123SELECT &lt;列1&gt;, &lt;列2&gt;, &lt;列3&gt;， …， AVG &lt;指定列&gt; OVER (ORDER BY &lt;指定列&gt; ROWS n PRECEDING)FROM &lt;表名&gt;; 使用了 ROWS（“行”）和 PRECEDING（“之前”）两个关键字，将框架指定为“截止到之前 n 行”，即：自身（当前记录）、之前第 1 行、 ……、 之前第 ~ 行，共 n+1 行 框架:在窗口中指定更加详细的汇总范围 FOLLOWING（“之后”）可替换 PRECEDING，指定“截止到之后 ~ 行”作为框架 GROUPING 运算符ROLLUP123SELECT GROUPING(聚合键)， SUM(求和列) AS sum_指定列 FROM &lt;表名&gt; GROUP BY ROLLUP(聚合键); 可一次计算出按不同聚合键组合的求和结果 此处聚合键和 GROUP BY 子句使用一样，可以为 NULL，可以指定多列 GROUP BY 不指定聚合键时会默认使用 NULL 作为聚合键（相当于没有使用 GROUP BY），此时会得到全部数据的合计行的记录，称超级分组 GROUPING 函数在其参数列的值为超级分组记录所产生的 NULL 时返回 1，其他情况返回 0，以分辨出原始数据中的 NULL 和超级分组记录中的 NULL 在 MySQL 中 GROUP BY 子句应改写为“GROUP BY &lt;指定列&gt; WITH ROLLUP;” CUBECUBE 将 GROUP BY 子句中聚合键的“所有可能的组合”的汇总结果集中到一个结果中。因此，组合的个数就是 $2^n$（n 是聚合键的个数） 123SELECT GROUPING(聚合键)， SUM(求和列) AS sum_指定列 FROM &lt;表名&gt; GROUP BY CUBE(聚合键); 可以把 CUBE 理解为将使用聚合键进行切割的模块堆积成一个立方体 GROUPING SETSCUBE 的结果就是根据聚合键的所有可能的组合计算而来的，使用 GROUPING SETS 可以取得部分组合的结果 123SELECT GROUPING(聚合键)， SUM(求和列) AS sum_指定列 FROM &lt;表名&gt; GROUP BY GROUPING SETS(&lt;聚合键组合1&gt;, &lt;聚合键组合2&gt;, …);]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + NexT 的问题汇总]]></title>
    <url>%2F2019%2F06%2F30%2Fhexo_problem_list%2F</url>
    <content type="text"><![CDATA[[Updating] 本文记录了 Hexo + NexT 使用过程中的一些问题 版本声明 hexo: 3.9.0 next: 7.0.1 local_search 不能用打开首页（本地or线上），打开浏览器开发工具 Network 选项卡，点击首页“搜索”按钮，观察 search.xml 状态： 200 问题：Algolia 问题 解决：主题配置文件中关闭 Algolia 404 其他 问题：存在非法字符 解决：sublime 排查 .md 文件中所存在非法字符 post_meta 不显示更新时间修改主题配置文件，post_meta 模块中设置：123updated: enable: true anotherday: false 数学公式不显示 更换渲染引擎为 hexo-renderer-marked 12npm uninstall hexo-renderer-marked --save # 卸载 hexo 默认引擎 hexo-renderer-markednpm install hexo-renderer-kramed --save Next 配置文件开启 MathJax 找到 math 模块，开启： 1234math： enable： true # 此处设为 true …… 在需要渲染公式的文章开头的 Front-matter 设置 MathJax 123456---title: ……date: ……tags:mathjax: true # 添加此项--- 图片不显示传统的 Makrdown 插入图片的语法无法显示图片。 查阅官方文档，发现可用资源文件来解决。 修改 hexo 的配置文件 _config.yml，将 post_asset_folder 选项设置为 true； 插入图片的语法： 1&#123;% asset_img example.jpg This is an example image %&#125; 在资源文件夹中放入图片 example.jpg 即可。 注：安装 hexo-asset-image 插件，每次新建文章后会自动在 source/_post 文件夹下创建文章同名的资源文件夹：1npm install hexo-asset-image --save]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github + Hexo 搭建博客]]></title>
    <url>%2F2019%2F05%2F30%2Fhexo%2F</url>
    <content type="text"><![CDATA[[Updating] 本文记录了使用Github和Hexo搭建该博客的过程 环境 系统：ubuntu 18.04 nodejs:： Hexo 的安装Node.js 的安装Hexo 的安装Git 的安装初步优化]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从 pip 到 Pipfile]]></title>
    <url>%2F2018%2F03%2F21%2Fpip%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 Python 软件包管理相关的 pip、requirements.txt、Pipfile，以及 Python 虚拟环境的内容。 从 pip 到 Pipfilepippip 是一个通用的 Python 软件包管理工具，可以从 Python 官方的第三方库仓库 PyPI （默认）或其他索引安装软件包。 pip 安装注：Python 2.7.9 + 或 Python 3.4+ 内置 pip。 在终端使用以下命令在当前全局 Python 解释器下安装 pip： sudo python get-pip.py 查看安装版本： pip --version 升级 pip 到最新版本： pip install -U pip pip 使用查看 pip 用法： pip --help pip 常用命令： 查看已安装包列表 pip list 查找/安装/卸载包 pip search/install/uninstall &lt;Package&gt; # 安装时使用 “&lt;Package&gt;=版本号” 可指定下载版本 升级包 pip install --upgrade &lt;Package&gt; 查看包的详细信息 pip show -f &lt;Package&gt; 虚拟环境虚拟环境作用Python 虚拟环境允许将 Python 包安装在特定应用程序的隔离位置，而不是全局安装，避免了不同程序因 Python 包的依赖版本不同而导致彼此的运行受到影响。 因为不同的项目通常会依赖不同版本的库或 Python 版本，所以建议使用虚拟环境为每一个项目创建独立的 Python 环境。 虚拟环境的使用 安装虚拟环境： pip install virtualenv 项目根目录下创建虚拟环境： virtualenv --no-site-packages &lt;虚拟环境名&gt; # --no-site-packages 参数：不复制全局 python 解释器下的任何第三方库 此时，项目根目录下会创建 venv 目录，用以存放虚拟环境。 激活虚拟环境： source venv/bin/activate 此时，终端前缀会出现字样“(&lt;虚拟环境名&gt;)”，表示已进入虚拟环境。 关闭当前虚拟环境： deactivate 此时，终端前缀消失，回到全局环境。 requirements.txt为了解决不同环境下构建项目时可能产生的依赖问题，Python 项目中一般会包含 requirements.txt 文件，文件内记录了该项目所使用到全部依赖包及其精确版本号，以便在新环境中安装该项目所需要的运行环境依赖。 生成 requirements.txt在 Python 项目的根目录终端执行以下命令： pip freeze &gt; requirements.txt 使用 requirements.txt使用 requirements.txt 安装依赖 pip install -r requirements.txt # -r 可以防止安装过程中的某个错误而提前终止安装 缺点requirements.txt 是一个简单的纯文本文件，需要手动维护，灵活性低。 Pipfile 和 Pipfile.lockPipfilePipfile 与 Pipfile.lock 是社区拟定的依赖管理文件，用于替代 requirements.txt。一个项目对应一个 Pipfile，支持开发环境与正式环境区分。默认提供 default 和 development 区分。 与 requirements.txt 区别： Pipfile 文件是 TOML 格式； requirements.txt 是纯文本。 Pipfile.lock根据 Pipfile 和当前环境自动生成的 JSON 格式的依赖文件。 注： 不可手动修改！ pipenvpipenv 结合了 Pipfile 、pip 和 virtualenv。其作用主要有如下几点： 自动在项目目录的 .venv 目录创建虚拟环境。 自动生成 Pipfile 和 Pipfile.lock。 自动管理 Pipfile 新安装和删除的包。 使用 pipenv 创建虚拟环境并安装 Pipfile 中所列的所有包： 在 Python 项目的根目录终端执行以下命令： pipenv install 此时，当前项目根目录下会创建 .venv 文件夹，并安装 Pipfile 中的所有第三方软件包。 确认 Pipfile 中所有包已安装： pipenv lock 确认并根据安装版本生成 Pipfile.lock。 激活虚拟环境： 在 Python 项目的根目录终端执行以下命令： pipenv shell 运行程序： pipenv run python **.py # 运行 python 文件 pipenv run flask run # 启动 flask 程序 注： 这里会调用虚拟环境中的 python 解释器，而不是全局的 python 解释器。 在激活虚拟环境后，pipenv run 可以省略，但养成使用 pipenv run 的习惯可以避免运行时忘记启用虚拟环境。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pip</tag>
        <tag>虚拟环境</tag>
        <tag>pipfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式语法速览]]></title>
    <url>%2F2018%2F03%2F21%2Fregex%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了正则表达式的常用语法。 正则表达式Def描述复杂文本规则的代码，用以匹配符合规则的字符串 转义字符用 “\” 转义。如：表达文本 “\”，表达式中应使用 “\“ 元字符 元字符 含义 表达式 匹配示例 . 除换行符以外任意字符 q.e qwe、qse ^ 字符串的开始（位置） ^qw （行首）qw $ 匹配字符串的结束（位置） we$ we（行尾） \b 单词的开头或结尾，即单词交界处（位置） \d 一位数字 q\dw q1w \s 空白字符(半/全角空格，制表符，换行符等) qw\se qw e \w 任意的字母，数字，下划线等 q\we q_e \A 仅匹配字符串开头 \Aqwe qwe \Ｚ 仅匹配字符串结尾 qwe\Z qwe [\u4E00-\u9FA5] 匹配汉字 q[\u4E00-\u9FA5]e q我e 限定符 限定符 含义 表达式 匹配示例 * 重复零次或更多次 qwe* qweeeee、qw + 重复一次或更多次 qw\s+e qw e ? 重复零次或一次 qw?e qwe、qe {n} 重复 n 次 qw{3}e qwwwe {n,} 重复 n 次或更多次 qw{3,}e qwwwwe {n,m} 重复 n 到 m 次(n ≠ m) qw{5,6}e qwwwwwwe | 分支条件 分支 含义 表达式 匹配示例 &#124; 使用分枝条件时，将会从左到右地测试每个条件，如果满足了某个分枝的话，就不会去再测试后面的条件，因此要注意各个条件的顺序 qw?e &#124; qw*q qe、qwwwwq 分组表达式中，被小括号括起来的子表达式称分组： 表达式左到右顺序按次遇到的左括号及其括号内容即编号第n个分组 分组作为一个整体，后可接数量词：(分组){重复次数} 分组中 “|” 仅分组内有效 分组 含义 表达式 匹配示例 (……) 分组 q(w &#124; e)r qer、qwr \&lt;编号&gt; 引用指定编号分组 (\d)qwe\1 1qwe1 (?&lt;别名&gt;) 为分组指定别名（仍可使用编号） (?P\d)abc(?P=id) 1abc1 (?=&lt;别名&gt;) 引用指定别名分组 (?P\d)abc(?P=id) 1abc1 反义查找不属于该定义的字符 反义代码 说明 表达式 匹配示例 \W 匹配任意不是字母，数字，下划线，汉字的字符 q\We q e \S 匹配任意不是空白符的字符 q\Se q!e \D 匹配任意非数字的字符 q\De qwe \B 匹配不是单词开头或结束的位置 q\Bwe qwe 字符集合 匹配除了字符集合中所有字符以外的任意字符 qwet qrt 后向引用 捕获 含义 (exp) 匹配exp,并捕获文本到自动命名的组里 (?&lt;组名&gt;exp) 匹配exp,并捕获文本到名称为组名的组里，也可以写成(?’组名’exp) (?:exp) 匹配exp,不捕获匹配的文本，也不给此分组分配组号 零宽断言 零宽断言 含义 (?=exp) 匹配exp前面的位置 (?&lt;=exp) 匹配exp后面的位置 (?!exp) 匹配后面跟的不是exp的位置 (?&lt;!exp) 匹配前面不是exp的位置 注释表达式中添加注释： (?#注释) 示例： \d{3}(?#区号)-\d{7} 加粗部分即注释 懒惰限定符匹配任意数量的重复，但是在能使整个匹配成功的前提下使用最少的重复 懒惰限定符 含义 *? 重复任意次，但尽可能少重复 +? 重复1次或更多次，但尽可能少重复 ?? 重复0次或1次，但尽可能少重复 {n,m}? 重复n到m次，但尽可能少重复 {n,}? 重复n次以上，但尽可能少重复]]></content>
      <categories>
        <category>regex</category>
      </categories>
      <tags>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 使用速览]]></title>
    <url>%2F2018%2F02%2F23%2Flinux-quick-use%2F</url>
    <content type="text"><![CDATA[[Updated] 本文整理了能够快速上手 Linux 的一些常用操作和基本知识。 Linux 使用速览终端快捷键进程控制 Ctrl+d ： 键盘输入结束或退出终端 Ctrl+C ： 杀死当前进程(也可以用来清空当前行) Ctrl+s ： 暂停当前程序，暂停后按下任意键恢复运行 Ctrl+z ： 将当前程序放到后台运行，恢复到前台为命令fg 编辑控制 Ctrl+A ： 将光标移至输入行头，相当于Home键 Ctrl+E ： 将光标移至输入行末，相当于End键 Ctrl+F ： 向前移动一个字符 Ctrl+B ： 向后移动一个字符 Ctrl+U ： 剪切文本直到行的起始(可以用于清空行) Ctrl+K ： 剪切文本直到行的末尾 Ctrl+Y ： 粘贴最近剪切的文本 Alt+Backspace ： 向前删除一个单词 Ctrl+P / Ctrl+N ： 上下历史记录，相当于↑↓键 Shift+PgUp ： 将终端显示向上滚动 Shift+PgDn ： 将终端显示向下滚动 通配符 通配符 含义 * 匹配 0 或多个字符 ? 匹配任意一个字符 [list] 匹配 list 中的任意单一字符 [!list] 匹配 除list 中的任意单一字符以外的字符 [c1-c2] 匹配 c1-c2 中的任意单一字符 如：[0-9] [a-z] {string1,string2,…} 匹配 string1 或 string2 (或更多)其一字符串 {c1..c2} 匹配 c1-c2 中全部字符 如{1..10} 获取帮助man 的用法1man k（区段） 内容 如： 1man 1（一般命令） ls（命令） 区段 k 含义 1 一般命令 2 系统调用 3 库函数，涵盖了C标准函数库 4 特殊文件（通常是/dev中的设备）和驱动程序 5 文件格式和约定 6 游戏和屏保 7 杂项 8 系统管理命令和守护进程 man 手册快捷键 /关键字 ： 搜索 n ： 切换到下一个关键字所在处 shift+n ： 切换到上一个关键字所在处 Space ： 翻页 Enter ： 向下滚动一行 用户及文件权限管理用户管理 查看用户 1who -[a/d/q/u] 创建用户 1sudo adduser USERNAME 删除用户 1sudo deluser USERNAME ：remove-home 用户组 查找所属用户组 1groups USERNAME 查看 /etc/group 1cat /etc/group | sort 将其他用户加入 sudo 用户组 1sudo usermod -G sudo USERNAME 权限管理查看权限 文件类型 d ： 目录 l ： 软链接 b ： 块设备 c ： 字符设备 s ： socket p ： 管道 文件权限 r ： 读 w ： 写 x ： 执行 链接数 连接到该文件所在的 inode 结点的文件名数目 文件大小 以 inode 结点大小为单位来表示的文件大小（ls -lh：直观的查看文件的大小） 查看权限1ll FILENAME 显示内容依次为： 1drwxrwxrwx INODE_LINKS USERNAE GROUPNAME INODE_SIZE TIME FILENAME 修改文件所有者1sudo chown USEENAME FILENAME 修改文件权限 二进制修改：rwx (421) 1chmod 777 FILENAME 加减赋值修改：[ u | g | o ] [+ | -] [r | w | x] 1chmod ugo+rwx FILENAME Linux 文件基本操作及目录结构基本操作文件操作文件名以 “test_file” 为例；以下&lt;位置&gt;默认为当前工作目录&lt;.&gt; 创建 1touch &lt;创建位置&gt;/test_file 移动/重命名 1mv &lt;文件位置&gt;/test_file &lt;移动位置&gt; 重命名 当移动位置为不存在的目录名，即将文件 test_file 重命名为该目录名 复制 1cp &lt;文件位置&gt;/test_file &lt;移动位置/复制名&gt; 删除 1rm &lt;文件位置&gt;/test_file &lt;移动位置/复制名&gt; 查看 1cat test_file &lt;移动位置/复制名&gt; 一次性查看文件所有内容 1less test_file &lt;移动位置/复制名&gt; 可以上下键翻页查看内容，按 q 退出 1head/tail test_file &lt;移动位置/复制名&gt; 查看文件内容的前/后10行 参数 (-n 行数)：查看前/后几行 编辑 通常 Linux 的自带编辑器有终端下的 vi/vim 编辑器（vim 可看做 vi 的高级版，对 vi 完全兼容），以及可视化编辑器 gedit。使用下方命令新建或打开已有文件： 12vi file.txtgedit file.txt 目录操作目录名以 “test_dir” 为例；缺省位置为当前工作目录&lt;.&gt; 创建 1mkdir test_dir &lt;创建位置/目录名&gt; 移动/重命名 1mv test_dir &lt;移动位置/目录名&gt; 重命名 当移动位置为不存在的目录名，即将目录 test_dir 重命名为该目录名 复制目录及目录内容 1cp -r test_dir &lt;创建位置/目录名&gt; 删除 1rm test_dir &lt;删除位置/目录名&gt; 参数 -r：递归删除 -f：强制删除 Linux 目录结构 / : 根目录 bin : 一般用户可用，启动时用到的命令 boot : 启动项 grub : 开关机设置相关文件 内核文件（vmlinuz） dev : 存放设备文件 etc : 包含系统特有的可配置文件，即用于控制程序运行的本地文件 home : 用户家目录 lib : 用于存放程序的动态库和模块文件 media : 挂载本地磁盘或其他存储设备。如：cdrom、floppy、U 盘 mnt : 用于挂载其他临时文件系统 opt : 发行版附加软件包的安装目录 root : 根用户的家 sbin : 存放大多数 root 用户才能执行的命令，系统进行更新、备份、还原和开关机所用的命令 srv : 存放服务进程所需的数据文件（如 ftp 服务）和一些服务的执行脚本 tmp : 存放各种临时文件 usr : 存储只读用户数据的第二层次，包含大多数用户工具和应用程序 bin : 非必要可执行文件，面向所有用户 include : 包含标准头文件 lib : /usr/bin 和 /usr/sbin 中二进制文件的库 local : 本地数据的第三层次，具体到本台主机 bin etc include lib share src share : 共享数据 sbin : 非必要的系统二进制文件 src : 源代码 var : 变量文件 account cache lib lock log run tmp spool mail]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 数学公式语法]]></title>
    <url>%2F2018%2F01%2F28%2Fmarkdown-formula%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 Markdown 数学公式语法 Markdown 数学公式语法公式公式类型行内公式将公式插入到本行内。使用一个 “$” 包裹数学公式。 极限：$\displaystyle \lim_{n \to +\infty}{x_n = \alpha}$ 独行公式将公式插入到新的一行内，并且居中。使用两个 “$$” 包裹数学公式。极限： \displaystyle \lim_{n \to +\infty}{x_n = \alpha}上下标 上下标 语法 预览 ^ x^2 $x^2$ _ x_1 $x_1$ 运算符普通运算 运算 语法 预览 加 + $+$ 减 - $-$ 乘 \times $\times$ 除 \div $\times$ 开方 \sqrt[n]{x} $\sqrt[n]{x}$ 加减 \pm $\pm$ 减加 \mp $\mp$ 绝对值 \ \ $ x+y $ 分数 \frac{b}{a} $\frac{b}{a}$ 分数 {b} \voer {a} ${b} \over {a}$ 对数运算 运算 语法 预览 对数 \log $\log$ ln \ln $\ln$ lg \lg $\lg$ 集合运算 运算 语法 预览 空集 \emptyset $\emptyset$ 属于 \in $\in$ 不属于 \notin $\notin$ ⊂ \subset $\subset$ ⊃ \supset $\supset$ ⊆ \subseteq $\subseteq$ ⊇ \supseteq $\supseteq$ ⊇ \bigcap $\bigcap$ ⋃ \bigcup $\bigcup$ ⋁ \bigvee $\bigvee$ ⋀ \bigwedge $\bigwedge$ 逻辑运算 运算 语法 预览 因为 \because $\because$ 所以 \therefore $\therefore$ 任取 \forall $\forall$ 存在 \exists $\exists$ 不等于 \neq $\neq$ 不属于 \not\subset $\not\subset$ 微积分运算 运算 语法 预览 极限 \lim $\lim$ 无穷 \infty $\infty$ 积分 \int $\int$ 重积分 \int（i 的个数为重数） $\iiint$ 曲线积分 \oint $\oint$ 三角函数 运算 语法 预览 度数 90^\circ $90^\circ$ ∠ \angle $\angle$ sin \sin $\sin$ cos \cos $\cos$ tan \tan $\tan$ 希腊字母 字母 语法 预览 Δ \Delta $\Delta$ Θ \Theta $\Theta$ Σ \Sigma（’S’ 大写） $\Sigma$ Ω \Omega（’O’ 大写） $\Omega$ α \alhpa $\alpha$ β \beta $\beta$ γ \gamma $\gamma$ δ \delta $\delta$ ϵ \epsilon $\epsilon$ η \eta $\eta$ θ \theta $\theta$ κ \kappa $\kappa$ λ \lambda $\lambda$ μ \mu $\mu$ ν \nu $\nu$ π \pi $\pi$ σ \sigma（’s’ 小写） $\sigma$ τ \tau $\tau$ ω \omega（’o’ 小写） $\omega$ ξ \xi $\xi$ ρ \rho $\rho$ 栗子求和公式\displaystyle \sum_{i=1}^n \frac{1}{i} 预览： \displaystyle \sum_{i=1}^n \frac{1}{i}求积公式\displaystyle \prod_{i=1}^n i 预览： \displaystyle \prod_{i=1}^n i求积分\displaystyle \int_0^{1} x dx 预览： \displaystyle \int_0^{1} x dx求极限\displaystyle \lim_{n \rightarrow +\infty} \frac{1}{n(n+1)} 预览： \displaystyle \lim_{n \rightarrow +\infty} \frac{1}{n}]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 基础语法]]></title>
    <url>%2F2018%2F01%2F22%2Fmarkdown-basic%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 Markdown 基础语法 Markdown 基础语法标题语法123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 预览一级标题二级标题三级标题四级标题五级标题六级标题 字体语法123456*这是斜体*_这是斜体_**这是粗体**__这是粗体__***这是粗斜体***___这是粗斜体___ 预览这是斜体 这是斜体 这是粗体 这是粗体 这是粗斜体 这是粗斜体 列表语法无序列表12345678* 第一项* 第二项+ 第一项+ 第二项- 第一项- 第二项 有序列表1231. 第一项2. 第二项3. 第三项 嵌套列表1234567891. 第一项2. 第二项 - 第一项 - 第二项- 第一项 - 第一项 - 第二项- 第二项 预览无序列表 第一项 第二项 第一项 第二项 第一项 第二项 有序列表 第一项 第二项 第三项 嵌套列表 第一项 第二项 第一项 第二项 第一项 第一项 第二项 第二项 区块语法12345678910111213&gt; 最外层&gt;&gt; 第一层嵌套&gt;&gt;&gt; 第二层嵌套&gt; 区块列表&gt; 1. 第一项&gt; 2. 第二项&gt; 3. 第三项- 第一项 &gt; 外层区块 &gt; 内外层区块- 第二项 预览 最外层 第一层嵌套 第二层嵌套 区块列表 第一项 第二项 第三项 第一项 区块 第二项 代码块语法单行代码1`import sys` 多行代码123```python &lt;-指定语言`import sys````. 或 1import sys &lt;-一个Tab指定代码区块 预览单行代码import sys 多行代码1import sys 或 import sys 表格语法1234col1 | col2 | col3- | - | -row1_1 | row1_2 | row1_3row2_1 | row2_2 | row2_3 预览 col1 col2 col3 row1_1 row1_2 row1_3 row2_1 row2_2 row2_3 插入链接/图片语法插入普通链接123[本站地址](https://xhzs.github.io/)这是本站地址：&lt;[链接地址](https://xhzs.github.io/)&gt; 插入图片1&lt;img src="图片的本地/线上地址" width="50%"&gt; 预览普通链接这是本站地址 这是本站地址：[链接地址](https://xhzs.github.io/) 图片]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 chrome 下 jupyter-notebook 异常问题]]></title>
    <url>%2F2018%2F01%2F16%2Fchrome-jupyter-exception%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 chrome 运行 jupyter-notebook 所出现的异常及解决方法 解决 chrome 下 jupyter-notebook 异常问题问题chrome 下使用 jupyter-notebook 出现以下问题： 按 tab 键补全后自动移至下一单元格 括号自动补全出问题。如：按 “(“ 打印 “(()” 原因开启了某些 chrome 扩展程序所致 解决方法采用排除法测试出引起问题的 chrome 拓展程序，将其关闭]]></content>
      <categories>
        <category>chrome 使用问题</category>
      </categories>
      <tags>
        <tag>chrome</tag>
        <tag>jupyter-notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 chrome 安装插件失败问题]]></title>
    <url>%2F2018%2F01%2F14%2Fchrome-install-extension%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 chrome 离线安装插件失败的问题及解决方法 解决 chrome 安装插件失败问题问题chrome 离线安装插件失败。页面左上角显示： Package is invalid:’CRX_HEADER_INVALID’ 解决以安装 example.crx 为例： 备份 example.crx，以防误操作或错误情况 将 example.crx 更改后缀名为 .rar 或 .zip 解压 example.rar(.zip) 到 example 文件夹，文件夹内包含：example.js、icon.png、manifest.json 打开 chrome 拓展页面，右上角开关开启开发者模式后，左上角点击“加载已解压拓展程序”，选择步骤 3 中解压的 example 文件夹，安装成功]]></content>
      <categories>
        <category>chrome 使用问题</category>
      </categories>
      <tags>
        <tag>chrome</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机组成原理]]></title>
    <url>%2F2018%2F01%2F06%2Fcomputer-struct%2F</url>
    <content type="text"><![CDATA[[Updated] 本文梳理了《计算机组成原理》的基础知识 目录 Chapter 1 Chapter 2 Chapter 3 Chapter 4 Chapter 5 Chapter 6 Chapter 7 Chapter 8 总线 存储器 I/O 计算的运算方法 指令系统 CPU 控制单元 控制单元的设计 总线 判优控制 ！当总线上各个主设备同时请求占用总线时，总线控制器按一定优先等级确定某个设备可以占用总线。 ？总线特点为某一刻时刻只允许一个设备向总线发送信息，若两个以上部件同时向总线发送信息，势必导致信号冲突传输无效。 链式查询 1 BR、1 BS、1 BG 优：优先级固定；结构简单、易扩充 缺：电路故障敏感，第i个有故障，第i个以后皆无法工作 计数器定时查询 1 BR、1 BS、1bN设备地址线 优：优先级可不固定；电路故障不如链式查询敏感 缺：控制比链式查询复杂 独立请求 N BR、N BS、N BG 优：响应速度快；优先级控制灵活，可预先固定，也可通过程序改变；可屏蔽设备请求 缺：仲裁线路复杂 通信控制 ！解决通信双方如何获知传输开始和传输结束，以及通信双方如何协调配合。 ？因为总线由众多部件共享，在传送时间上只能用分时方式解决，故通信双方必须按某种约定的方式进行通信。 同步通信 采用公共时钟信号控制，统一传输周期（必须按最慢速度部件设计） 适用：总线长度较短，各部件存取时间相较一致 异步通信 没有公共时钟，采用应答式通信，无固定传输周期 全互锁（完全制约，可靠性最高）/半互锁（简单制约）/不互锁（无制约） 适用：总线各部件速度不一致 分离式通信 总线传输周期分为两个子周期供不同模块占用，总线上无等待时间，最充分发挥了总线的有效占用 半同步通信 既有公共时钟，又允许速度不同的模块和谐工作，采用插入等待周期的措施协调通信双方的配合问题 串行传输与并行传输 串行传输 数据在一条线路上按位依次传输 成本低，但速度慢，适合远距离的传输 并行传输 每个数据位都有一条独立传输线路，所有数据按位同时传输 成本高，速度快，适合近距离、高速传输 总线复用 不同信号（数据/地址）共用同一组物理线路，分时使用 需先给地址信号，然后用地址锁存信号将其保存 总线带宽（MBps）：单位时间总线上可传输数据位数，也称“数据传输率” 影响因素：总线宽度、传输距离、主频 总线带宽 = 一个传输周期传输字节数/时钟周期 | 一个传输周期传输字节数*时钟频率 存储器 芯片：16K×8位/16KB 地址线 = 14根 数据线 = 32根 引出线最少数目 = 数据线+地址线+2 多体结构存储器 将存储器分成若干个（n个）独立的模块，每个模块的容量和存取周期均相等，且可独立进行读写操作。将独立模块： 高位交叉编址 各模块分别响应不同请求源，实现多体并行 高位—存体号，低位—选择存储体内的字 低位交叉编址， 不改变存取周期的前提下，增加存储器带宽，n个模块则带宽提高至n倍 高位—选择存储体内的字，低位—存体号 存取周期T，总线传输周期t，连续读取n个字时间=T+（n-1）t 提高访存的措施 采用高速器件，选用存取周期短的芯片，提高存储器速度 采用缓存，CPU将近期要用的信息先调入缓存，而缓存速度比主存快得多，CPU从缓存存取信息则缩短访存时间，提高了访存速度 调整主存结构，如采用单体多字结构或多体结构（都增加存储器带宽） 程序访问的局部性原理 由于指令和数据在主存的地址分布不是随机的，而是相对地聚簇，故程序执行时对存储器的访问使不均匀的 利用该原理：对缓存-主存，把CPU最近期执行的程序放在容量较小速度较高的缓存中；对主存-辅存，把程序中访问频度高、比较活跃的部分放在主存中。既提高了访存速度又扩大了存储器容量 地址映射（硬件完成） 直接 假设C块缓存，每个主存块j只与一个缓存块i对应：i = j mod C 映射简单，但主存块只能固定对应某个缓存块，不够灵活、命中率低 全相联 主存任一块都可以映射到缓存中的任一块上 灵活、命中率高，但所需电路多、成本高 组相联 把缓存分Q组，组内分R块，主存块号j映射到缓存组号i内任一块：i = j mod Q，缓存内1~R任一块 比直接灵活、命中率高，比全相联成本低，是两者的折中，广泛应用 三级存储系统（平衡—速度、容量、价格） 高速缓存 解决：CPU和主存速度匹配，提高访存速度缓存 管理：硬件和操作系统完成 地址对用户透明 虚存 解决：扩大存储容量 管理：硬件和操作系统完成 CPU不直接访问二级存储器 RAM刷新 方式：集中/分散/异步 原因：存储电荷电容放电 I/O I/O编址方式 独立编址：I/O地址与主存地址分开，不占主存容量，但需要专用I/O指令访I/O 统一编址：在主存地址划出一定范围作I/O地址，通过访存指令访问I/O，但减少了主存容量 主机与I/O交换信息的控制方式 程序查询 主机与I/O串行工作 程序中断 主机与I/O并行工作,主程序和信息传送串行 DMA 主机与I/O并行工作,主程序和信息传送并行 通道 I/O处理机 程序查询 CPU启动I/O后停止现行程序，插入一段程序时刻查询I/O设备准备状况，等待I/O准备就绪时可实现信息交换，存在“踏步”现象 程序中断 管理（多重）中断硬件 中断请求触发器（INT）：标志中断源向CPU提出中断请求 中断屏蔽触发器（MASK）：为“1”表示屏蔽该中断源 排队器：中断判优 向量地址形成部件：产生中断源向量地址 允许中断触发器（EINT）：为“1”允许处理中断 中断标志触发器（INTR）：标志进入中断周期 堆栈：保护现场 中断查询信号电路：每条指令执行周期结束时刻，向各中断源发查询信号 过程： 中断请求：CPU启动I/O设备，I/O准备就绪后向CPU提出中断请求 中断判优：中断判优逻辑选择优先级最高的中断请求，待CPU处理 中断响应：若INT（中断请求触发器）为”1”且请求中断设备未被屏蔽，系统进入中断响应周期—CPU自动执行中断隐指令[ 硬件完成：保护程序断点(即PC内容)、硬件关中断、向量地址送PC（硬件向量法）或中断识别程序入口地址送PC（软件查询法） ] 中断服务：中断响应周期结束，CPU转入取指周期，按向量地址取出无条件转移指令（或按向量地址查入口地址表）；转至向量地址对应的中断程序服务入口地址，开始执行中断服务程序[ 保护现场（PC内容—中断隐指令；寄存器内容—软件编程）、与I/O传送信息、恢复现场 ] 中断返回：中断服务程序最后一条即中断返回指令（返回程序断点） 响应条件和时间： 条件：EINT为“1”（即开中断）；中断请求未被屏蔽，且排队后被选中 时间：指令执行阶段的结束时刻，CPU发出中断查询信号，才能获取中断请求信号 向量地址 存放服务程序入口地址的存储单元地址，由硬件形成 当有中断请求且排队选中时，通过自由组合逻辑电路组成的向量地址形成部件可形成向量地址 输入：排队器；输出：中断周期送至PC；传送：数据总线 开/关中断 EINT为“1”时，允许CPU响应中断；EINT为“0”时，CPU不能响应中断 关中断即将EINT置“0”；开中断即置“1” 多重中断(主要区别在中断服务程序)：CPU处理中断过程中出现新的中断请求，暂停现行中断处理转至处理新的中断 多重中断条件 必须重新开中断 优先级更高的中断请求才能中断现行程序（内部中断&gt;不可屏蔽中断&gt;可屏蔽中断） 单重中断：保护现场-&gt;设备服务-&gt;恢复现场-&gt;开中断-&gt;中断返回 多重中断：保护现场-&gt;开中断-&gt;设备服务-&gt;恢复现场-&gt;中断返回 中断服务程序与调用子程序区别 中断服务程序与中断时CPU正在运行程序相互独立；子程序与CPU正在运行程序是同一程序的两部分 除了软中断，中断通常随机产生；子程序调用由CALL指令引起 中断服务程序入口地址可通过硬件向量法产生向量地址，再由向量地址找到入口地址；子程序调用入口地址由CALL指令地址码给出 中断需要对多个同时发生的中断进行裁决；子程序调用无此操作 都要保护程序断点：前者中断隐指令完成；后者CALL指令完成 都要保护寄存器内容的操作 中断和DMA区别 数据传送：中断靠程序传送；DMA靠硬件传送 CPU响应时间：中断在一条指令执行结束时响应；DMA在存取周期结束时响应 异常处理能力：中断有；DMA无 保护现场：中断需中断现行程序，需保护现场；DMA不需中断现行程序，无需保护现场 优先级：DMA高于中断 DMA 特点： I/O和CPU并行工作 主存和I/O接口间有一条直接数据通路 不中断现行程序，无需保护、恢复现场 DMA请求占用总线时，若采用周期挪用，CPU暂停一个存取周期访问主存，但可继续自身内部操作（如乘法），即DMA传送和主程序并行 硬件：数据缓存寄存器、DAR、AR、WC、中断机构、DMA控制逻辑 过程： 预处理 指明数据传送方向输入（读）/输出（写） 设备地址送DAR（设备地址寄存器） 主存地址送AR（主存地址计数器） 传送数据字数送WC（字计数器） 启动设备 数据传送 主存地址送总线 数据送I/O设备（或主存） 修改主存地址和WC 重复直至数据块传送结束 后处理 由中断服务程序作DMA结束处理（测试传送过程是否出错、决定是否继续使用DMA传送数据） DMA和CPU分时使用主存： 停止CPU访存 DMA在传送数据时独占主存，CPU放弃总线使用权，基本处于不工作或保持原状态，直至DMA传送结束 周期挪用 一旦I/O有DMA请求，由I/O设备挪用一个存取周期。此时CPU可完成自身操作，但要停止访存 DMA和CPU交替访存 适用CPU工作周期比主存存取周期长时。CPU工作周期的上下半周期由DMA和CPU交替使用访存，使DMA传送和CPU工作效率最高，但硬件逻辑复杂 计算的运算方法 判溢出 定点机 参与运算的两个操作数符号相同，结果的符号与原操作数符号不同，则溢出 求和时最高进位与次高进位异或结果为1，则溢出 浮点机判溢出 当阶码大于最大正阶码时，则溢出 当阶码小于最小负阶码时，则按机器零处理 进位：影响加减运算速度的关键 进位链：传递进位的逻辑电路 先行进位：高位进位和低位进位同时产生 单重分组跳跃进位 n位全加器分若干小组，组内进位同时产生，组间串行进位 多重分组跳跃进位 n位全加器分若干大组，若干大组内又包含若干小组，大组内各小组进位同时产生，小组内进位同时产生，大组间串行进位 快于单重，但线路更复杂 指令系统 不同地址格式指令 地址格式 访存次数 备注 四地址 4 A4指出下条指令地址 三地址 4 PC指出下条指令地址 二地址 4 操作结果存回A1、A2或ACC 一地址 4 ACC存放操作数和结果 数据存放方式。存储字长32位，可按字节、半字、字寻址： 边界对准：数据字地址一定是4的整数倍。所存数据不满足该要求时，填充一个或多个空白字节（浪费存储空间） 边界不对准：数据字跨两个存储字时需两次访存，并对高低字节位置进行调整后才能取得数据字（影响取数时间） 间址/基址/变址：可扩大寻址范围 通过访存（多次间址多次访存）得到有效地址 间址 访存导致时间较长（T一次访存 &gt;&gt; T一次寄存器） 地址变换（R+A）得到有效地址 基址 基址寄存器内容由操作系统给定，且在程序执行过程中不可变 支持多道程序技术的应用 变址 变址寄存器内容由用户给定，且在程序执行过程中可变 适用于处理数组问题 相对/堆栈寻址 相对：EA = (PC) + A A为位移量（字节），决定寻址范围；可正可负，补码表示 便于程序浮动，用于转移指令 堆栈：SP +/- ▲ -&gt; PC 有效地址在SP中，指令中可少一个指令字段 ▲与主存编址方式相关：按字编址，▲取1；按字节编址，字长16位时▲取2，字长32时▲取4 RISC（CISC） 选用频度高简单指令，复杂指令功能由简单指令实现（指令系统复杂庞大） 指令长度固定，指令格式种类少，寻址方式种类少（不固定、多、多） 只有LOAD/STORE访存，其余指令皆在寄存器进行（可访存指令不受限制） CPU中有多个通用寄存器（设专用寄存器） 控制器采用组合逻辑控制（微程序） 采用流水技术，大部分指令1个时钟周期内完成（各指令执行时间相差大，大部分需多个时钟周期） 采用优化的编译程序（难以用优化编译生成高效代码） 与CISC比较： 提高指令执行速度 便于设计，可降低硬件设计复杂度 简化指令功能，有利于编译程序代码优化 不易实现指令系统兼容 CPU CPU 功能 指令控制：控制程序的顺序执行 操作控制：产生完成每条指令所需控制命令 时间控制：对各种操作加以时间上的控制 数据加工：对数据进行算术和逻辑运算 中断处理：处理计算机在运行过程中出现的异常情况和特殊请求 组成 寄存器 PC：存放现行指令地址，位数取决于存储器容量 IR：存放现行指令，位数取决于指令字长 通用寄存器：存放数据和地址，位数取决于机器字长 指令译码器 + 控制单元CU：根据指令译码在规定时间发出操作命令 ALU：算术逻辑运算 中断系统：处理中断 指令周期：取指+（间址）+执行+（中断） 执行 -&gt; 中断周期 -&gt; 取值 存取周期 -&gt; DMA周期 -&gt; 存取周期（指令周期任一阶段皆可） 指令流水 结构相关 硬件资源满足不了指令重叠执行的要求，发生资源冲突 如：同一时间，几条重叠的指令分别取值、取数、存数，发生访存冲突 数据相关 指令重叠执行，可能改变操作数的读写访问顺序，导致数据相关冲突 如：某条指令需要用到前面指令的执行结果，而这些指令在流水线中重叠执行，可能改变对操作数读写访问顺序 控制相关 流水线遇到分支指令或其他改变程序计数器PC的指令，造成指令执行顺序的改变 如：某条指令需等前面指令做出转移方向的决定才能进入流水线 流水线多发技术 超标量流水：每个时钟周期内可同时并发多条独立指令，处理器中需配置多个功能部件和指令译码电路，以便同时执行多个操作 超流水线：在原来的时钟周期内，功能部件被使用多次 超长指令字：对编译器要求高，充分挖掘指令间潜在并行性（一个时钟周期内，各功能部件无数据相关），把能并行的指令合成一条具有多个操作码（需相应个数功能部件）的超长指令 中断系统 INTR 与 EINT INTR 中断标志触发器：指示CPU是否进入中断周期 EINT 允许中断触发器：开放或关闭中断系统 置“1”：系统开放，允许中断（开中断指令） 置“0”：关中断（关中断指令、中断隐指令、硬件自动复位） 中断判优：在某一时刻可能有多个中断源（中断源请求随机）提出请求，而CPU只能响应一个，故须判优已解决响应优先次序 硬件排队：组合逻辑电路实现 软件排队：程序按优先级（从高至低）顺序查询各中断源 中断服务程序入口地址寻找 硬件向量法（向量中断）：当有中断请求时，由硬件产生该中断源对应的向量地址，再由向量地址找到服务程序的入口地址，然后暂停现行程序转至中断服务程序 排队器输出 -&gt; 向量地址形成部件 -&gt; 输出向量地址 向量地址寻找入口地址方式 向量地址单元内存放一条无条件转移指令 在向量地址单元内直接存放入口地址，形成一个中断向量地址表 软件查询法：编写中断识别程序实现 屏蔽 屏蔽触发器：内容即屏蔽字，每个中断源对应一个屏蔽字，为“1”时CPU不响应该中断源请求 优先级 响应优先级：CPU响应各中断源请求的优先次序，通常硬件线路已设置好，不便改动（不采用屏蔽时） 处理优先级：CPU实际对各中断源请求的处理优先次序（采用屏蔽） 采用屏蔽技术的中断服务流程 保护现场 -&gt; 置屏蔽字 -&gt; 开中断 -&gt; 中断服务 -&gt; 关中断 -&gt; 恢复现场 -&gt; 恢复屏蔽字 -&gt; 开中断 -&gt; 中断返回 作用 改变处理优先级 为实现多重中断，屏蔽低级别中断请求对现行中断处理程序的干扰 封锁部分中断请求，使程序控制更灵活 控制单元 控制单元CU 功能：发出各种操作命令(即控制信号) 受控制：指令寄存器(操作码)、时钟、标志、系统总线控制信号(中断) 多级时序 指令周期：完成（取出并执行）一条指令所需的时间 机器周期：指令执行过程中一个基准时间，通常以存取周期作为机器周期（因为完成指令都需取指，而一次访存时间固定）。一个机器周期内完成若干微操作，可通过节拍控制产生每一个微操作命令 时钟周期：主频（时钟信号的频率）的倒数，也可称为节拍（时钟信号控制产生，每个节拍宽度对应一个时钟周期）。一个节拍内完成若干需同时执行的操作，是控制计算机操作的最小时间单位 三者关系：每个指令周期含若干个机器周期，可不相等；每个机器周期含若干个时钟周期（节拍），可不相等 机器速度：同主频下， 机器周期中时钟周期数和指令周期中机器周期数不同，机器速度不同。（机器周期中含时钟周期少的机器速度更快） 控制方式 同步控制：微操作受统一基准时标时序信号控制。存取周期不统一时，取最长存取周期作为机器周期 采用定长的机器周期：采用完全统一、具有相同时间间隔和相同数目节拍 采用不定长的机器周期：机器周期内节拍数可不等；大多数微操作一个机器周期内完成，复杂微操作延长机器周期或增加节拍 采用中央控制和局部控制相结合的方法：大部分中央控制，少数局部控制 局部控制每一个节拍T*宽度与中央控制节拍宽度相同 局部控制节拍作为中央控制中机器节拍的延续，插入中央控制的执行周期 异步控制：不存在基准时标信号，微操作时序由专用的应答线路控制。控制器发出某一个微操作命令后，等待执行部件完成该操作时所发回的应答信号，再开始执行下一个操作 联合控制：同步与异步结合。即大多数微操作在同步时序信号控制下进行，而对时间难以确定的微操作（如I/O相关）采用异步控制 控制单元设计 组合逻辑控制器 采用硬连线逻辑：一个微操作命令对于一个逻辑电路 思路清晰，简单明了 结构复杂，线路复杂。一旦构成，除非物理上重新连线，否则无法增加新的控制功能 组合逻辑与微程序控制组成异同 同：均有PC、IR、时序电路、中断系统、状态条件 异： 微操作命令序列形成部件不同。组合逻辑核心部件—门电路；微程序核心部件—控制存储器ROM(存放全部微程序) 微操作命令及节拍安排的主要差别： 取指阶段 12OP(IR) -&gt; ID //组合逻辑：指令操作码送指令译码器OP(IR) -&gt; 微地址形成部件 //微程序：指令操作码送微地址形成部件 微程序每条指令都要增加一个将微指令下地址字段送CMAR的微操作 1Ad(CMDR) -&gt; CMAR 微程序控制器 采用存储逻辑：每条机器指令编写成一个微程序，每一个微程序包含若干条微指令（操作控制字段+顺序控制字段），每一条微指令对应一个或几个微操作命令 优点：规整形、灵活性、可维护性 控制器中微程序个数 = 机器指令数 + 3（取指/间址/中断周期) 组成 控存：存放全部微程序 CMAR（控存地址寄存器）：存放欲读出微指令地址。采用增量计数器法形成后续微指令地址时，有计数功能 CMDR（控存数据寄存器）：存放取出的微指令 顺序逻辑：控制微指令序列 输入：微地址形成部件、微指令下地址字段、外来标志 输出：CPU内部和系统总线的控制信号 微指令编码方式 直接编码（直接控制）：操作控制字段每一位代表一个微命令 简单直观，输出直接用于控制，执行速度快 微指令字较长，使控存容量大 字段直接编码（显示编码）：操作控制字段分段，每个字段经译码发出微操作命令，且互斥 缩短字长，以较少二进制信息表示较多微命令信号 增加译码电路，执行速度降低 字段间接编码（隐式编码）：一个字段某些命令需由另一字段某些微命令解释 更能缩短微指令字长，但速度更慢 微指令序列地址形成 直接由微指令的下地址字段给出 根据机器指令的操作码形成 增量计数器法，即 （CMAR）+ 1 -&gt; CMAR 根据各钟标志决定微指令分支转移的地址 测试网络形成 硬件直接产生微程序入口地址 微指令格式 水平型：一次能定义多个并行操作的微命令。直接编码、字段直接编码、字段间接编码以及直接及混合编码都属于水平型指令格式 大多数微命令可直接控制对象，故每条微指令执行时间短 微指令字长较长，故可用较少微指令数实现一条机器指令的功能 垂直型：采用类似机器指令操作码方式，在微指令中设置微操作码字段，由微操作码规定微指令功能 经过译码控制对象，影响执行时间 微指令字长较短，实现一条机器指令微程序比水平型微指令长的多，以较长微程序结构换取较短微指令结构]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>计组</tag>
      </tags>
  </entry>
</search>
